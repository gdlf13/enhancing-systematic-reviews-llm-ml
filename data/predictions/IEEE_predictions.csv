Authors;Year;Abstract;Title;Mig review;GPT4 Review;Predicted Label
"A. Jadon; S. Kumar";2023;The widespread adoption of electronic health records and digital healthcare data has created a demand for data-driven insights to enhance patient outcomes, diagnostics, and treatments. However, using real patient data presents privacy and regulatory challenges, including compliance with HIPAA [1] and GDPR [2]. Synthetic data generation, using generative AI models like GANs [3] and VAEs [4], offers a promising solution to balance valuable data access and patient privacy protection. In this paper, we examine generative AI models for creating realistic, anonymized patient data for research and training [5], explore synthetic data applications in healthcare, and discuss its benefits, challenges, and future research directions. Synthetic data has the potential to revolutionize healthcare by providing anonymized patient data while preserving privacy and enabling versatile applications.;Leveraging Generative AI Models for Synthetic Data Generation in Healthcare: Balancing Research and Privacy;health related;health related;1
"J. Shin; Y. Song; J. Ahn; T. Lee; D. -H. Im";2023;Mobile social networking (MSN) is gaining significant popularity owing to location-based services (LBS) and personalized services. This direct location sharing increases the risk of infringing the user’s location privacy. In order to protect the location privacy of users, many studies on generating synthetic trajectory data using generative adversarial networks (GANs) are being conducted. However, GAN generates limited synthesis trajectory data due to mode collapse problem. In this paper, we propose a trajectory category auxiliary classifier-GAN (TCAC-GAN) that generates synthetic trajectory data with improved utility and anonymity by reducing mode collapse using ACGAN. In experiments, the performance of utility and anonymity of TCAC-GAN is compared with LSTM-TrajGAN.;TCAC-GAN: Synthetic Trajectory Generation Model Using Auxiliary Classifier Generative Adversarial Networks for Improved Protection of Trajectory Data;Not health related;Not health related;0
"T. Lee; C. S. Park; K. Nam; S. -S. Kim";2022;In exploratory data analysis, interactive latency can make a significant impact on the data exploration space and user productivity. To provide low latency for the aggregation query, approximate query processing can be considered as a possible alternative. In this paper, we describe the transformation rules for processing approximation queries. We present the preliminary experimental results on the performance of approximate query processing with synthetic data.;Query Transformation for Approximate Query Processing Using Synthetic Data from Deep Generative Models;Not health related;Not health related;0
"C. -T. Dang; V. -H. Tran; N. -H. -L. Le; C. -C. Huang";2023;One of the major challenges in developing deep learning models for automatic retail stores is the availability of diverse and accurate data. Achieving high accuracy requires a large amount of data that is specifically collected and labeled for the task at hand. However, this process is time-consuming and prone to potential errors during labeling, which can affect model training. To mitigate this challenge, data augmentation methods have been widely adopted to train models when data is limited. However, the effectiveness of data augmentation is not always guaranteed, as it still relies on the original data. To address this limitation, a new approach has been developed that utilizes synthetic data generated through 3D scanning of objects. However, the challenge lies in ensuring that this synthetic data accurately represents real-world data to maintain model accuracy. In this paper, we propose an approach that integrates adversarial learning into the YOLOv8 model and trains it using synthetic data. We then apply this method to automatic retail stores to demonstrate its efficacy in real-life scenarios. Experimental results on the AIC-22 dataset reveal significant improvements compared to the original YOLOv8 version. The proposed approach achieves around 6.7% increase in precision, a 7.1% improvement in recall, and a 10.3% enhancement in mAP while maintaining the same inference speed of 127 fps on the RTX 3090Ti.;From Synthetic To Real: Enhancing Deep Learning Models With Generative Adversarial Networks For Efficient Data Utilization In Automatic Retail Stores;Not health related;Not health related;0
"F. Regol; A. Kroon; M. Coates";2023;The machine learning community has mainly relied on real data to benchmark algorithms as it provides compelling evidence of model applicability. Evaluation on synthetic datasets can be a powerful tool to provide a better understanding of a model’s strengths, weaknesses and overall capabilities. Gaining these insights can be particularly important for generative modeling as the target quantity is completely unknown. Multiple issues related to the evaluation of generative models have been reported in the literature. We argue those problems can be avoided by an evaluation based on ground truth. General criticisms of synthetic experiments are that they are too simplified and not representative of practical scenarios. As such, our experimental setting is tailored to a realistic generative task. We focus on categorical data and introduce an appropriately scalable evaluation method. Our method involves tasking a generative model to learn a distribution in a high-dimensional setting. We then successively bin the large space to obtain smaller probability spaces where meaningful statistical tests can be applied. We consider increasingly large probability spaces, which correspond to increasingly difficult modeling tasks, and compare the generative models based on the highest task difficulty they can reach before being detected as being too far from the ground truth. We validate our evaluation procedure with synthetic experiments on both synthetic generative models and current state-of-the-art categorical generative models.;Evaluation of Categorical Generative Models - Bridging the Gap Between Real and Synthetic Data;Not health related;Not health related;0
"N. K. Nik Aznan; A. Atapour-Abarghouei; S. Bonner; J. D. Connolly; N. Al Moubayed; T. P. Breckon";2019;Despite significant recent progress in the area of Brain-Computer Interface (BCI), there are numerous shortcomings associated with collecting Electroencephalography (EEG) signals in real-world environments. These include, but are not limited to, subject and session data variance, long and arduous calibration processes and predictive generalisation issues across different subjects or sessions. This implies that many downstream applications, including Steady State Visual Evoked Potential (SSVEP) based classification systems, can suffer from a shortage of reliable data. Generating meaningful and realistic synthetic data can therefore be of significant value in circumventing this problem. We explore the use of modern neural-based generative models trained on a limited quantity of EEG data collected from different subjects to generate supplementary synthetic EEG signal vectors, subsequently utilised to train an SSVEP classifier. Extensive experimental analysis demonstrates the efficacy of our generated data, leading to improvements across a variety of evaluations, with the crucial task of cross-subject generalisation improving by over 35% with the use of such synthetic data.;Simulating Brain Signals: Creating Synthetic EEG Data via Neural-Based Generative Models for Improved SSVEP Classification;Not health related;Not health related;1
"P. Eigenschink; T. Reutterer; S. Vamosi; R. Vamosi; C. Sun; K. Kalcher";2023;A growing interest in synthetic data has stimulated the development and advancement of a large variety of deep generative models for a wide range of applications. However, as this research has progressed, its streams have become more specialized and disconnected from one another. This is why models for synthesizing text data for natural language processing cannot readily be compared to models for synthesizing health records anymore. To mitigate this isolation, we propose a data-driven evaluation framework for generative models for synthetic sequential data, an important and challenging sub-category of synthetic data, based on five high-level criteria: representativeness, novelty, realism, diversity and coherence of a synthetic data-set relative to the original data-set regardless of the models’ internal structures. The criteria reflect requirements different domains impose on synthetic data and allow model users to assess the quality of synthetic data across models. In a critical review of generative models for sequential data, we examine and compare the importance of each performance criterion in numerous domains. We find that realism and coherence are more important for synthetic data natural language, speech and audio processing tasks. At the same time, novelty and representativeness are more important for healthcare and mobility data. We also find that measurement of representativeness is often accomplished using statistical metrics, realism by using human judgement, and novelty using privacy tests.;Deep Generative Models for Synthetic Data: A Survey;health related;Not health related;1
"D. A. Rosa de Jesús; P. Mandal; T. Senjyu; S. Kamalasadan";2021;This paper contributes to the field of deep generative learning applied to solar photovoltaic (PV) synthetic data generation problems by exploring Deep Generative Model (DGM) that combines Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN), i.e., VAEGAN. We build upon knowledge in the area of deep learning to incorporate our Hybrid Deep Neural Network (HDNN), combining convolutional and Long Short-Term Memory (LSTM) layers at the encoding level for producing robust latent representations and subsequently high-quality synthetic PV data samples. The major advantage of these approaches is that it allows the DGMs to perform better feature extraction as well as to capture the historical trends in data effectively. The simulations on actual data acquired from a real PV system demonstrate the effectiveness of the DGMs to produce high-quality samples for multiple seasons of the year.;Unsupervised Hybrid Deep Generative Models for Photovoltaic Synthetic Data Generation;Not health related;Not health related;0
"K. Kusumoto; S. Murata";2023;Latent psychiatric and cognitive characteristics of individuals may be present in multiple data sources, such as subjective self-report measures and objective behavioral data. Therefore, integrative learning of multimodal data from individuals has the potential to contribute to understanding these characteristics. In this study, we propose a novel generative model and evaluate it with synthetically generated multimodal data. In multimodal data, each modality is considered to have low-dimensional latent representations, but these representations are not always fully shared with another modality. Therefore, we assume that latent representations of multimodal data consist of shared representations across all the modalities and private representations unique to each modality. Under this assumption, we develop a deep generative model that can learn to extract these different latent representations from both non-time-series and time-series data in an end-to-end manner. To evaluate our proposed model, we conducted simulation experiments with a synthetic multimodal dataset containing shared and private information. Experimental results on the inference of shared and private latent representations, as well as on self-modal and cross-modal reconstructions, demonstrate the potential of our proposed model to better understand latent psychiatric and cognitive characteristics in multimodal data.;Toward Understanding Psychiatric and Cognitive Characteristics: A Deep Generative Model for Extracting Shared and Private Representations and Its Evaluation with Synthetic Multimodal Data;health related;health related;0
"D. C. Lozoya; S. D’Alfonso; M. Conway";2023;Natural language generation (NLG) systems have proven to be effective tools to create domain-specific synthetic data. The mental health research field could benefit from data augmentation techniques, given the challenges associated with obtaining and utilizing protected health information. Yet, NLG systems are often trained using datasets that are biased with respect to key demographic factors such as ethnicity, religion, and gender. This can perpetuate and propagate systematic human biases that exist and ultimately lead to inequitable treatment for marginalized groups. In this research we studied and characterized biases present in the Generative Pre-trained Transformer 3 (GPT-3), which is an autoregressive language model that produces human-like text. The prompts used to generate text via GPT-3 were based on the Brief Cognitive Behavioral Therapy framework, and each prompt also specified to write the answer as a female or male patient. By controlling the sex distributions within our prompts, we observed the impact of each trait in the generated text. The synthetic data was analysed using the Linguistic Inquiry and Word Count software (LIWC-22) and ccLDA for cross-collection topic modeling. LIWC-22 results show that stereotypical competence features such as money, work, and cognition are more present in the male’s synthetic text, whereas warmth features such as home, feeling, and emotion are highly present in female’s generated data. The ccLDA results also associate competence features with males and warmth features with females.;Identifying Gender Bias in Generative Models for Mental Health Synthetic Data;health related;health related;0
M. Gupta;2023;In the rapidly evolving domain of healthcare, the availability of large and diverse datasets is paramount for the development and validation of advanced algorithms. However, the acquisition of such data is often hindered by privacy concerns, limited patient cohorts, and the inherent variability in medical conditions. This research delves into the potential of Deep Generative Models (DGMs) as a solution for synthetic data generation in healthcare applications. We present a comprehensive study of various DGM architectures, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Normalizing Flows, and their efficacy in generating high-fidelity medical images. Our experiments demonstrate that these models can produce synthetic data that retains the intricate features of real medical images while ensuring patient privacy. Furthermore, when used as augmentation in training diagnostic algorithms, the synthetic data generated by our proposed models showed a significant improvement in the generalization capabilities of these algorithms. This work not only underscores the potential of DGMs in addressing the data scarcity issue in healthcare but also paves the way for their broader application in medical research, training, and diagnostics.;Advances in AI: Employing Deep Generative Models for the Creation of Synthetic Healthcare Datasets to Improve Predictive Analytics;health related;health related;1
"C. Wirtz; M. Murglat; J. Tran; S. Krahl; A. Moser";2021;Threats to stability and security of the electric grid are not limited to one voltage level, but can form complex event cascades across multiple network levels. Furthermore, the structure of the local grid as well as the composition of customers have a significant influence on the characteristics of the incidents. High-resolution, interconnected grid models are therefore required for detailed investigations of current and future threats like temporary overvoltages. Here we present an approach to produce and combine synthetic high voltage (HV) and medium voltage (MV) grid models based on the actual supply task of German grids. The supply task can be inferred by open data approaches, e.g., high spatial satellite data and register databases. The resulting grids reproduce local characteristics and can form a basis for grid stability studies.;Modelling of synthetic high voltage networks based on open data and integration into a modular synthetic distribution grid generator;Not health related;Not health related;0
"E. Forte; M. Dossi; M. Pipan; R. R. Colucci";2014;We implemented a procedure to estimate the electromagnetic (EM) velocity using common offset ground penetrating radar (GPR) data. The technique is based on the inversion of reflection amplitudes to compute the series of reflection coefficients used to estimate the velocity in each interpreted layer. The proposed method recursively calculates the incident angles at any interface, taking into account the offset between antennas, and needs as input, in addition to the picked amplitudes values, a reference amplitude for each analysed GPR trace and a velocity value for the first (shallowest) layer. The latter two parameters can be estimated directly from the available data or can be better constrained by further dedicated GPR acquisitions or by additional direct measurements. We critically evaluated the performances for both synthetic and real data acquired with different antenna frequencies and we demonstrated that the new method can be applied in several real situations. Despite the necessary approximations and simplifying hypotheses, the velocity values calculated for each layer are consistent with direct information and with cross-validations obtained considering profiles acquired using different antennas and various path directions. Tests of the method on synthetic and real data sets show that the errors in the calculated velocity fields are quite low and comparable with more demanding velocity analysis techniques. The obtained EM velocity field is crucial in many processing steps, such as, for example, true amplitude recovery, depth conversion and imaging, and provide essential information to characterize the subsurface materials.;Velocity analysis from common offset GPR data inversion: theory and application to synthetic and real data;Not health related;Not health related;0
"A. Amoruso; S. Barba; L. Crescentini; A. Megna";2013;"The inversion of geodetic data to obtain earthquake parameters is often performed by assuming that the medium is isotropic, elastic and either homogeneous or layered. The layered medium often offers the best estimate of the structure of the crust; however, predicted displacements and observed data may differ beyond the measurement errors. The slip distribution on the fault plane is usually obtained by dividing the best uniform slipping fault into an arbitrarily large number of subfaults and minimizing a cost function that includes a smoothness (Laplacian) term and a data misfit term. The smoothing factor controls the trade-off between the smoothness and the goodness-of-fit. The main focus of this work is the determination and effect of the smoothing parameter. We conducted several inversion tests of noiseless synthetic surface displacement due to faults embedded in media with properties consistent with the geology of the Central Apennines (Italy), where the 2009 April 6, L'Aquila earthquake occurred. We used the following three-step procedure: (i) global optimization with no smoothness constraint for a fault divided into a small number of equally sized equal-rake subfaults; (ii) selection of the best fault parameters using information criteria and (iii) evaluation of the slip amplitude distribution on an expanded fault after choosing the smoothing factor from trade-off curves or from cross-validation for different numbers of subfaults. We show that all of the fault features obtained by the inversion procedure, including the slip distribution, agree with those (‘true’) used in the forward modelling when the data cover the majority of the displacement field. Notable departures from the true slip distribution occur when a suboptimal smoothing factor (obtained from the trade-off curves or cross-validation) is used. If different crustal stratifications are used in the inversions, the best results are obtained for the stratification that is the closest to the true crustal structure. When we use more realistic GPS distributions, prominent spurious slip patches can be obtained. Modellers should use synthetic tests and sensitivity analyses as an initial step in the data inversion for source parameters.";Inversion of synthetic geodetic data for dip-slip faults: clues to the effects of lateral heterogeneities and data distribution in geological environments typical of the Apennines (Italy);Not health related;Not health related;0
"H. Chauris; A. M. Noble";2001;The quality of the migration/inversion in seismic reflection is directly related to the quality of the velocity macro model. We present here an extension of the differential semblance optimization method (DSO) for 2-D velocity field estimation. DSO evaluates via local measurements (horizontal derivatives) how flat events in common-image gathers are. Its major advantage with respect to the usual cost functions used in reflection seismic inverse problems is that it is—at least in the 1-D case—unimodal and thus allows a local (gradient) optimization process. Extension of DSO to three dimensions in real cases involving a large number of inverted parameters thus appears much more feasible, because convergence might not require a random search process (global optimization).Our differential semblance function directly measures the quality of the common-image gathers in the depth-migrated domain and does not involve de-migration. An example of inversion on a 2-D synthetic data set shows the ability of DSO to handle 2-D media with local optimization algorithms. The horizontal derivatives have to be carefully calculated for the inversion process. However, the computation of only a few common-image gathers is sufficient for a stable inversion. As a Kirchhoff scheme is used for migration, this undersampling largely reduces the computational cost.Finally, we present an application to a real North Sea marine data set. We prove with this example that DSO can provide velocity models for typical 2-D acquisition that improve the quality of the final pre-stack depth images when compared to the quality of images migrated with a velocity model obtained by a classical NMO/DMO analysis. Whilst random noise is not a real difficulty for DSO, coherent noise, however, has to be carefully eliminated before or during inversion for the success of the velocity estimation.;Two-dimensional velocity macro model estimation from seismic reflection data by local differential semblance optimization: applications to synthetic and real data sets;Not health related;Not health related;0
M. Campbell;2019;"Artificial intelligence (AI) empowers countless applications by consuming data and spotting patterns; however, this paradigm is being flipped. AI is now being used to produce data from patterns, and the resulting ""synthetic data"" are rapidly enabling a vast range of groundbreaking solutions.";Synthetic Data: How AI Is Transitioning From Data Consumer to Data Producer... and Why That's Important;Not health related;Not health related;0
"D. A. Carpenter; D. E. Robinson; P. L. Ho; D. C. C. Martin; P. Isaacs";1993;Body-wall refraction has been implicated in degrading and distorting medical ultrasonic images. Even with sophisticated image-forming systems refraction imposes a limitation on image quality. A forward propagation technique has been developed to correct for the refraction at the fat/muscle boundaries in the subcutaneous tissues. A synthetic-aperture scanning approach has been used to evaluate the technique. This permits the procedure to be carried out on a single set of ultrasonic data from a live patient, making rescanning, with its associated effects of tissue movement, unnecessary;Body wall aberration correction in medical ultrasonic images using synthetic-aperture data;health related;health related;1
"D. Moghadas; F. André; E. C. Slob; H. Vereecken; S. Lambot";2010;A joint analysis of full-waveform information content in ground penetrating radar (GPR) and electromagnetic induction (EMI) synthetic data was investigated to reconstruct the electrical properties of multilayered media. The GPR and EMI systems operate in zero-offset, off-ground mode and are designed using vector network analyser technology. The inverse problem is formulated in the least-squares sense. We compared four approaches for GPR and EMI data fusion. The two first techniques consisted of defining a single objective function, applying different weighting methods. As a first approach, we weighted the EMI and GPR data using the inverse of the data variance. The ideal point method was also employed as a second weighting scenario. The third approach is the naive Bayesian method and the fourth technique corresponds to GPR–EMI and EMI–GPR sequential inversions. Synthetic GPR and EMI data were generated for the particular case of a two-layered medium. Analysis of the objective function response surfaces from the two first approaches demonstrated the benefit of combining the two sources of information. However, due to the variations of the GPR and EMI model sensitivities with respect to the medium electrical properties, the formulation of an optimal objective function based on the weighting methods is not straightforward. While the Bayesian method relies on assumptions with respect to the statistical distribution of the parameters, it may constitute a relevant alternative for GPR and EMI data fusion. Sequential inversions of different configurations for a two layered medium show that in the case of high conductivity or permittivity for the first layer, the inversion scheme can not fully retrieve the soil hydrogeophysical parameters. But in the case of low permittivity and conductivity for the first layer, GPR–EMI inversion provides proper estimation of values compared to the EMI–GPR inversion.;Joint full-waveform analysis of off-ground zero-offset ground penetrating radar and electromagnetic induction synthetic data for estimating soil electrical properties;Not health related;Not health related;0
W. G. Rees;1988;;Synthetic Aperture Radar Data Over Terrestrial Ice;Not health related;Not health related;0
R. C. Robertson;1988;The magnetotelluric (MT) method, an electromagnetic sounding technique used in geophysical exploration, is considered. The MT method is based upon the measurement of the naturally occurring electromagnetic field at the surface of the Earth and the interpretation of the surface electromagnetic field to infer subsurface structure. When the region of the Earth being surveyed by the MT method has strong lateral heterogeneities, information about the deep structure of the Earth is obscured by near surface features, leading to significant interpretational problems. A low-pass wavenumber filtering method used to deal with this problem is described and evaluated by comparing the calculated apparent resistivity of a model of the Earth consisting of a two-dimensional heterogeneous layer on the surface of a horizontally stratified Earth with the wavenumber filtered apparent resistivity. Its effectiveness is confirmed.<>;The effect of wavenumber filtering on synthetic two-dimensional magnetotelluric data;Not health related;Not health related;0
"W. Siripunvaraporn; G. Egbert; M. Uyeshima";2005;Traditional methods for interpretation of magnetotelluric (MT) profile data are based on 2-D inversion, under the assumption that 3-D complications in the data can be treated as ‘geological noise’. We show with synthetic models that fitting 3-D data with a 2-D inversion can result in spurious features, especially if transverse electric (TE) data are used. Inversion of a single profile of MT data with a 3-D algorithm results in significantly more realistic images of structure beneath the data profile, and also allows some resolution of nearby off-profile structure. We also consider the importance of including the on-diagonal impedance tensor terms, Zxx and Zyy, in the inversion. In synthetic test cases, fitting these diagonals improves the accuracy of images of off-profile structure, particularly near the edge of a conductive feature.;Interpretation of two-dimensional magnetotelluric profile data with three-dimensional inversion: synthetic examples;Not health related;Not health related;0
R. C. Robertson;1988;;Interpretation Of Synthetic Three-dimensional Magnetoelluric Data;Not health related;Not health related;0
"S. Khaleghian; T. Kramer; A. Everett; A. Kiarbech; N. Hughes; T. Eltoft; A. Marinoni";2021;In this paper, we explore the potential of deep learning (DL) networks to produce reliable sea ice classification by analyzing data collected by synthetic aperture radar (SAR) sensors over polar regions. Taking advantage of their ability to extract features from complex datasets, DL schemes can be used to perform large scale data investigation, so to help manual interpretation conducted by experts in sea ice charting services. We highlighted the ability of different DL settings as well as their limits (mainly associated with scarce training data). Experimental results show the validation accuracy and the inference potential of three DL networks.;Synthetic aperture radar data analysis by deep learning for automatic sea ice classification;Not health related;Not health related;0
"X. Yuan; R. Kind; X. Li; R. Wang";2006;Recently, the S receiver function method has been successfully developed to identify upper mantle interfaces. S receiver functions have the advantage of being free of S-wave multiple reflections and can be more suitable than P receiver functions for studying mantle lithosphere. However, because of specific ray geometry and interference of diverse phases, the S receiver function method has some technical difficulties and limitations. We use synthetic seismograms to demonstrate the feasibility and limitations of S receiver functions for studying mantle structures. Full-wavefield seismograms were calculated using the reflectivity method and processed to generate synthetic S receiver functions for S, SKS and ScS waves. Results show that S receiver functions can be obtained from waveforms of S, SKS and ScS waves. The synthetic S receiver functions for these incident waves show S-to-P converted phases at all discontinuities in the crust and upper mantle. Useful ranges of epicentral distances for calculation of S receiver functions are: 55°–85° for S, >85° for SKS and 50°–75° for ScS waves. We apply both the S and P receiver function methods to data recorded at broadband station YKW3 in Northwest Canada. The study shows that there is significant agreement among different receiver function methods, and demonstrates the usefulness of S receiver functions for imaging the mantle lithosphere.;The S receiver functions: synthetics and data example;Not health related;Not health related;0
"T. Bolz; U. Soergel";2021;The extraction of structure lines such as breaking edges is done using a variety of different methods and algorithms. As SAR (Synthetic Aperture Radar) data gain more importance in Remote Sensing, the need for edge detectors capable to deal with this type of input increases. In this paper different ways to extract the information from the SAR data are discussed. Therefore 2D information as well as 3D DEM information is used. To investigate the usability of the methods airborne SAR data in S- and X-Band is used.;Detection of Structure Lines in 2D and 3D Synthetic Aperture Radar Data;Not health related;Not health related;0
Shih-Tseng Wu;1988;;Analysis Of Synthetic Aperture Radar Data For The Study Of Deciduous Forest;Not health related;Not health related;0
"C. Lezcano; M. Arias";2019;This paper proposes three different data generators, tailored to transactional datasets, based on existing itemset-based generative models. All these generators are intuitive and easy to implement and show satisfactory performance. The quality of each generator is assessed by means of three different methods that capture how well the original dataset structure is preserved.;Synthetic Dataset Generation with Itemset-Based Generative Models;Not health related;Not health related;0
"D. Upadhyay; Q. Luo; J. Manero; M. Zaman; S. Sampalli";2023;The demand for securing SCADA (Supervisory Control and Data Acquisition)-based power grid systems from cyber-attacks has been increasing significantly in the last few years. Current research trends widely adopt Machine Learning (ML) techniques to prevent attacks against such critical infrastructure. However, the efficiency of these techniques largely depends upon the availability of large datasets. Acquiring large data from such critical systems is not always feasible and this has inhibited the research progress in the development of advanced ML algorithms that can make a notable difference in the prediction of malicious events. Thus, there is a strong need for generating large synthetic yet realistic datasets from existing small datasets. This paper presents a comparative analysis of tabular Generative Adversarial Network (GAN) models for the generation and validation of synthetic datasets from existing datasets of power grids. Moreover, the synthetic datasets are validated using statistical analysis, and machine learning efficacy. These synthetic datasets open opportunities for the research community to explore advanced machine learning and deep learning methodologies for the protection of industrial systems.;Comparative Analysis of Tabular Generative Adversarial Network (GAN) Models for Generation and Validation of Power Grid Synthetic Datasets;Not health related;Not health related;0
"Zhaoshun Wang; Guicheng Shen; Jinjin Huang";2010;A new kind of retrieval software architecture about structured data and Non-structured data was proposed in this paper. In the past, because of the great differences between them, we have to use different ways to deal with their data retrieval. We have been using SQL statements in the structured database search, and using index search in the non-structured data retrieval. But regardless of which search method was taken, it will bring the incomplete problem. Now the architecture resolve the problem.;Synthetic retrieval technology for structured data and Non-structured data;Not health related;Not health related;0
"N. Cogan; F. Bao; R. Paus; A. Dobreva";2021;The goal of patient-specific treatment of diseases requires a connection between clinical observations with models that are able to accurately predict the disease progression. Even when realistic models are available, it is very difficult to parameterize them and often parameter estimates that are made using early time course data prove to be highly inaccurate. Inaccuracies can cause different predictions, especially when the progression depends sensitively on the parameters. In this study, we apply a Bayesian data assimilation method, where the data are incorporated sequentially, to a model of the autoimmune disease alopecia areata that is characterized by distinct spatial patterns of hair loss. Using synthetic data as simulated clinical observations, we show that our method is relatively robust with respect to variations in parameter estimates. Moreover, we compare convergence rates for parameters with different sensitivities, varying observational times and varying levels of noise. We find that this method works better for sparse observations, sensitive parameters and noisy observations. Taken together, we find that our data assimilation, in conjunction with our biologically inspired model, provides directions for individualized diagnosis and treatments.;Data assimilation of synthetic data as a novel strategy for predicting disease progression in alopecia areata;health related;health related;1
"Y. E. Kurniawati; A. E. Permanasari; S. Fauziati";2018;"Annually about 1,500 cases of cervical cancer are found in Indonesia, which made Indonesia as the country with the highest number of cervical cancer cases in the world. Cervical cancer screening and HPV testing are done with a Pap smear test. However, this examination requires a lot of time, costly and highly susceptible bias of the observer during the process of investigation and analysis. To overcome these problems, several studies have modeled the machine learning with a variety of approaches have been made. However, these studies are constrained by the limitation of the data amounts and the imbalanced data that caused by the different ratio of each case. This can lead to errors in the classification of the minority due to the tendency of the classification results that focus on the majority class. This study addressed the handling imbalance data on classification of cases Pap test results using the method of over-sampling. ADASYN-N and ADASYN-KNN algorithms were proposed as a development of ADASYN algorithm to handle datasets with nominal data types. This study included SMOTE-N algorithm to deal with the problem as comparison algorithm. As the results, ADASYN-KNN with the preference “0” gave the highest accuracy, precision, recall and f-score of 95.38%; 95.583%; 95.383%; and 95.283%. The highest ROC area value was obtained with the ADASYN-KNN with preference “1” of 99.183%.";Adaptive Synthetic-Nominal (ADASYN-N) and Adaptive Synthetic-KNN (ADASYN-KNN) for Multiclass Imbalance Learning on Laboratory Test Data;Not health related;Not health related;0
"J. Eno; C. W. Thompson";2008;Synthetic data sets can be useful in a variety of situations, including repeatable regression testing and providing realistic - but not real - data to third parties for testing new software. Researchers, engineers, and software developers can test against a safe data set without affecting or even accessing the original data, insulating them from privacy and security concerns as well as letting them generate larger data sets than would be available using only real data. Practitioners use data mining technology to discover patterns in real data sets that aren't apparent at the outset. This article explores how to combine information derived from data mining applications with the descriptive ability of synthetic data generation software. Our goal is to demonstrate that at least some data mining techniques (in particular, a decision tree) can discover patterns that we can then use to inverse map into synthetic data sets. These synthetic data sets can be of any size and will faithfully exhibit the same (decision tree) patterns. Our work builds on two technologies: synthetic data definition language and predictive model markup language.;Generating Synthetic Data to Match Data Mining Patterns;Not health related;Not health related;0
"D. R. Jeske; P. J. Lin; C. Rendon; R. Xiao; B. Samadi";2006;Recently, due to commercial success of data mining tools, there has been much attention to extracting hidden information from large databases to predict security problems and terrorist threats. The security applications are somewhat more complicated than commercial applications due to (i) lack of sufficient specific knowledge on what to look for, (ii) R&D labs developing these tools are not able to easily obtain sensitive information due to security, privacy or cost issues. Tools developed for security applications require substantially more testing and revisions in order to prevent costly errors. This paper describes a platform for the generation of realistic synthetic data that can facilitate the development and testing of data mining tools. The original applications for this platform were people information and credit card transaction data sets. In this paper, we introduce a new shipping container application that can support the testing of data mining tools developed for port security;Synthetic Data Generation Capabilties for Testing Data Mining Tools;Not health related;Not health related;0
"Z. Kreiser; B. Killough; S. R. Rizvi";2018;The detection of inland water bodies from Synthetic Aperture Radar (SAR) data provides a great advantage over water detection with optical data, since SAR imaging is not impeded by cloud cover. Traditional methods of detecting water from SAR data involves using thresholding methods that can be labor intensive and imprecise. This paper describes Water Across Synthetic Aperture Radar Data (WASARD): a method of water detection from SAR data which automates and simplifies the thresholding process using machine learning on training data created from Geoscience Australia's WOFS algorithm. Of the machine learning models tested, the Linear Support Vector Machine was determined to be optimal, with the option of training using solely the VH polarization or a combination of the VH and VV polarizations. WASARD was able to identify water in the target area with a correlation of 97% with WOFS.;Water Across Synthetic Aperture Radar Data (WASARD): SAR Water Body Classification for the Open Data Cube;Not health related;Not health related;0
"O. Mohamed Salim; H. Taher Dorrah; M. Adel El-Kahawy";2018;"Renewable energy resources have a great influence on country's future planning. However, building such investments needs reliable and accurate studies based on huge information and historical data, that might be considered a great challenge. Thus, generating some form of artificial or what they call it “synthetic” patterns that give the same hidden information and characteristics as the original records are so important. In this paper real wind speed data is gathered for a very promising candidate location. This dataset is used extensively to generate a synthetic data for planning/feasibility purposes for wind-farm project planning. Moreover, most commonly considered stochastic techniques were utilized to either model, extract all main probabilistic features of original data and hence; generate the required synthetic data. In addition, this paper proposed a new stochastic model that could generate synthetic wind data extremely has the same features as the original records.";A Novel Algorithm to Generate Synthetic Data for Continuous-State Stationary Stochastic Process (Wind Data Application);Not health related;Not health related;0
"S. Kusano; M. Sato; Y. Yokota";2011;In this paper, we introduce interpolation for a detailed observation of synthetic aperture radar (SAR) data. The interpolation is done by zero-padding in the frequency domain. By comparing the raw SAR data and the interpolated data, for example, it is observed that the raw span at a corner reflector we deployed is 0.75 times smaller than its real values. Also, we propose a resampling method applied to the interpolated data to keep data amount same as raw data. The proposed method resamples points where peaks of the span due to coherent targets exist. As for pixels where the peak does not exist, points with the smallest polarimetric entropy are selected. The proposed method is compared to raw data and the resampled data by the maximum filter. It is confirmed that the proposed data emphasizes the characteristics of the raw data. Also, its blurring effect is not as much as that in the resampled data by the maximum filter. This technique is quite basic, however, the effect is not negligible.;Data interpolation and resampling for a synthetic aperture radar data;Not health related;Not health related;0
"A. Kiran; S. S. Kumar";2023;Synthetic data has emerged as an acceptable solution in machine learning that overcomes the constraints of data availability due to data privacy restrictions. Other major challenge with machine learning is dealing with imbalanced dataset. Several techniques exist to deal with the data imbalance, however, the problem continues to exist when using synthetic data generators dealing with highly imbalanced datasets. Generative Adversarial Network is already proven to be an excellent model to generate synthetic data, especially for high-dimensional datasets. There are other deep learning models that use Variational Autoencoders and Recurrent Neural Networks which are also being explored. To understand how these generators perform when presented situations dealing with highly imbalanced datasets, we experimentally evaluate two deep-learning synthetic data generators, one is based on Generative Adversarial Network (CTGAN) and the other is on Variational Auto Encoder (TVAE). We assess how each of these performs when presented with two datasets of distinct characteristics. The datasets used are high dimensional, highly imbalanced tabular data with one dataset having 19.3% minority class and the other having only 5.68% of minority class. Our test results find that TVAE fails to generate minority data when the minority class is very small in number.;A Comparative Analysis of GAN and VAE based Synthetic Data Generators for High Dimensional, Imbalanced Tabular data;Not health related;Not health related;0
"W. Wang; L. Zhang; C. -M. Pun; J. -C. Xie";2023;Face recognition is one of the most precise and straightforward methods to establish individual identity, and is important in our daily life. To solve the issues of privacy, bias, and collection difficulty caused by face recognition relying heavily on collecting a huge number of real face images from the Internet, a seemingly promising idea is to employ GAN-generated synthetic faces as the training data. However, there are obvious surface gaps and domain gaps between real and synthetic face images, and cannot be replaced directly. In this paper, we attempt to boost face recognition simultaneously using synthetic data and limited real data. Specifically, we first design an augmented space for auto augmentation methods to augment synthetic images to alleviate the surface gap, then propose to disentangle the underlying style distributions through dual batch normalization layers so that both synthetic and real images can be learned jointly by convolution layers without mixing across domains. Extensive experiments demonstrate our method can achieve better results than training with large quantities of real data.;Boosting Face Recognition Performance with Synthetic Data and Limited Real Data;Not health related;Not health related;0
"F. -C. Chen; Y. -W. Liu";2023;Labeling onset and offset time and class information for audio files has always been a laborious task for sound event detection (SED) research. Therefore, it would be appealing to consider using strongly-labeled synthetic data and a large amount of unlabeled data when training a SED model. To utilize these data, previous research showed that mean-teacher based semi-supervised consistency training strategies could work with adversarial domain adaptation. However, the effectiveness has only been demonstrated in the domestic environment. In this work, we expand the scope of application to bird sound event detection in the natural environment. To achieve this, we established a synthetic strongly-labeled bird sound dataset consisting of 20 species. Then, we tuned the consistency training strategies and domain adaptation techniques that have been proven effective in the domestic environment to this new dataset. Our evaluation on the Eastern North American bird sound dataset resulted in an F1 score of >30%, which approaches the upper-bound performance that could be obtained when strongly-labeled data are assumed to be available. Thus, the strategies exhibit potential for advancing research in both indoor and outdoor SED.;Utilizing Unlabeled Data and Synthetic Data for Bird Sound Detection: Consistency Training, Mean Teacher, and Domain Adaptation Techniques;Not health related;Not health related;0
"A. Rauland; D. Merhof";2023;Several studies have investigated the possibility of predicting the fiber orientation distribution function (fODF), which is obtained using the very accurate multi-shell multi-tissue constrained spherical deconvolution (MT-CSD) from single-shell or low angular resolution multi-shell diffusion magnetic resonance imaging (dMRI) data using deep learning.While all these approaches show promising results, the vast majority have in common that they require multi-shell high angular resolution diffusion imaging (HARDI) data to calculate the ground truth fODF using the MT-CSD for training their networks. This data, however, is difficult to acquire in a clinical context and it is yet unclear how well networks trained on data acquired on a certain scanner with a certain protocol would generalize to different data.In this work, we address these shortcomings and present a method that can estimate an accurate fODF from single-shell diffusion data without the need for multi-shell data for training. This is achieved by generating patient-, acquisition-and scanner-specific synthetic single voxel diffusion signals with a known ground truth fODF from single shell data that can be used to train the neural network. The trained network will then be applied to the real patient data to predict the fODF with a quality standard close to that of an MT-CSD and the ability to determine if white matter (WM) is present in the underlying voxel.The approach is evaluated on 20 subjects from the Human Connectome Project (HCP) for all three shells (b=1000, 2000, 3000 s/mm2). When comparing both this approach and a single shell constrained spherical deconvolution (CSD) to the results of the MT-CSD, this work outperforms the single shell CSD in terms of the angular correlation coefficient and root mean squared error on all three shells.;Using Synthetic Training Data in Neural Networks for the Estimation of Fiber Orientation Distribution Functions from Single Shell Data;Not health related;Not health related;0
"S. Kharod; K. Patel; P. Patel; H. S. Srivastava";2021;Synthetic Aperture Radar(SAR) is being actively used in the fields of land cover classification, terrain analysis, calamity inspection, etc. In this study we tried to classify land covers on the SAR dataset using a different approach called the decision rule based approach. The decision rule based approach was carried out by studying the various scattering mechanisms exhibited by different land covers and then devising a decision rule algorithm to classify the land covers for the SAR data split over the duration of a year.;Scattering mechanism based Decision Rule Classifier for Land Cover Classification using multi polarized Synthetic Aperture Data(SAR) data;Not health related;Not health related;0
"A. Nasybullin; V. Maksimenko; S. Kurkin";2022;In this paper, we discuss a Machine Learning pipeline for the classification of EEG data. We propose a combination of synthetic data generation, long short-term memory artificial neural network (LSTM), and fine-tuning to solve classification problems for experiments with implicit visual stimuli, such as the Necker cube with different levels of ambiguity. The developed approach increased the quality of the classification model of raw EEG data.;How Long short-term memory artificial neural network, synthetic data, and fine-tuning improve the classification of raw EEG data;health related;health related;0
"H. Caliskan; O. F. Yayla; Y. Genc";2023;Creating synthetic data is a practical approach to provide a solution for privacy and scalability issues of data in machine learning applications. Data science in finance is encountering an increasing need for anonymized data for the same reasons: strict privacy regulations, and need for balanced data for many modeling tasks. In this work, we address three problems in machine learning applications for financial application and offer solutions. First, we successfully generate synthetic data using a set of actual credit loan offer data by training a custom variational autoencoder and a GAN model. Second, we present a comparative analysis of these models using statistical methods. As far as we know, there are no golden standards for the assessment of synthetically generated data for finance applications. Lastly, we introduce a performance comparison method to evaluate synthetically generated data. Our experimental analysis has shown that the proposed methods achieve a satisfactory performance with the generated data in various machine learning models.;A Comparative Analysis of Synthetic Data Generation with VAE and CTGAN Models on Financial Credit Loan Offer Data;Not health related;Not health related;0
"F. Liu; Z. Wang";2010;"Emergency Response Mechanism (ERM) for remote sensed image sequential deficiency is extremely crucial and the concerned researches are in urgent need. STARFM (Spatial and Temporal Adaptive Reflectance Fusion Model, STARFM) is such a fusion method for image sequences prediction, which is a sound theoretical and valid applicable approach. Gao et al. have successfully produced synthetic Landsat imagery with the corresponding approach, and Hiker et al. have applied it in coniferous study. However, STARFM algorithm has limit modeling power, for it has not fully considered disturbance events occurring within the observation process, meanwhile this paper does not supply an accuracy of synthetic images with variance indicators. Currently, it is still a pending issue addressing the stated estimation and uncertainty existence of sensor measurement values. In this paper, it is assumed that multiple sources data cross-use could minimize the observation error by effective blending technology. Under this assumption, we propose to modify the existing STARFM model, to optimize the predicted imagery through data assimilation theory, and the resulting synthesis image after iterations will be close to the true image. This paper proposes a practical approach of dynamic assimilated STARFM algorithm (DASTARFM), and supplies an application of crop yield estimation by remote sensing technology. In this paper, we mainly: (1)discuss the limits of the current concerned image synthetic researches; and (2) give a description of the practical approach which is featured as adaptive to the dynamic optimization, namely the dynamic optimized STARFM. Besides, (3) based on the new model-DASTARFM, we give the concrete implementation example to present the optimal estimated achievement of this model and illustrate its benefits over the typical algorithm according to the prior and post variances.";Synthetic Landsat data through data assimilation for winter wheat yield estimation;Not health related;Not health related;0
"D. R. Alattal; Z. Wang; P. Myles; A. Tucker";2023;Synthetic Individual-Level Geospatial Data (SIL-GSD) offers a number of advantages in Spatial Epidemiology when compared to census data or surveys conducted on regional or global levels. The use of SILGSD could bring a new dimension to the study of the patterns and causes of diseases in a particular location while minimizing the risk of patient identity disclosure, especially for rare conditions. Additionally, it could help in building and monitoring regional machine learning models, improving the quality and effectiveness of local healthcare services. Finally, SILGSD could help in controlling the spread and causes of diseases by studying disease movement across areas through the travelling patterns of populations. To our knowledge, no synthetic health records data containing synthesised geographic locations for patients has been published for research purposes so far. Therefore, in this paper we explore generating SILGSD by allocating synthetic patients to general practices (healthcare providers) in the UK using the demographics and prevalence of health conditions in each practice. The assigned general practice locations can be used as proxies for patient locations due to people being registered to their nearest practice from home. We use high-fidelity synthetic primary care patients from the Clinical Practice Research Datalink (CPRD) and allocate them to England's general practices (GPs), using the publicly available GP health conditions statistics from the Quality and Outcomes Framework (QOF). The allocation relies on similarities between patients in different locations without using real location information for the patients. We demonstrate that the Allocation Data is able to accurately mimic the real health conditions distribution in the general practices and also preserves the underlying distribution of the original primary care patients data from CPRD (Gold Standard).;Creating Synthetic Geospatial Patient Data to Mimic Real Data Whilst Preserving Privacy: *2022 35th International Symposium on Computer-Based Medical Systems (CBMS);health related;health related;1
"M. A. Ferrer; M. Diaz-Cabrera; A. Morales; J. Galbally; M. Gomez-Barrero";2013;"A novel method for the generation of synthetic offline signatures is presented. The proposed algorithm follows a two steps scheme: first, the raw synthetic dynamic functions of the synthetic signature are generated; second, several ink and paper models are applied to transform the on-line data to realistic static signatures. The novel approach is validated using four different publicly available databases both real and synthetic. The experimental protocol includes the comparison of both types of signatures in terms of: i) performance evaluation of two competitive and totally different verification systems; and ii) visual appearance according to human observers. The experimental results show the high similarity existing between synthetically generated and humanly produced samples, and the potential of the proposed method for the study of the signature trait.";Realistic synthetic off-line signature generation based on synthetic on-line data;Not health related;Not health related;0
"K. A. H_gda; S. Jacobsen; H. E. Krogstad; G. Engen";1993;The well-known along-track resolution loss in synthetic aperture radar (SAR) ocean wave image spectra is investigated comparing simulations based on Hasselmann's nonlinear integral transform and measurements from the Norwegian Continental Shelf Experiment 1988 experiment. In the literature the resolution loss has often been modeled as a low-pass filter process, described by the rms azimuth shift width _x, acting within a quasi-linear SAR transformation. Estimates from real data of _x, as a function the range-to-platform velocity (R/V) and incidence angle, are compared to Hasselmann's new nonlinear spectral transformation and the widely used quasi-linear model. Simulations correlated with real wave data show that the quasi-linear model, with contributions to _x from the entire spectrum, overestimates _x by roughly 30–40%. The conformity between Hasselmann's model and real data is excellent, however. The numerics also indicate that the degree of nonlinearity in Hasselmann's transform is explicitly related to the surface truth parameters’ significant wave height and peak wavelength. Furthermore, the spectral bandwidth, including the subresolution part of the ocean wave spectrum, seems to be of minor importance. It is also shown that the resultant smearing (due to imaging nonlinearities) cannot explicitly be separated from the coherent linear velocity bunching part of the transform. This point is especially discussed since it has led previously to some dissension regarding which ocean spectral components are most essential in the smearing process.;Azimuth smearing in ocean–synthetic aperture radar image spectra: A study of Hasselmann's closed-form transformation based on Norwegian Continental Shelf Experiment 1988 synthetic aperture radar data;Not health related;Not health related;0
"M. Frid-Adar; E. Klang; M. Amitai; J. Goldberger; H. Greenspan";2018;In this paper, we present a data augmentation method that generates synthetic medical images using Generative Adversarial Networks (GANs). We propose a training scheme that first uses classical data augmentation to enlarge the training set and then further enlarges the data size and its diversity by applying GAN techniques for synthetic data augmentation. Our method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). The classification performance using only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. By adding the synthetic data augmentation the results significantly increased to 85.7% sensitivity and 92.4% specificity.;Synthetic data augmentation using GAN for improved liver lesion classification;health related;Not health related;1
"S. Bhattacharya; T. Blumensath; B. Mulgrew; M. Davies";2007;Synthetic Aperture Radar (SAR) is active and coherent microwave high resolution imaging system, which has the capability to image in all weather and day-night conditions. SAR transmits chirp signals and the received echoes are sampled into In-phase (I) and Quadrature (Q) components, generally referred to as raw SAR data. The various modes of SAR coupled with the high resolution and wide swath requirements result in a huge amount of data, which will easily exceed the on-board storage and downlink bandwidth of a satellite. This paper addresses the compression of the raw SAR data by sampling the signal below Nyquist rate using ideas from Compressed Sensing (CS). Due to the low computational resources available onboard satellite, the idea is to use a simple encoder, with a 2D FFT and a random sampler. Decoding is then based on convex optimization or uses greedy algorithms such as Orthogonal Matching Pursuit (OMP).;Fast Encoding of Synthetic Aperture Radar Raw Data using Compressed Sensing;Not health related;Not health related;0
"M. Alzantot; S. Chakraborty; M. Srivastava";2017;"Our ability to synthesize sensory data that preserves specific statistical properties of the real data has had tremendous implications on data privacy and big data analytics. The synthetic data can be used as a substitute for selective real data segments - that are sensitive to the user - thus protecting privacy and resulting in improved analytics. However, increasingly adversarial roles taken by data recipients such as mobile apps, or other cloud-based analytics services, mandate that the synthetic data, in addition to preserving statistical properties, should also be “difficult” to distinguish from the real data. Typically, visual inspection has been used as a test to distinguish between datasets. But more recently, sophisticated classifier models (discriminators), corresponding to a set of events, have also been employed to distinguish between synthesized and real data. The model operates on both datasets and the respective event outputs are compared for consistency. Prior work on data synthesis have often focussed on classifiers that are built for features explicitly preserved by the synthetic data. This suggests that an adversary can build classifiers that can exploit a potentially disjoint set of features for differentiating between the two datasets. In this paper, we take a step towards generating sensory data that can pass a deep learning based discriminator model test, and make two specific contributions: first, we present a deep learning based architecture for synthesizing sensory data. This architecture comprises of a generator model, which is a stack of multiple Long-Short-Term-Memory (LSTM) networks and a Mixture Density Network (MDN); second, we use another LSTM network based discriminator model for distinguishing between the true and the synthesized data. Using a dataset of accelerometer traces, collected using smartphones of users doing their daily activities, we show that the deep learning based discriminator model can only distinguish between the real and synthesized traces with an accuracy in the neighborhood of 50%.";SenseGen: A deep learning architecture for synthetic sensor data generation;Not health related;Not health related;0
"F. Wang; Y. Zhuang; H. Gu; H. Hu";2019;"The recent success of deep learning in 3-D data analysis relies upon the availability of large annotated data sets. However, creating 3-D data sets with point-level labels are extremely challenging and require a huge amount of human efforts. This paper presents a novel open-sourced method to extract light detection and ranging point clouds with ground truth annotations from a simulator automatically. The virtual sensor can be configured to simulate various real devices, from 2-D laser scanners to 3-D real-time sensors. Experiments are conducted to show that using additional synthetic data for training can: 1) achieve a visible performance boost in accuracy; 2) reduce the amount of manually labeled real-world data; and 3) help to improve the generalization performance across data sets.";Automatic Generation of Synthetic LiDAR Point Clouds for 3-D Data Analysis;Not health related;Not health related;0
E. J. M. Rignot;2000;Electromagnetic waves traveling through the ionosphere undergo a Faraday rotation of the polarization vector, which modifies the polarization and phase characteristics of the electromagnetic signal. Using L-band (/spl lambda/=24 cm), polarimetric synthetic aperture radar (SAR) data from the shuttle imaging radar C (SIR-C) acquired in 1994, the author simulates the effect of a change in the Faraday rotation angle /spl psi/ on spaceborne interferometric and polarimetric data. In one experiment, it was found that phase coherence is reduced by up to 33% when /spl psi/ changes between successive data acquisitions. If /spl psi/ changes by more than 40/spl deg/, a differential phase signal, which varies from field to field, appears in the interferogram and impairs the mapping of surface topography and/or the detection of ground deformation. This signal is caused by phase differences between horizontal-polarized and vertical-polarized radar signals from intermediate levels of vegetation canopy, similar to the phase difference measured between H-polarized and V-polarized signals on a single date. In a second experiment, data from the Japanese Earth Resources Satellite (JERS-1) L-band radar acquired in an area of active deforestation in Rondonia, Brazil, are compared with SIR-C L-band polarimetric data acquired at the same incidence, two weeks later, but from a lower orbiting altitude. Large differences in scattering behavior are recorded between the two datasets in the areas of slash and burn forest, which are difficult to reconcile with surface changes. A simulation with SIR-C polarimetric data, however, suggests that those differences are consistent with a Faraday rotation angle of about 30/spl plusmn/10/spl deg/ in the JERS-1 data and 0/spl deg/ in the SIR-C data. Based on these two experiments and on Global Positioning System (GPS) records of ionospheric activity, it is concluded that Faraday rotation should not affect the analysis of L-band spaceborne data during periods of low ionospheric activity (solar minima).;Effect of Faraday rotation on L-band interferometric and polarimetric synthetic-aperture radar data;Not health related;Not health related;0
"T. Varga; H. Bunke";2003;A perturbation model for generating synthetic text lines from existing cursively handwritten lines of text produced by human writers is presented. Our purpose is to improve the performance of an HMM-based off-line cursive handwriting recognition system by providing it with additional synthetic training data. Two kinds of perturbations are applied, geometrical transformations and thinning/thickening operations. The proposed perturbation model is evaluated under different experimental conditions.;Generation of synthetic training data for an HMM-based handwriting recognition system;Not health related;Not health related;0
McCorkle;1991;The synthetic aperture radar (SAR) focusing problem is considered, with special attention given to focusing an area in the near-field of the synthetic aperture over a decade or more of bandwidth in a manner that preserves target resonance characteristics. An approximation that reduces the computational load is shown. An error analysis of the approximation shows quantitatively what conditions are required to guarantee that the depth of focus is adequate to preserve long-duration target resonance effects.<>;Focusing of synthetic aperture ultra wideband data;Not health related;Not health related;0
"R. Esteller; G. Vachtsevanos; J. Echauz; B. Lilt";1999;The fractal dimension (FD) of a waveform represents a powerful tool for transient detection. In particular, in analysis of electroencephalograms (EEG) and electrocardiograms (EGG), this feature has been used to identify and distinguish specific states of physiologic function. A variety of algorithms are available for the computation of FD. In this study, the most common methods of estimating the FD of biomedical signals are analyzed and compared. The analysis is performed over both synthetic data and intracranial EEG (IEEG) data recorded during pre-surgical evaluation of individuals with epileptic seizures. The advantages and drawbacks of each technique are highlighted. The effects of window size, number of overlapping points, and signal to noise ratio (SNR) are evaluated for each method. This study demonstrates that a careful selection of FD algorithm is required for specific applications.;A comparison of fractal dimension algorithms using synthetic and experimental data;health related;health related;0
K. El Emam;2020;Access to individual-level health data is going to be critical for managing the COVID-19 pandemic and enabling society to return to some form of (new) normal functioning. Broader data access is already starting to happen. At the same time, there has been growing alarm by the privacy community about the extent and manner of the level of data sharing that is going on with such sensitive information. In South Korea, broad data sharing has already resulted in some patients being reidentified and experiencing judgment and ridicule,<sup>1,2</sup> and some governments have begun to reduce the amount of information being shared about COVID-19 cases.<sub>3-8</sub> Data synthesis can provide a solution by enabling access to useful information while ensuring reasonable privacy protections.;Seven Ways to Evaluate the Utility of Synthetic Data;Not health related;Not health related;1
"N. Rossenbach; A. Zeyer; R. Schlüter; H. Ney";2020;Recent advances in text-to-speech (TTS) led to the development of flexible multi-speaker end-to-end TTS systems. We extend state-of-the-art attention-based automatic speech recognition (ASR) systems with synthetic audio generated by a TTS system trained only on the ASR corpora itself. ASR and TTS systems are built separately to show that text-only data can be used to enhance existing end-to-end ASR systems without the necessity of parameter or architecture changes. We compare our method with language model integration of the same text data and with simple data augmentation methods like SpecAugment and show that performance improvements are mostly independent. We achieve improvements of up to 33% relative in word-error-rate (WER) over a strong baseline with data-augmentation in a low-resource environment (LibriSpeech-100h), closing the gap to a comparable oracle experiment by more than 50%. We also show improvements of up to 5% relative WER over our most recent ASR baseline on LibriSpeech-960h.;Generating Synthetic Audio Data for Attention-Based Speech Recognition Systems;Not health related;Not health related;0
"B. Xin; W. Yang; Y. Geng; S. Chen; S. Wang; L. Huang";2020;"Generative Adversarial Network (GAN) has already made a big splash in the field of generating realistic ""fake"" data. However, when data is distributed and data-holders are reluctant to share data for privacy reasons, GAN’s training is difficult. To address this issue, we propose private FL-GAN, a differential privacy generative adversarial network model based on federated learning. By strategically combining the Lipschitz limit with the differential privacy sensitivity, the model can generate high-quality synthetic data without sacrificing the privacy of the training data. We theoretically prove that private FL-GAN can provide strict privacy guarantee with differential privacy, and experimentally demonstrate our model can generate satisfactory data.";Private FL-GAN: Differential Privacy Synthetic Data Generation Based on Federated Learning;Not health related;Not health related;0
"H. Sun; M. Shimada; F. Xu";2017;This letter closes a special stream consisting of selected papers from the fifth Asia-Pacific Conference on Synthetic Aperture Radar in 2015 (APSAR 2015). The latest research results and outcomes from APSAR 2015, particularly on the synthetic aperture radar (SAR) systems/subsystems design, data processing techniques, and various SAR applications in remote sensing, are summarized and presented. All these results represent the recent advances in SAR remote sensing. Hopefully, this letter can provide some references for SAR researchers/engineers and stimulate the future development of SAR technology for remote sensing.;Recent Advances in Synthetic Aperture Radar Remote Sensing—Systems, Data Processing, and Applications;Not health related;Not health related;0
"Y. Guo; S. Wang; C. Gao; D. Shi; D. Zhang; B. Hou";2015;Deep Belief Network (DBN) is a classic deep learning model, and it can learn higher feature and do better classification job. We combine DBN's basic component Restricted Boltzmann Machines (RBM) with the statistic distribution of Polarimetric SAR (PolSAR) data. Based on it, we develop a deep learning classification method that is suitable for PolSAR data. To verify the effectiveness of the method, a real PolSAR dataset is tested. Experiment result confirms that the proposed method provides fine improvements both in classification accuracy and visual effect.;Wishart RBM based DBN for polarimetric synthetic radar data classification;Not health related;Not health related;0
"P. J. Lin; B. Samadi; A. Cipolone; D. R. Jeske; S. Cox; C. Rendon; D. Holt; Rui Xiao";2006;Data mining research has yielded many significant and useful results such as discovering consumer-spending habits, detecting credit card fraud, and identifying anomalous social behavior. Information discovery and analysis systems (IDAS) extract information from multiple sources of data and use data mining methodologies to identify potential significant events and relationships. This research designed and developed a tool called IDAS data and scenario generator (IDSG) to facilitate the creation, testing and training of IDAS. IDSG focuses on building a synthetic data generation engine powerful and flexible enough to generate synthetic data based on complex semantic graphs;Development of a Synthetic Data Set Generator for Building and Testing Information Discovery Systems;Not health related;Not health related;0
"C. M. Ward; J. Harguess; C. Hilton";2018;In this paper, we revisit the problem of classifying ships (maritime vessels) detected from overhead imagery. Despite the last decade of research on this very important and pertinent problem, it remains largely unsolved. One of the major issues with the detection and classification of ships and other objects in the maritime domain is the lack of substantial ground truth data needed to train state-of-the-art machine learning algorithms. We address this issue by building a large (200k) synthetic image dataset using the Unity gaming engine and 3D ship models. We demonstrate that with the use of synthetic data, classification performance increases dramatically, particularly when there are very few annotated images used in training.;Ship Classification from Overhead Imagery using Synthetic Data and Domain Adaptation;Not health related;Not health related;0
"J. W. Owens; M. W. Marcellin; B. R. Hunt; M. Kleine";1999;Synthetic aperture radar (SAR) is a remote-sensing technology that uses the motion of the radar transmitter to synthesize an antenna aperture much larger than the actual antenna aperture to yield high-spatial resolution radar images. In this paper, trellis-coded quantization (TCQ) techniques are shown to provide a high-performance, low bit-error sensitivity solution to the problem of downlink data rate reduction for SAR systems. Trellis-coded vector quantization (TCVQ) and universal TCQ coding systems are discussed, implemented, and compared with other data compression schemes [block adaptive quantization (BAQ) and VQ] that can be used to compress SAR phase history data.;Compression of synthetic aperture radar video phase history data using trellis-coded quantization techniques;Not health related;Not health related;0
"A. Eichenseer; A. Kaup";2016;In video surveillance as well as automotive applications, so-called fisheye cameras are often employed to capture a very wide angle of view. As such cameras depend on projections quite different from the classical perspective projection, the resulting fisheye image and video data correspondingly exhibits non-rectilinear image characteristics. Typical image and video processing algorithms, however, are not designed for these fisheye characteristics. To be able to develop and evaluate algorithms specifically adapted to fisheye images and videos, a corresponding test data set is therefore introduced in this paper. The first of those sequences were generated during the authors' own work on motion estimation for fish-eye videos and further sequences have gradually been added to create a more extensive collection. The data set now comprises synthetically generated fisheye sequences, ranging from simple patterns to more complex scenes, as well as fisheye video sequences captured with an actual fisheye camera. For the synthetic sequences, exact information on the lens employed is available, thus facilitating both verification and evaluation of any adapted algorithms. For the real-world sequences, we provide calibration data as well as the settings used during acquisition. The sequences are freely available via www.lms.lnt.de/fisheyedataset/.;A data set providing synthetic and real-world fisheye video sequences;Not health related;Not health related;0
"V. Kukreja; D. Kumar; A. Kaur; Geetanjali; Sakshi";2020;In today's modern era, parking remains a big problem for a lot of people. This problem consumes a person individual's time by finding the right spot for parking. In the current research, the concept of an automatic parking system using the vehicle license plate or the number plate recognition is discussed. It will improve the process with much less hassle by removing human interaction. It will also lead to advancement in the security of vehicles evading the requirement of a slip or a magnetic card which is used to goes in and out for registering vehicles in a parking place. The researcher uses image processing algorithms to make an entry in the database of the parking automatically. AVNPR (Automatic Vehicle Number Plate Recognition) is used for the identification of the number of plates. Due to noise issues, deep algorithms like CNN (convolutional neural networks), RNN (recurrent neural networks) do not correctly recognize the miss-identification of the numbers in the vehicle plate. This problem is rectified by the authors by using the GAN (Generative adversarial networks) algorithm. GAN helps to create high-resolution images from a single low-resolution image. After applying the GAN, the classification of the vehicle plate is done through CNN. During experimentation, the proposed approach achieves 99.39% recognition accuracy for a vehicle number plate. Hence, the proposed system is suitable for identifying the numbers in the vehicle number plate automatically. Moreover proposed system compared with existing models, it has been found that it has achieved higher accuracy than the other models.;GAN-based synthetic data augmentation for increased CNN performance in Vehicle Number Plate Recognition;Not health related;Not health related;0
"S. Bhattacharya; T. Blumensath; B. Mulgrew; M. Davies";2008;Synthetic aperture radar (SAR) is active and coherent microwave high resolution imaging system, which has the capability to image in all weather and day-night conditions. SAR transmits chirp signals and the received echoes are sampled into In-phase (I) and Quadrature (Q) components, generally referred to as raw SAR data. Raw data compression is an essential future requirement for high resolution space borne SAR sensor in order to reduce the volume of data that is stored onboard and later transmitted to ground station. Due to the low computational resources available onboard satellite a simple encoding algorithm based on compressed sensing framework to compress SAR raw data with real wavelets is proposed in this paper. The decoding of the data on ground is then based on convex optimization through projections on convex sets (POCS) or uses greedy algorithms such as orthogonal matching pursuit (OMP). The option of converting the complex SAR signal to real data by shifting the frequency spectrum by half bandwidth and then using real wavelets as a sparsifying transform to compress the SAR signal is studied and compared with using the wavelets with the complex signal in the CS framework.;Synthetic Aperture Radar raw data encoding using Compressed Sensing;Not health related;Not health related;0
"A. Potsis; A. Reigber; K. P. Papathanassiou";1999;Addresses the least mean square method for estimation and coherent subtraction of the RF interference in interferometric SAR data applications. The authors also compare the results with a phase preserving notch filter. For this purposes the authors use polarimetric interferometric SAR data from a test site in Solothum/Switzerland collected by the DLR's Experimental SAR (ESAR).;A phase preserving method for RF interference suppression in P-band synthetic aperture radar interferometric data;Not health related;Not health related;0
F. Biondi;2016;Synthetic Aperture Radar (SAR) systems produce a tremendous amount of redundant data if persistent radar surveillance of a specific area is implemented. This paper performs an efficient data reduction extrapolating maritime targets in motion from background subtraction. The technique is based on Robust Principal Component Analysis (RPCA). The algorithm is implemented by Convex Programming (CP). This Low Rank and Sparse Decomposition (LRSD) activity permits the separation of sparse objects of interest, with a stationary low-rank background. RPCA applied to SAR surveillance permits the saving of a large amount of data. Dynamic SAR is procured by Multi Chromatic Analysis (MCA) of Native (RAW)1 satellite data.;Low rank plus sparse decomposition of synthetic aperture radar data for maritime surveillance;Not health related;Not health related;0
"V. Margner; M. Pechwitz";2001;A system for the automatic generation of synthetic databases for the development or evaluation of Arabic word or text recognition systems (Arabic OCR) is presented. The proposed system works without any scanning of printed paper. Firstly Arabic text has to be typeset using a standard typesetting system. Secondly a noise-free bitmap of the document and the corresponding ground truth (GT) is automatically generated. Finally, an image distortion can be superimposed to the character or word image to simulate the expected real world noise of the intended application. All necessary modules are presented together with some examples. Special problems caused by specific features of Arabic, such as printing from right to left, many diacritical points, variation in the height of characters, and changes in the relative position to the writing line, are suggested. The synthetic data set was used to train and test a recognition system based on hidden Markov model (HMM), which was originally developed for German cursive script, for Arabic printed words. Recognition results with different synthetic data sets are presented.;Synthetic data for Arabic OCR system development;Not health related;Not health related;0
"S. G. Schock; J. Wulf; G. Quentin; J. Sara";2005;Synthetic aperture processing improves the resolution and signal to scattering noise performance of buried object scanning sonar (BOSS) imagery. Time delay focusing coherently sums acoustic data measured by a line hydrophone array located in the wings of the BOSS vehicle. Synthetic aperture processing improves the along track spatial resolution by coherently summing the data over a sequence of transmissions. SAS (synthetic aperture sonar) motion compensation is implemented by calculating the changes in projector and hydrophone positions between transmissions using IMU (inertial measurement unit) and DVL (Doppler velocity log) data. The coherent summation is performed at each location in a 3D volume of focal points including the upper meter of sediments and the sediment-water interface. Sonar images are projections of the 3D data set onto orthogonal planes. A set of three cylinders with diameters of 5,7.5 and 10 cm, buried in sand, are used to measure the relationship between SAS aperture length and the SNR and spatial resolution of BOSS imagery. The BOSS data sets containing the cylindrical targets were collected in September 2004 as part of SAX-04 (Sediment Acoustics Experiment - 2004), located off Fort Walton Beach, Florida.;Synthetic aperture processing of buried object scanning sonar data;Not health related;Not health related;0
"A. A. El-Sayed; M. A. M. Mahmood; N. A. Meguid; H. A. Hefny";2015;The autism diagnostic interview-revised (ADI-R) is a semi-structured interview designed to assess the three core aspects of autism spectrum disorder (ASD). In this research a synthetic minority over-sampling technique (SMOT) was presented for handling autism imbalanced data to increase accuracy credibility. SMOT can potentially lead to over fitting on multiple copies of minority class examples. The autism data collected from National Research Center in Egypt (NRC). The experimental dataset applied on several machine learning algorithms and compared the accuracy before and after over-sampling techniques. The result show that over-sampling for imbalanced data making accuracy realistic and non-deceptive and can be Reliable.;Handling autism imbalanced data using synthetic minority over-sampling technique (SMOTE);health related;Not health related;0
"H. C. Powell; J. Lach; M. Brandt-Pearce; C. L. Brown";2010;The use of artificial neural network (ANN) classifiers as a signal processing element in resource constrained embedded computing systems has been restricted due to the difficulty of predicting performance and execution requirements on the deployed platform. In this paper, techniques are presented which provide a means of efficiently estimating data complexity, generating meaningful synthetic data, and evaluating ANN classifiers in terms of achievable performance.;Systematic estimation of ANN classification performance employing synthetic data;Not health related;Not health related;0
K. Tan;2021;Multiple-input-multiple-output synthetic aperture radar (MIMO-SAR) is being studied and used in more and more sensing applications. However, to deal with large-size scanning data without acceleration technology, there is still a certain distance away from real-time operation using the previous state-of-the-art imaging algorithms, which must use complicated interpolation or iteration operations. To conquer this difficulty, a fast imaging algorithm is developed for 3-D near-range imaging on MIMO-SAR. An appropriate expansion is carried out to the coupling phase term, and thus the differential correction for the MIMO data set can be realized with only a range inverse fast Fourier transform (IFFT) and a matched filtering, which are easy to implement. The computational complexity of the algorithm is O(N4log2 N ), which is lower than the previous state-of-the-art algorithms. Short-range measurements on a millimeter-waveband MIMO-SAR with different distributed targets demonstrate the performance of the proposed algorithm. Real-time operation is achieved using the algorithm even with a regular CPU.;A Fast Omega-K Algorithm for Near-Field 3-D Imaging of MIMO Synthetic Aperture Radar Data;Not health related;Not health related;0
"W. Xu; W. Xing; C. Fang; P. Huang; W. Tan";2021;Radio frequency interference (RFI) sources pose threats to wideband synthetic aperture radar (SAR) systems and accurate SAR image interpretation. Since most of RFI sources are narrowband, notch filtering is a simple but effective method for RFI suppression. In this letter, a modified two-step notch filtering approach combined with linear prediction is proposed to improve the SAR image quality. The notch filtering is used to mitigate narrowband RFI energy, while the linear prediction is introduced to recover the missing range spectral component of the SAR raw data from the desired scene, which is removed together with RFI sources by the notch filter. Because of the Gibbs phenomenon in Fourier series, the small residual RFI energy after notch filtering is enough to cause image visual disturbances and affect the accuracy of the following missing range spectrum extrapolation. The two-step notch filtering with a limited bandwidth is applied for better RFI sources mitigation. Simulation results on both simulated targets and real SAR raw data validate the proposed approach.;RFI Suppression Based on Linear Prediction in Synthetic Aperture Radar Data;Not health related;Not health related;0
"C. Clemente; M. di Bisceglie; M. Di Santo; N. Ranaldo; M. Spinelli";2009;Synthetic aperture radar processing is a complex task that involves advanced signal processing techniques and intense computational effort. While the first issue has now reached a mature stage, the question of how to produce accurately focused images in real-time, without mainframe facilities, is still under debate. The recent introduction of general-purpose graphic processing units seems to be quite promising in this view, especially for the decreased per-core cost barrier and for the affordable programming complexity. The authors explain, in this work, the main computational features of a range-Doppler Synthetic Aperture Radar (SAR) processor, trying to disclose the degree of parallelism in the operations at the light of the CUDA programming model. Given the extremely flexible structure of the Single Instruction Multiple Threads (SIMT) model, the authors show that the optimization of a SAR processing unit cannot reduce to an FFT optimization, although this is a quite extensively used kernel. Actually, it is noticeable that the most significant advantage is obtained in the range cell migration correction kernel where a complex interpolation stage is performed very efficiently exploiting the SIMT model. Performance show that, using a single Nvidia Tesla-C1060 GPU board, the obtained processing time is more than fifteen time better than our test workstation.;Processing of synthetic Aperture Radar data with GPGPU;Not health related;Not health related;0
"X. Zheng; B. Wang; L. Xie";2019;This paper concerns with the production of synthetic phasor measurement unit (PMU) data for research and education purposes. Due to the confidentiality of real PMU data and no public access to the real power systems infrastructure information, the lack of credible realistic data becomes a growing concern. Instead of constructing synthetic power grids and then producing synthetic PMU measurement data by time simulations, we propose a model-free approach to directly generate synthetic PMU data. we train the generative adversarial network (GAN) with real PMU data, which can be used to generate synthetic PMU data capturing the system dynamic behaviors. To validate the sequential generation by GAN to mimic PMU data, we theoretically analyze GAN's capacity of learning system dynamics. Further by evaluating the synthetic PMU data by a proposed quantitative method, we verify GAN's potential to synthesize realistic samples and meanwhile realize that GAN model in this paper still has room to improve. Moreover it is the first time that such generative model is applied to synthesize PMU data.;Synthetic Dynamic PMU Data Generation: A Generative Adversarial Network Approach;Not health related;Not health related;0
"B. Bhattarai; S. Baek; R. Bodur; T. -K. Kim";2020;Generative Adversarial Networks (GANs) have been used widely to generate large volumes of synthetic data. This data is being utilised for augmenting with real examples in order to train deep Convolutional Neural Networks (CNNs). Studies have shown that the generated examples lack sufficient realism to train deep CNNs and are poor in diversity. Unlike previous studies of randomly augmenting the synthetic data with real data, we present our simple, effective and easy to implement synthetic data sampling methods to train deep CNNs more efficiently and accurately. To this end, we propose to maximally utilise the parameters learned during training of the GAN itself. These include discriminator's realism confidence score and the confidence on the target label of the synthetic data. In addition to this, we explore reinforcement learning (RL) to automatically search a subset of meaningful synthetic examples from a large pool of GAN synthetic data. We evaluate our method on two challenging face attribute classification data sets viz. AffectNet and CelebA. Our extensive experiments clearly demonstrate the need of sampling synthetic data before augmentation, which also improves the performance of one of the state-of-the-art deep CNNs in vitro.;Sampling Strategies for GAN Synthetic Data;Not health related;Not health related;0
"S. Jung; J. Park; S. Lee";2019;"This paper presents a novel approach to improve the performance of polyphonic sound event detection that combines a convolutional bidirectional recurrent neural network (CBRNN) with transfer learning. The ordinary convolutional recurrent neural network (CRNN) is known to suffer from a vanishing gradient problem, which significantly reduces the efficiency of information transfer to past events. To resolve this issue, we combine forward and backward long short-term memory (LSTM) modules and demonstrate that they complement each other. To effectively deal with the issue of overfitting that arises from increased model complexity, we apply transfer learning with a dataset that contains synthesized artifacts. We show that the model achieves faster and better performance with less data. Simulations with the 2016 TUT dataset show that the performance of the CBRNN with transfer learning is dramatically improved compared to the ordinary CRNN; the F1 score was 28.4% higher, and the error rate was 0.42 lower.";Polyphonic Sound Event Detection Using Convolutional Bidirectional Lstm and Synthetic Data-based Transfer Learning;Not health related;Not health related;0
"N. Macia; E. Bernado-Mansilla; A. Orriols-Puig";2008;Usually, performance of classifiers is evaluated on real-world problems that mainly belong to public repositories. However, we ignore the inherent properties of these data and how they affect classifier behavior. Also, the high cost or the difficulty of experiments hinder the data collection, leading to complex data sets characterized by few instances, missing values, and imprecise data. The generation of synthetic data sets solves both issues and allows us to build problems with a minor cost and whose characteristics are predefined. This is useful to test system limitations in a controlled framework. This paper proposes to generate synthetic data sets based on data complexity. We rely on the length of the class boundary to build the data sets, obtaining a preliminary set of benchmarks to assess classifier accuracy. The study can be further matured to identify regions of competence for classifiers.;Preliminary approach on synthetic data sets generation based on class separability measure;Not health related;Not health related;0
"T. Varga; H. Bunke";2004;In this paper, a perturbation model for the generation of synthetic textlines from existing cursively handwritten lines of text, produced by human writers, is presented. The goal of synthetic textline generation is to improve the performance of an off-line cursive handwriting recognition system by providing it with additional, synthetic training data. In earlier papers, it has been shown that it is possible to improve the recognition performance by using such synthetically expanded training sets. In this paper, we investigate the suitability of synthetically generated handwriting when enlarging the training set of a handwriting recognition system in a more rigorous way. In particular, the improvements achieved with synthetic training data are compared to those achieved by expanding the training set using natural, i.e. human written, textlines.;Comparing natural and synthetic training data for off-line cursive handwriting recognition;Not health related;Not health related;0
"T. -Y. Hu; M. Armandpour; A. Shrivastava; J. -H. R. Chang; H. Koppula; O. Tuzel";2022;With recent advances in speech synthesis, synthetic data is becoming a viable alternative to real data for training speech recognition models. However, machine learning with synthetic data is not trivial due to the gap between the synthetic and the real data distributions. Synthetic datasets may contain artifacts that do not exist in real data such as structured noise, content errors, or unrealistic speaking styles. Moreover, the synthesis process may introduce a bias due to uneven sampling of the data manifold. We propose two novel techniques during training to mitigate the problems due to the distribution gap: (i) a rejection sampling algorithm and (ii) using separate batch normalization statistics for the real and the synthetic samples. We show that these methods significantly improve the training of speech recognition models using synthetic data. We evaluate the proposed approach on keyword detection and Automatic Speech Recognition (ASR) tasks, and observe up to 18% and 13% relative error reduction, respectively, compared to naively using the synthetic data.;SYNT++: Utilizing Imperfect Synthetic Data to Improve Speech Recognition;Not health related;Not health related;0
"S. Norgaard; R. Saeedi; K. Sasani; A. H. Gebremedhin";2018;Recent advancements in mobile devices, data analysis, and wearable sensors render the capability of in-place health monitoring. Supervised machine learning algorithms, the core intelligence of these systems, learn from labeled training data. However, labeling vast amount of data is time-consuming and expensive. Moreover, sensor data often contains personal information that a user may not be comfortable sharing. Therefore, there is a strong need to develop methods for generating realistic labeled sensor data. In this paper, we propose a supervised generative adversarial network architecture that learns from feedback from both a discriminator and a classifier in order to create synthetic sensor data. We demonstrate the effectiveness of the architecture on a publicly available human activity dataset. We show that our generator learns to output diverse samples that are similar but not identical to the training data.;Synthetic Sensor Data Generation for Health Applications: A Supervised Deep Learning Approach;health related;health related;0
"S. Back; J. Kim; R. Kang; S. Choi; K. Lee";2020;Segmentation of unseen industrial parts is essential for autonomous industrial systems. However, industrial components are texture-less, reflective, and often found in cluttered and unstructured environments with heavy occlusion, which makes it more challenging to deal with unseen objects. To tackle this problem, we present a synthetic data generation pipeline that randomizes textures via domain randomization to focus on the shape information. In addition, we propose an RGB-D Fusion Mask R-CNN with a confidence map estimator, which exploits reliable depth information in multiple feature levels. We transferred the trained model to real-world scenarios and evaluated its performance by making comparisons with baselines and ablation studies. We demonstrate that our methods, which use only synthetic data, could be effective solutions for unseen industrial components segmentation.;Segmenting Unseen Industrial Components In A Heavy Clutter Using RGB-D Fusion And Synthetic Data;Not health related;Not health related;0
"X. Zhang; Y. Fu; S. Jiang; L. Sigal; G. Agam";2015;Learning from synthetic data has many important and practical applications, An example of application is photo-sketch recognition. Using synthetic data is challenging due to the differences in feature distributions between synthetic and real data, a phenomenon we term synthetic gap. In this paper, we investigate and formalize a general framework -- Stacked Multichannel Autoencoder (SMCAE) that enables bridging the synthetic gap and learning from synthetic data more efficiently. In particular, we show that our SMCAE can not only transform and use synthetic data on the challenging face-sketch recognition task, but that it can also help simulate real images, which can be used for training classifiers for recognition. Preliminary experiments validate the effectiveness of the framework.;Learning from Synthetic Data Using a Stacked Multichannel Autoencoder;Not health related;Not health related;0
D. N. Ostrov;1999;Most attempts to determine surface height from noiseless synthetic aperture radar (SAR) data involve approximating the surface by solving a related standard shape from shading (SFS) problem. Through analysis of the underlying partial differential equations for both the original SAR problem and the approximating standard SFS problem, the authors demonstrate significant differences between them. For example, if it is known that the surface is smooth, the standard SPS problem can generally be uniquely solved from knowledge of the height and concavity at one surface point, whereas for SAR, multiple valid solutions will generally exist unless height information is specified along entire curves on the surface (i.e., boundary conditions). Unlike the standard SFS approximation, the underlying SAR equation can be reexpressed as a time-dependent Hamilton-Jacobi equation. This transformation allows the authors to compute the correct surface topography from noiseless SAR data with boundary conditions extremely quickly. Finally, they consider the effect of radar noise on the computed surface reconstruction and discuss the ability of the presented PDE method to quickly compute an initial surface that will significantly cut the computational time needed by cost minimization algorithms to approximate surfaces from noisy radar data.;Boundary conditions and fast algorithms for surface reconstructions from synthetic aperture radar data;Not health related;Not health related;0
"W. Masarczyk; I. Tautkute";2020;"Catastrophic forgetting is a problem caused by neural networks' inability to learn data in sequence. After learning two tasks in sequence, performance on the first one drops significantly. This is a serious disadvantage that prevents many deep learning applications to real-life problems where not all object classes are known beforehand; or change in data requires adjustments to the model. To reduce this problem we investigate the use of synthetic data, namely we answer a question: Is it possible to generate such data synthetically which learned in sequence does not result in catastrophic forgetting? We propose a method to generate such data in two-step optimisation process via meta-gradients. Our experimental results on Split-MNIST dataset show that training a model on such synthetic data in sequence does not result in catastrophic forgetting. We also show that our method of generating data is robust to different learning scenarios.";Reducing catastrophic forgetting with learning on synthetic data;Not health related;Not health related;0
"H. Xie; S. Wang; K. Liu; S. Lin; B. Hou";2014;Features are important for polarimetric synthetic aperture radar (PolSAR) image classification. Various methods focus on extracting feature artificially. Compared with them, we have developed a method to learn feature automatically. The method is based on deep learning which can learn multilayer features. In this paper, stacked sparse autoencoder (SAE) as one of the deep learning models is applied as a useful strategy to achieve the goal. For improving the classification result, we use a small amount of labels to fine-tuning the parameters of the proposed method. Finally, a real PolSAR dataset is used to verify the effectiveness. Experiment result confirms that the proposed method provides noteworthy improvements in classification accuracy and visual effect.;Multilayer feature learning for polarimetric synthetic radar data classification;Not health related;Not health related;0
"W. Liu; B. Luo; J. Liu";2022;Deep learning approaches require enough training samples to perform well, but it is a challenge to collect enough real training data and label them manually. In this letter, we propose a practical framework for automatically generating content-rich synthetic images with ground-truth annotations. By rendering 3-D CAD models, we generate two synthetic aircraft image data sets with wide distribution (Syn N and Syn U). For improving the quality of synthetic images, we propose a multiscale attention module which enhances the Cycle-Consistent Adversarial Network (CycleGAN) in spatial and channel dimensions. Then, we compare the synthetic images before and after translation qualitatively and quantitatively. Experiments on Northwestern Polytechnical University (NWPU) very high resolution (VHR)-10, University of Chinese Academy of Sciences, orientation robust object detection in aerial images (UCAS-AOD), and benchmark for object DetectIon in Optical Remote sensing images (DIOR) data sets demonstrate that synthetic data augmentation can improve the performance of aircraft detection in remote sensing images, especially when real data are insufficient. Synthetic data are available at: https://weix-liu.github.io/.;Synthetic Data Augmentation Using Multiscale Attention CycleGAN for Aircraft Detection in Remote Sensing Images;Not health related;Not health related;0
"A. Pinceti; O. Kosut; L. Sankar";2019;A generative model for the creation of realistic historical bus-level load data for transmission grid models is presented. A data-driven approach based on principal component analysis is used to learn the spatio-temporal correlation between the loads in a system and build a generative model. Given a system topology and a set of base case loads, individual, realistic time-series data for each load can be generated. This technique is demonstrated by learning from a large proprietary dataset and generating historical data for the 2383-bus Polish test case.;Data-Driven Generation of Synthetic Load Datasets Preserving Spatio-Temporal Features;Not health related;Not health related;0
"J. Talukdar; A. Biswas; S. Gupta";2018;Training of deep Convolutional Neural Networks (CNNs) for object detection tasks requires a huge amount of annotated data which is expensive, difficult and time-consuming to produce. This requirement can be fulfilled by automating the process of dataset generation. We utilize the approach of training deep CNNs using completely synthetically rendered data, with the focus of improving the overall transfer learning performance through online and offline data augmentation techniques. We focus on the problem of detecting packaged food products in indoor refrigerator environments. We analyze the impact of various data augmentation strategies like randomized cropping, pixel shifting, image scaling, image rotation, oversaturation, Gaussian blurring, noise addition, color inversion etc. on the overall accuracy of the object detection and increase the overall mean average precision (mAP). It is found that the use of a combination of data augmentation techniques performs best, with highest mAP of 20.54 obtained with combinations of linear augmentation techniques like scaling, shifting and scaling and rotation.;Data Augmentation on Synthetic Images for Transfer Learning using Deep CNNs;Not health related;Not health related;0
"G. Riegler; M. Urschler; M. Rüther; H. Bischof; D. Stern";2015;An important initial step in many medical image analysis applications is the accurate detection of anatomical landmarks. Most successful methods for this task rely on data-driven machine learning algorithms. However, modern machine learning techniques, e.g. convolutional neural networks, need a large corpus of training data, which is often an unrealistic setting for medical datasets. In this work, we investigate how to adapt synthetic image datasets from other computer vision tasks to overcome the under-representation of the anatomical pose and shape variations in medical image datasets. We transform both data domains to a common one in such a way that a convolutional neural network can be trained on the larger synthetic image dataset and fine-tuned on the smaller medical image dataset. Our evaluations on data of MR hand and whole body CT images demonstrate that this approach improves the detection results compared to training a convolutional neural network only on the medical data. The proposed approach may also be usable in other medical applications, where training data is scarce.;Anatomical Landmark Detection in Medical Applications Driven by Synthetic Data;health related;health related;1
"P. Serranho; C. Maduro; T. Santos; J. Cunha-Vaz; R. Bernardes";2011;The use of synthetic images is needed for testing the performance of image processing methods in order to establish a ground truth to test performance metrics. However, these synthetic images do not represent real applications. The aim of this paper is to build a mathematical model to obtain a synthetic noise-free image mimicking a real Optical Coherence Tomography (OCT) B-scan or volume from the human retina, in order to establish a ground truth for filtering performance metrics in this context. Moreover we also suggest a method to add speckle noise to this image based on the speckle noise of the given OCT volume. In this way we establish a replicable method to obtain a ground truth for image processing performance metrics that actually mimics a real case.;Synthetic OCT data for image processing performance testing;Not health related;Not health related;1
"T. M. Benson; B. K. B. De Man";2010;Synthetic CT noise emulation in the raw data domain is reported. A method that synthetically adds noise directly in the raw data domain (i.e., before the negative log and associated pre processing) is presented. Initial results are obtained for a noise simulation method that lowers the effective tube current for a CT data set while considering quantum noise, electronic noise, bowtie filtration, and kVp. Focus is on simulation results where the control is more on the parameters generating many realizations.;Synthetic CT noise emulation in the raw data domain;Not health related;Not health related;0
"S. Küçük; S. E. Yüksel";2015;Anomaly detection refers to detecting the deviations from the normal background behavior without any prior information about the target or the background. For hyperspectral image analysis, Reed-Xiaoli (RX) algorithm is arguably the most popular anomaly detector. It models the background as a multidimensional Gaussian distribution and computes how much a test vector is deviating from the background model. Over the years, many versions of RX have been developed and compared on VNIR or SWIR data, but longwave-infrared (LWIR) data comparisons are very few. In this paper, a comprehensive comparison of six different anomaly detectors, namely the global RX, local RX, dual window RX, subspace RX, kernel RX and the global RX combined with a uniform target detector, have been presented. The comparisons have been made on real LWIR hyperspectral data and synthetic data with varying noise levels and target sizes. Several factors to consider such as parameter selection, resilience to noise, effect of window size, computational complexity have been discussed and the detection performance have been presented on receiver operating characteristic curves.;Comparison of RX-based anomaly detectors on synthetic and real hyperspectral data;Not health related;Not health related;0
"M. Benna; J. . -P. Barriot; W. Kofman; Y. Barbin";2004;This paper presents a 3-D simulation of the COmet Nucleus Sounding Experiment by Radiowave Propagation (CONSERT) radiotomography experiment of Comet 67P/Churyumov-Gerasimenko. This experiment is part of the ROSETTA space mission. Our simulation is based on a ray tracing algorithm and takes into account multiple internal reflections, power losses, phase delays and polarization distortions. We emulate as closely as possible the true CONSERT data and discuss the results of the simulation.;Generation of 3-D synthetic data for the modeling of the CONSERT experiment (the radiotomography of Comet 67P/Churyumov-Gerasimenko);Not health related;Not health related;0
"F. C. Akyon; O. Eryuksel; K. A. Ozfuttu; S. O. Altinuc";2021;As the usage of drones increases with lowered costs and improved drone technology, drone detection emerges as a vital object detection task. However, detecting distant drones under unfavorable conditions, namely weak contrast, long-range, low visibility, requires effective algorithms. Our method approaches the drone detection problem by fine-tuning a YOLOv5 model with real and synthetically generated data using a Kalman-based object tracker to boost detection confidence. Our results indicate that augmenting the real data with an optimal subset of synthetic data can increase the performance. Moreover, temporal information gathered by object tracking methods can increase performance further.;Track Boosting and Synthetic Data Aided Drone Detection;Not health related;Not health related;0
"N. Macià; A. Orriols-Puig; E. Bernadó-Mansilla";2008;In this paper, we highlight the use of synthetic data sets to analyze learners behavior under bounded complexity. We propose a method to generate synthetic data sets with a specific complexity, based on the length of the class boundary. We design a genetic algorithm as a search technique and find it useful to obtain class labels according to the desired complexity. The results show the suitability of the genetic algorithm as a framework to provide artificial benchmark problems that can be further enriched with the use of multi-objective and niching strategies.;Genetic-Based Synthetic Data Sets for the Analysis of Classifiers Behavior;Not health related;Not health related;0
"E. Dritsas; N. Fazakis; O. Kocsis; K. Moustakas; N. Fakotakis";2021;Synchronous office working environments are characterized by a high diversity of expertise and skills requirements during a product or service development. However, without the cooperation and optimal skills pairing among the employees, the success of a project may be negatively impacted. To take full advantage of the potential of each employee, project managers must learn how they can most effectively combine humans skills to benefit from their value. The current research work will assist businesses to make the power of intelligent collaborative matching part of the business core. The elaborated optimal team pairing framework, an integral part of the conceptual architecture of the SmartWork project, aims to combine numerous data sources and novel modelling processes from Machine Learning to provide the employers and project managers with a powerful tool in team pairing of elder office employees concerning a specific work task.;Optimal Team Pairing of Elder Office Employees with Machine Learning on Synthetic Data;Not health related;Not health related;0
"S. Hinz; D. Weihing; S. Suchandt; R. Bamler";2008;Automatic estimation of traffic parameters has evolved to an important topic of research. Current and upcoming SAR satellite missions offer new possibilities for traffic monitoring and control from space as an alternative to conventional traffic data acquisition. In this paper a detection approach is presented which evaluates simultaneously the effects moving objects suffer from in the SAR focusing process. Information about the measured signal and the expected signal are utilized in the detection framework. Analyses of the proposed technique are done with real spaceborne SAR data.;Detection and velocity estimation of moving vehicles in high-resolution spaceborne synthetic aperture radar data;Not health related;Not health related;0
"J. Chen; Q. Liang; J. Paden; P. Gogineni";2012;This work addresses the use of compressive sensing to compress real Synthetic Aperture Radar (SAR) raw data. Due to the low computational resources of the acquisition platforms and the steadily increasing resolution of SAR systems, huge amounts of data are collected and stored, which cannot generally be processed on board and must be transmitted to the ground to be processed and archived. Although compressive sensing (CS) has been proposed and studied by a lot of researchers, almost none of them touches the real application of it. While, in this paper, we test the sparsity of the real SAR raw data (obtained by University of Kansas in Greenland, 2010), compress it using compressive sensing, and then recover the original signal using several CS recovery algorithms (Basis Pursuit, Matching Pursuit and Orthogonal Matching Pursuit), and compare these methods' performance. Simulation results are presented to prove the successful application of CS to real SAR raw data. When proper sparsity matrix is chosen, the real SAR data could be transformed to sparse signal. Using our designed algorithm, the positions and the exact values of the SAR raw data can be almost perfectly recovered with a very low MSE at a compression ratio of 1/8. This is of great significance to help us perform further research in the applications of CS to real SAR raw data.;Compressive sensing analysis of Synthetic Aperture Radar raw data;Not health related;Not health related;0
"U. Majumder; M. Soumekh; M. Minardi; S. Scarborough; L. Gorham; C. Casteel; M. Judge; J. Kirk";2009;"This paper is concerned with imaging and moving target detection using a Synthetic Aperture Radar (SAR) platform that is known as Gotcha. The SAR platform can interrogate a scene using an imperfect circular trajectory; we refer to this as nonlinear SAR data collection. This collection can make monostatic and quasi-monostatic measurements in the along-track domain. We present subaperture-based wavefront reconstruction algorithms for motion compensation and imaging from this nonlinear SAR database. We also discuss adaptive filtering algorithms to construct MTI imagery from the two receiver channels of the system. Results will be provided.";Synthetic Aperture Radar moving target indication processing of along-track monopulse nonlinear gotcha data;Not health related;Not health related;0
"W. Feng; W. Huang; H. Ye; L. Zhao";2018;In this paper, we propose a novel Synthetic Minority Oversampling Technique based Rotation forest (SMOTERoF) algorithm for the classification of imbalanced hyperspectral image data. The main idea of the proposed method is to iteratively balance the class distribution of training set by SMOTE for each rotation decision tree. Experiment results on the hyperspectral image Indian Pines AVRIS with different imbalance ratio (IR) show that our algorithm obtains better classification performance compared with Rotation Forest (RoF), random undersampling, random oversampling, SMOTE, as well as Under sampling based RoF (UnderRoF) which is an extended version of UnderBagging.;Synthetic Minority Over-Sampling Technique Based Rotation Forest for the Classification of Unbalanced Hyperspectral Data;Not health related;Not health related;0
"J. Yu; D. Farin; C. Krüger; B. Schiele";2010;Person detection in complex real-world scenes is a challenging problem. State-of-the-art methods typically use supervised learning relying on significant amounts of training data to achieve good detection results. However, labeling training data is tedious, expensive, and error-prone. This paper presents a novel method to improve detection performance by supplementing real-world data with synthetically generated training data. We consider the case of detecting people in crowded scenes within an AdaBoost-framework employing Haar and Histogram-of-Oriented-Gradients (HOG) features. Our evaluations on real-world video sequences of crowded scenes with significant occlusions show that the combination of real and synthetic training data significantly improves overall detection results.;Improving person detection using synthetic training data;Not health related;Not health related;0
"A. Kothare; S. Chaube; Y. Moharir; G. Bajodia; S. Dongre";2021;Synthetic data is superficial data generated using various machine learning techniques. The respective synthetic data generated can be used to preserve privacy, test systems, or create training data for machine learning algorithms. Synthetic data generation is critical as the need for specific data is huge in today's world, for example, synthetic data can be used to practice various data science tasks and techniques, while maintaining the anonymity of the samples generated. We used an open-source engine named Faker (v5.6.1) and Gaussian copula to create a platform that can generate datasets, based on user requirements as well as available resources. The user can also perform a variety of machine learning algorithms and differentiate their performance either over the generated dataset or a predefined dataset.;SynGen: Synthetic Data Generation;Not health related;Not health related;0
"L. Dell’Amore; M. Villano; G. Krieger";2019;Synthetic aperture radar (SAR) remote sensing is very attractive for the systematic observation of dynamic processes on the Earth's surface since it allows high resolution imaging independently of weather conditions and sunlight illumination.Waveform-encoded SAR is a novel SAR concept based on pulse-to-pulse variation of the transmitted waveform that allows focusing the nadir echo and the range ambiguities and suppressing them through a multi-focus post-processing. However, the assessment of the ambiguity suppression performance for such a system is not trivial, as the processing involves a (non-linear) thresholding and blanking approach. This work proposes a novel methodology, which exploits real TerraSAR-X data to accurately simulate the effect of the range ambiguity on the useful signal and allows for a quantitative assessment of the image quality of a waveform-encoded SAR. The analysis considers different waveform variation schemes (e.g., up- and down-chirps, cyclically-shifted chirps) and a contrast-minimization technique for threshold selection, whose performance is compared to the best achievable one (i.e., optimum threshold). The results of this work further highlight the potentialities of the waveform-encoded SAR concept and also allow accounting for its ambiguity suppression capability in the design of a SAR system.;Assessment of Image Quality of Waveform-Encoded Synthetic Aperture Radar Using Real Satellite Data;Not health related;Not health related;0
"Q. Wu; L. Wang; K. N. Ngan; H. Li; F. Meng";2019;Deraining quality assessment (DQA) plays an important role in evaluating and guiding the design of the image deraining algorithm. Due to the absence of rain-free image in the real rainy weather, the existing deraining algorithms are typically tested on several synthetic data by simulating very limited types of rain streaks, which are far from sufficient to measure the practicability of a deraining algorithm. In this paper, we first build a subjective DQA database that collects diverse authentic rain images and their derained versions. Then, a blind quality metric is developed to predict the deraining quality. Since the deraining artifacts are anisotropic and variable, we propose to describe the image via a bi-directional gated fusion network (B-GFN), which adaptively integrates the multi-scale cues of deraining artifact. Experiments confirm the effectiveness of the proposed method and its superiority with respect to many state-of-the-art blind image quality metrics.;Beyond Synthetic Data: A Blind Deraining Quality Assessment Metric Towards Authentic Rain Image;Not health related;Not health related;0
"Y. . -L. Desnos; H. Laur; P. Lim; P. Meisl; T. Gach";1999;An Advanced Synthetic Aperture Radar operating at C-band (5.331 GHz) has been selected for the Envisat-1 payload ensuring continuity of ERS SAR and featuring enhanced capability in terms of coverage, range of incidence angles, polarisations, modes of operation. This paper presents the key features of the Envisat ASAR system. The ASAR instrument modes of operation are discussed. The ASAR Ground Processor is presented highlighting the concepts for its development and the performance achieved versus the ESA specifications. The selection of processing algorithm for each product is discussed based on the image quality requirements and the throughput requirements. Finally the newly developed ENVISAT ASAR wave mode product is introduced.;The ENVISAT-1 Advanced Synthetic Aperture Radar processor and data products;Not health related;Not health related;0
"M. Pasinato; C. E. Mello; M. -A. Aufaure; G. Zimbrão";2013;Context-Aware Recommender Systems (CARS) have emerged as a different way of providing more precise and interesting recommendations through the use of data about the context in which consumers buy goods and/or services. CARS consider not only the ratings given to items by consumers (users), but also the context attributes related to these ratings. Several algorithms and methods have been proposed in the literature in order to deal with context-aware ratings. Although there are lots of proposals and approaches working for this kind of recommendation, adequate and public datasets containing user's context-aware ratings about items are limited, and usually, even these are not large enough to evaluate the proposed CARS very well. One solution for this issue is to crawl this kind of data from e-commerce websites. However, it could be very time-expensive and also complicated due to problems regarding legal rights and privacy. In addition, crawled data from e-commerce websites may not be enough for a complete evaluation, being unable to simulate all possible users' behaviors and characteristics. In this article, we propose a methodology to generate a synthetic dataset for context-aware recommender systems, enabling researchers and developers to create their own dataset according to the characteristics in which they want to evaluate their algorithms and methods. Our methodology enables researchers to define the user's behavior of giving ratings based on the Probability Distribution Function (PDF) associated to their profiles.;Generating Synthetic Data for Context-Aware Recommender Systems;Not health related;Not health related;0
"L. Nguyen; T. Do";2012;In this paper, we present a novel technique to recover the missing spectral information in multiple frequency bands of ultra-wideband (UWB) synthetic aperture radar (SAR) data that are either corrupted (due to the presence of interference sources) or nonexistent (because of no transmission in the prohibited frequency bands). Although the spectral information is lost due to the contaminated and missing frequency bands, each backscatter receive data record can be modeled as a linear combination of the spectrally filtered and time-shifted versions of the transmitted waveform. Thus, the target information (range, amplitude, and phase) can be computed based on direct sparse recovery via orthogonal matching pursuit using a dictionary that contains many spectrally filtered and time-shifted versions of the transmitted waveform. On the other hand, the desired receive signal (with full spectral information) can be modeled as a linear combination of the time-shifted versions of the desired transmitted waveform (with full spectrum). Thus, once the target information is computed by the sparse recovery process using the receive data and the dictionary, the desired received signal can be reconstructed using the target information and the desired transmitted waveform. Using both simulation data and SAR data from the U.S. Army Research Laboratory (ARL) UWB SAR, we show that the proposed technique can successfully recover the information from the missing or corrupted frequency bands. The paper also compares the result to that using the conventional technique that simply zeros out the fast Fourier transform (FFT) bins that correspond to the corrupted frequency bands.;Recovery of missing spectral information in ultra-wideband synthetic aperture radar (SAR) data;Not health related;Not health related;0
"N. S. Subotic; L. M. Collins; J. D. Gorman; B. J. Thelen";1994;We demonstrate the utility of a multiresolution approach for target detection in SAR imagery. In particular, man-made objects exhibit characteristic phase and amplitude fluctuations as the image resolution is varied, while natural terrain (i.e. clutter) has a random signature. We show that the multiresolution clutter process decomposes into a Brownian motion process in resolution. We then construct an optimal invariant multiresolution detector based on a derived multiresolution increments process and show that it significantly outperforms a standard energy detector operating on the finest available SAR resolution.<>;A multiresolution approach to target detection in synthetic aperture radar data;Not health related;Not health related;0
"H. Lee; S. Jung; M. Kim; S. Kim";2017;Some solutions to solve the class imbalance problem that is one of the representative difficulties in machine learning have been proposed. Among them, SMOTE algorithm is proposed recently to reduce the influence of the problem and shows its remarkable performance to solve real world problems. This paper proposes a novel kind of the SMOTE algorithm by combining the previous SMOTE algorithm and fuzzy logic to deal with uncertainties underlying learning samples. Also, fuzzy c-means clustering is used to assign membership degree to given samples efficiently. Moreover, the extended gap statistics is applied to select the optimal number of cluster. The proposed algorithm is evaluated on using several benchmark datasets and shows its good performance by combining support vector machine classifier.;Synthetic minority over-sampling technique based on fuzzy c-means clustering for imbalanced data;Not health related;Not health related;0
"R. Shuchman; J. Lyden; D. Lyzenga";1983;Simultaneously obtainedX- andL-band synthetic aperture radar (SAR) data collected during the Marineland Experiment were spectrally analyzed by fast Fourier transform (FFT) techniques to estimate ocean wavelength and direction. An eight-sided flight pattern was flown over the same ocean area in order to study the sensitivity of the spectral estimate on radar look direction. These spectral estimates were compared with in situ wave measurements made by a pitch-and-roll buoy. The comparison revealed that theX-band SAR detected all gravity waves independent of radar look direction, while theL-band SAR detected all range-traveling gravity waves but failed to detect waves in three of four cases in which the waves were traveling within 25° of the azimuth direction. The analysis also indicates that azimuth-traveling waves appear longer and more range-traveling in the SAR imagery than observed by in situ instrumentation. It is postulated that degraded azimuth resolution due to scatterer motion is responsible for these observations.;Estimates of ocean wavelength and direction from X-and L-band synthetic aperture radar data collected during the Marineland experiment;Not health related;Not health related;0
"K. von Neumann-Cosel; E. Roth; D. Lehmann; J. Speth; A. Knoll";2009;In this paper, it is shown that synthetic images can be used to test specific use cases of a lane tracking algorithm which has been developed by Audi AG. This was achieved by setting up a highly configurable and extendable simulation framework ldquovirtual test driverdquo. The main components are a traffic simulation, visualization and a sensor model which supplies ground truth data about the street lanes. Additionally, the visualization is used to generate synthetic camera sensor data. The testbed also contains a realistic driving dynamics simulation and a real image processing soft ECU (which is represented as a standard PC in the early development stages). One of the modules on the image processing ECU is a lane tracking algorithm. The algorithm is designed to calculate the transition curves while driving. This information can be used as input for driving assistance functions, e.g. lane departure warning. By running the lane tracker on a synthetic image it is possible to compare the results of the lane tracker with the ground truth data provided by the simulation. In this particular case, the information has been used to test and optimize parts of the systems by using specific and determined scenarios in the simulation.;Testing of Image Processing Algorithms on Synthetic Data;Not health related;Not health related;0
"T. Varga; H. Bunke";2004;In this paper the problem of off-line handwritten cursive text recognition is considered. A method for expanding the set of available training textlines by applying random perturbations is presented. The goal is to improve the recognition performance of an off-line handwritten textline recognizer by providing it with additional synthetic training data. Three important issues - quality, variability, and capacity - related to this method are discussed, and a basic strategy to make use of the possibility of expanding the training set by synthetic textlines is proposed. It is shown that significant improvement of the recognition performance is possible even when the original training set is large and the textlines are provided by many different writers.;Off-line handwritten textline recognition using a mixture of natural and synthetic training data;Not health related;Not health related;0
"L. H. Nguyen; T. D. Tran";2011;In this paper, we propose a novel robust sparse-recovery technique that allows sub-Nyquist uniform under-sampling of wide-bandwidth radar data in real time (single observation). Although much of the information is lost in the received signal due to the low sampling rate, we hypothesize that each wide- bandwidth radar data record can be modeled as a superposition of many backscattered signals from reflective point targets in the scene. In other words, our proposed technique is based on direct sparse recovery via orthogonal matching pursuit using a special dictionary containing many time-delayed versions of the transmitted probing signal. Using data from the U.S. Army Research Laboratory (ARL) Ultra-Wideband (UWB) synthetic aperture radar (SAR), we show that the proposed sparse-recovery model- based (SMB) technique successfully models and synthesizes the returned radar data from real-world scenes using only an analytical waveform that models the transmitted signal and a handful of reflectivity coefficients. More importantly, the reconstructed SAR imagery using the SBM technique with data sampled at only 20% of the original sampling rate has a comparable signal-to-noise ratio (SNR) to the original SAR imagery. For comparison purpose, the paper also presents SAR images recovered from conventional interpolation techniques and the standard random projection based compressed sensing technique, both of which resulted in very poor SAR image quality at the same sub-Nyquist sampling rate (20%).;Robust recovery of synthetic aperture radar data from uniformly under-sampled measurements;Not health related;Not health related;0
"A. Pinceti; L. Sankar; O. Kosut";2021;A framework for the generation of synthetic time-series transmission-level load data is presented. Conditional generative adversarial networks are used to learn the patterns of a real dataset of hourly-sampled week-long load profiles and generate unique synthetic profiles on demand, based on the season and type of load required. Extensive testing of the generative model is performed to verify that the synthetic data fully captures the characteristics of real loads and that it can be used for downstream power system and/or machine learning applications.;Synthetic Time-Series Load Data via Conditional Generative Adversarial Networks;Not health related;Not health related;0
"Y. Hou; C. Li; Y. Lu; L. Zhu; Y. Li; H. Jia; X. Xie";2022;In this article, we propose a simulated crowd counting dataset CrowdX, which has a large scale, accurate labeling, parameterized realization, and high fidelity. The experimental results of using this dataset as data enhancement show that the performance of the proposed streamlined and efficient benchmark network ESA-Net can be improved by 8.4%. The other two classic heterogeneous architectures MCNN and CSRNet pre-trained on CrowdX also show significant performance improvements. Considering many influencing factors determine performance, such as background, camera angle, human density, and resolution. Although these factors are important, there is still a lack of research on how they affect crowd counting. Thanks to the CrowdX dataset with rich annotation information, we conduct a large number of data-driven comparative experiments to analyze these factors. Our research provides a reference for a deeper understanding of the crowd counting problem and puts forward some useful suggestions in the actual deployment of the algorithm.;Enhancing and Dissecting Crowd Counting by Synthetic Data;Not health related;Not health related;0
"D. Singh; V. V. Chamundeeswari; K. Singh; W. Wiesbeck";2008;Synthetic Aperture Radar (SAR) is capable of generating fine-resolution images of the earth terrain unhindered by weather and illumination conditions. These special properties develop interest in the researchers worldwide for maximum use of Radar images for various applications. In this chain, monitoring natural disaster with SAR images is one of the challenging research areas. The present paper deals with monitoring and observing the changes in subsidence due to natural disaster. For this purpose, New Orleans City of USA has been taken as a test area. MRD(Minimum Ratio detector) is applied on raw SAR data and D-InSAR on complex data to monitor and observe change detection due to subsidence in this area.;Monitoring and change detection of natural disaster (like subsidence) using Synthetic Aperture Radar (SAR) data;Not health related;Not health related;0
"S. Mohanty; G. Singh; C. S. Carrano; S. Sripathi";2018;Networks of ground-based global navigation satellite system (GNSS) receivers have been widely used to monitor scintillation caused by irregularities in the disturbed ionosphere. Due to the relative sparseness of such networks, however, scintillation measurements are lacking in many regions of the globe, and even in well-instrumented geographic areas, the spacing between receivers is often too large to study the systematic spatial changes in scintillation characteristics, for example, across the equatorial anomaly region. This paper discusses the potential of studying ionospheric scintillations using low-frequency synthetic aperture radar (SAR). It compares standard metrics of scintillation including the amplitude scintillation index S4 and vertically integrated strength of turbulence CkL, from GNSS and SAR, on two different dates with varying ionospheric conditions. For this study, polarimetric L-band SAR images acquired from the Phased Array-type L-band Synthetic Aperture Radar sensor onboard the Advanced Land Observational Satellite-2 have been used. A number of GNSS satellites also observed the particular scintillation event that was encountered by SAR on the night of 23 March 2015 over the southern and mid-central India. The S4 index derived from SAR are computed using previously published techniques in terms of radar backscatter (_°) enhancement and the image contrast. The results show a favorable correlation with the GNSS observations. Along with accurate information about satellite geometry and operating frequency, few spectral properties of ionospheric irregularities, such as spectral index, anisotropy, and outer scale, have been assumed from historically available low-latitude scintillation observations to calculate the turbulence strength parameter. The results are well corroborated by measurements from four GNSS stations in India, thus demonstrating the utility of the SAR measurements in augmenting and complementing the ionospheric scintillation diagnostics available from GNSS.;Ionospheric scintillation observation using space-borne synthetic aperture radar data;Not health related;Not health related;0
"S. Gaur; S. Sonkar; P. P. Roy";2015;This paper presents a novel approach to create synthetic dataset for word recognition systems. Our purpose is to improve performance of off-line handwritten text recognizers by providing it with additional synthetic training data. Due to lack of proper data-set for many languages it becomes hard to train recognition systems. To solve such problems synthetic handwriting could be used to expand the existing training dataset. Any available digital data from online newspaper and such sources can be used to generate this synthetic data. The digital data is distorted in such a way that the underlying pattern is conserved for identification of the word by both machine and human user. The images hence produced can be used to train any classification system for handwriting recognition. This data can be used independently to train the system or be combined with natural handwritten data to augment the original dataset and improve the accuracy of the results. We experimented using only synthetic data obtaining high recognition accuracy in both character and word recognition. The data was tested on 3 Indian scripts for numerals- Hindi, Bengali and Telugu, and 1 script-Hindi for words, the results achieved hence are highly promising.;Generation of synthetic training data for handwritten Indic script recognition;Not health related;Not health related;0
"Y. R. Pandeya; B. Bhattarai; J. Lee";2020;The sound event detection is a reasonable choice for several application domains like cattle shed, dense forest, or any dark environment where the visual object usually obscured or unseen. The aim of this study is the development of an autonomous monitoring system for welfare management in large cow farms based on sound characteristics. In this paper, we prepare a cow sound artificial dataset and develop a sound event annotation tool for annotation of data. We propose a convolutional neural network (CNN) architecture for rare sound event detection. The applied object detection method achieves a higher quantitative evaluation score and a more precise qualitative result than the past related study. Finally, we conclude that the CNN based architecture for rare sound object detection can be one solution for domestic welfare management. Indeed, the artificial data preparation strategy can be a way to deal with the data scarcity problem and annotation difficulties for rare sound event detection.;Sound Event Detection in Cowshed using Synthetic Data and Convolutional Neural Network;Not health related;Not health related;0
"M. Babaee; A. R. N. Nilchi";2014;Image processing softwares, like all softwares, need to be both verified and validated. Synthetic images are very useful during the medical software development process to verify the accuracy of algorithms. In this paper we introduce the process of generating synthetic 2D medical X-ray images in addition to ground truth imaging parameters. First, a 3D model of an organ (e.g., vessels) is made in a 3D-modeling software. Then, this volume model is voxelized based on the specified resolution in order to create a 3D CT image of that organ by assigning proper Hounsfield unit to each voxel. The obtained 3D CT image volume is used in DRR program as the input. Geometry parameters such as internal and external parameters are adjusted to take some images from different views. We demonstrated this process by three examples to confirm its usage in validation of medical image processing applications.;Synthetic data generation for X-ray imaging;health related;Not health related;1
"X. Tu; M. S. Zhdanov";2021;Towed streamer electromagnetic (TSEM) survey is an efficient data acquisition technique capable of collecting a large volume of electromagnetic (EM) data over extensive areas rapidly and economically. The TSEM survey is capable of detecting and characterizing marine hydrocarbon (HC) reservoirs. However, interpretation of the TSEM data is still a very challenging problem. We propose solving this problem by migrating the optimal synthetic aperture (OSA) data for the TSEM survey. We first represent the OSA data as a solution of Lippmann-Schwinger equation and then demonstrate that the migration of OSA data is just the inner product of the backward-propagated and forward-propagated EM fields. The migration problem is solved iteratively within the general framework of the reweighted, regularized, conjugate gradient (RRCG) method. The proposed method was tested with two synthetic models. We also applied this method to the TSEM data set collected in the Barents Sea and revealed a resistive layer at a depth of about 500 m.;Least Squares Migration of Synthetic Aperture Data for Towed Streamer Electromagnetic Survey;Not health related;Not health related;0
"R. L. Price; J. Ramirez; T. V. Rovito; O. Mendoza-Schrock";2012;This paper will look at using open source tools (Blender, LuxRender, and Python) to generate a large data set to be used to train an object recognition system. The model produces camera position, camera attitude, and synthetic camera data that can be used for exploitation purposes. We focus on electro-optical (EO) visible sensors to simplify the rendering but this work could be extended to use other rendering tools that support different modalities. The key idea of this paper is to provide an architecture to produce synthetic training data which is modular in design and constructed on open-source off-the-shelf software yielding a physics accurate virtual model of the object we want to recognize. For this paper the objects we are focused on are civilian vehicles. This architecture shows how leveraging existing open-source software allows for practical training of Electro-Optical object recognition algorithms.;Electro-optical synthetic civilian vehicle data domes;Not health related;Not health related;0
J. Ahrens;2019;The spatial decomposition method decomposes acoustic room impulse responses into a pressure signal and a direction of arrival for each time instant of the pressure signal. An acoustic space can be auralized by distributing the pressure signal over the available loudspeakers or head-related transfer functions so that the required instantaneous propagation direction is recreated. We present a user study that demonstrates based on binaural auralization that the arrival directions can be synthesized from random data such that the auralization is nearly indistinguishable from the auralization of the original data. The presented concept constitutes the fundament of a highly scalable spatialization method for omnidirectional room impulse responses.;Auralization of Omnidirectional Room Impulse Responses Based on the Spatial Decomposition Method and Synthetic Spatial Data;Not health related;Not health related;0
"X. Liu; M. Serhir; A. Kameni; M. Lambert; L. Pichon";2017;The localization of buried targets using Ground Penetrating Radar (GPR) is dealt with. The bi-static GPR is made of two identical Vivaldi antennas operating from 0.5 GHz to 3.5 GHz and installed in front of a sand box. The experimental data acquired in a controlled laboratory environment are validated by electromagnetic simulation. Then, both synthetic and experimental data are processed to detect the buried targets via three imaging methods. The targets localization is achieved from the GPR B-scan data. The results obtained via Stripmap SAR, Frequency-Wavenumber (F-K) migration and Kirchhoff migration are presented and compared.;Buried targets detection from synthetic anc measured B-scan ground penetrating radar data;Not health related;Not health related;0
"P. Sasmal; M. K. Bhuyan; S. Sonowal; Y. Iwahori; K. Kasugai";2020;Early diagnosis of cancer in polyps detected in the endoscopic video frames helps in better prognosis and clinical management. For this, the polyp regions are exhaustively analyzed by an endoscopist. In this paper, an automated polyp classifier in a deep learning framework is proposed. As the availability of ground truth data for colonic polyp is always in paucity, data augmentation is indispensable in such task. Our work proposes the use of Generative adversial networks (GANs) for synthetic data generation. For classification, a CNN is trained which discriminate between normal (benign) and cancer (malignanat) polyps. Experiments carried on two databases prove that the proposed data augmentation technique can efficiently be used in the classification of colonic polyps. Also, our proposed method compares the performance achieved using classical augmentation approach which is generally considered in limited data scenario. Experimental results show that the classification accuracy is competitive to the state-of-the-art methods.;Improved Endoscopic Polyp Classification using GAN Generated Synthetic Data Augmentation;health related;Not health related;1
"H. Kloos; J. P. Wittenburg; W. Hinrichs; H. Lieske; L. Friebe; P. Pirsch";2000;The authors present the HiPAR-DSP 16, a parallel and programmable architecture which is adapted to the demands of SAR image processing. To provide a high FFT performance, the HiPAR-DSP 16 features an array of 16 parallel processing units. Efficient data exchange between the processing units can be done by a shared memory with concurrent access. The HiPAR-DSP 16 is able to perform a range compression of 2100 lines (4k complex samples) per second. Therefore the HiPAR-DSP is one of the enabling technologies for onboard real-time processing of SAR images.;HiPAR-DSP 16: A parallel DSP for onboard real-time processing of synthetic aperture radar data;Not health related;Not health related;0
"A. Risti_; M. Vrtunski; M. Govedarica; L. Pajewski; X. Derobert";2017;The main goal of this paper is to investigate the performance of an algorithm for point extraction from hyperbolic reflections in synthetic and real Ground-Penetrating Radar (GPR) data. The real radargrams that we considered contain hyperbolic reflections due to the presence, in the surveyed area, of district heating pipelines DN250 (250mm inner diameter pipe). These are buried 88 cm deep in a soil trench, and covered by compacted sand and concrete bricks (behaton pavement). The synthetic radargrams result from the simulation of a model representing the real geometry on the location of interest. The simulation was carried out by using gprMax, ver. 3.;Automated data extraction from synthetic and real radargrams of district heating pipelines;Not health related;Not health related;0
"T. Konidaris; B. Gatos; S. J. Perantonis; A. Kesidis";2008;In this paper we propose a novel and efficient technique for finding keywords typed by the user in digitised machine-printed historical documents using the dynamic time warping (DTW) algorithm. The method uses word portions located at the beginning and end of each segmented word of the processed documents and try to estimate the position of the first and last characters in order to reduce the list of candidate words. Since DTW can become computational intensive in large datasets the proposed method manages to significantly prune the list of candidate words thus, speeding up the entire process. Word length is also used as a means of further reducing the data to be processed. Results are improved in terms of time and efficiency compared to those produced if no pruning is done to the list of candidate words.;Keyword Matching in Historical Machine-Printed Documents Using Synthetic Data, Word Portions and Dynamic Time Warping;Not health related;Not health related;0
"D. J. Leamy; T. E. Ward; K. T. Sweeney";2011;Accurately modelled computer-generated data can be used in place of real-world signals for the design, test and validation of signal processing techniques in situations where real data is difficult to obtain. Bio-signal processing researchers interested in working with fNIRS data are restricted due to the lack of freely available fNIRS data and by the prohibitively expensive cost of fNIRS systems. We present a simplified mathematical description and associated MATLAB implementation of model-based synthetic fNIRS data which could be used by researchers to develop fNIRS signal processing techniques. The software, which is freely available, allows users to generate fNIRS data with control over a wide range of parameters and allows for fine-tuning of the synthetic data. We demonstrate how the model can be used to generate raw fNIRS data similar to recorded fNIRS signals. Signal processing steps were then applied to both the real and synthetic data. Visual comparisons between the temporal and spectral properties of the real and synthetic data show similarity. This paper demonstrates that our model for generating synthetic fNIRS data can replicate real fNIRS recordings.;Functional Near Infrared Spectroscopy (fNIRS) synthetic data generation;Not health related;Not health related;0
"L. Naidoo; R. Mathieu; R. Main; W. Kleynhans; K. Wessels; G. P. Asner; B. Leblon";2014;The woody component in African Savannahs provides essential ecosystem services such as fuel wood and construction timber to large populations of rural communities. Woody canopy cover (i.e. the percentage area occupied by woody canopy or CC) is a key parameter of the woody component. Synthetic Aperture Radar (SAR) is effective at assessing the woody component, because of its capacity to image within-canopy properties of the vegetation while offering an all-weather capacity to map relatively large extents of the woody component. This study compared the modelling accuracies of woody canopy cover (CC), in South African Savannahs, through the assessment of a set of modelling approaches (Linear Regression, Support Vector Machines, REPTree decision tree, Artificial Neural Network and Random Forest) with the use of X-band (TerraSAR-X), C-band (RADARSAT-2) and L-band (ALOS PALSAR) datasets. This study illustrated that the ANN, REPTree and RF non-parametric modelling algorithms were the most ideal with high CC prediction accuracies throughout the different scenarios. Results also illustrated that the acquisition of L-band data be prioritized due to the high accuracies achieved by the L-band dataset alone in comparison to the individual shorter wavelengths. The study provides promising results for developing regional savannah woody cover maps using limited LiDAR training data and SAR images.;The assessment of data mining algorithms for modelling Savannah Woody cover using multi-frequency (X-, C- and L-band) synthetic aperture radar (SAR) datasets;Not health related;Not health related;0
"C. H. Schmidt; D. T. Schobert; T. F. Eibert";2011;Measurements in the antenna near field allow to compute the far-field radiation pattern by a postprocessing near-field far-field transformation. These algorithms use equivalent sources to model the radiation behavior of the antenna and have to consider the field probe influence present in near-field antenna measurements when relating the measured probe signals to the equivalent sources. In order to test and validate near-field transformation algorithms without errors introduced by the measurement setup and environment, synthetic data is often used. However, in order to generate realistic near-field data the probe effects on the measurement have to be considered. In this paper a synthetic data generation technique is shown which models the antenna as well as the probe by distributions of electric dipoles. The probe signals are obtained by evaluating the dyadic Green's function of free space for all antenna-probe-dipole combinations and finally superimposing them. This allows to synthesize near-field measurement scenarios with manifold antennas and probes very flexibly. The technique is applied to validate the plane wave based near-field far-field transformation algorithm.;Electric dipole based synthetic data generation for probe-corrected near-field antenna measurements;Not health related;Not health related;0
"F. L. Bouquet; J. W. Winslow";1984;Spacecraft, nuclear power plant equipment, and other engineering systems usually include components made from radiation-sensitive materials. One common class of such materials is synthetic organic insulation and dielectrics. Equipment designers and qualification engineers need reliable data describing the effects of ionizing radiation on materials of this class. Existing radiation response data for 130 materials of this class have been tabulated as part of a more comprehensive effort. This paper presents graphical presentations of thresholds and 25 percent change dose levels taken from the final report [1] for that project, along with a brief review of the field, including background and cautions to be observed in applying the data to current problems.;Radiation Effects Response Data for Synthetic Organic Insulation and Dielectrics;Not health related;Not health related;0
"M. Dorodchi; E. Al-Hossami; A. Benedict; E. Demeter";2019;Data sharing is a common contribution to open science. The creation of open datasets can speed up research advancements by allowing researchers to focus efforts on developing and validating analytical techniques, rather than on obtaining data. Open datasets also allow researchers to benchmark new analytical approaches against a known standard, and increase the reproducibility of research. The field of higher education learning analytics could benefit from the creation of open, shared datasets on higher education students as these data do not currently exist in open and accessible formats. Here, we propose the use of synthetic data generators to create open access versions of student data. Synthetic datasets have an advantage over real data, as private student data is protected by federal laws. We compare the characteristics of the synthetic data to the original data and illustrate a model for how the synthetic data can be leveraged for developing and optimizing a common learning analytics algorithm.;Using Synthetic Data Generators to Promote Open Science in Higher Education Learning Analytics;Not health related;Not health related;0
"Z. Luo; X. Jiang; X. Liu";2020;The deep convolutional neural networks (CNNs) have achieved the state of art performance in synthetic aperture radar (SAR) automatic target recognition (ATR). However, these networks often provide sub-optimal recognition results in the case of imbalanced SAR data distribution. In this paper, a synthetic minority class data method for improving imbalanced SAR target recognition using the generative adversarial network (GAN) is proposed. The minority class SAR data is first over-sampled by optimized data augmentation policies from automatic search method, which enlarge the training set for GAN. The progressive growing of GANs (PGGAN) is then trained on these data and generates high quality and diverse minority class SAR data to alleviate imbalanced data distribution. Experimental results on the designed imbalanced distributed Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset indicate that our method can effectively improve the recognition accuracy of minority class by approximately 11.68%.;SYNTHETIC MINORITY CLASS DATA BY GENERATIVE ADVERSARIAL NETWORK FOR IMBALANCED SAR TARGET RECOGNITION;Not health related;Not health related;0
I. P. Kirsteins;2004;Multipath interference is a major source of interference for synthetic aperture sonar systems operating in shallow water at long ranges. In this paper we describe an iterative algorithm for blindly separating the signal from the multipath interference that uses differences in the temporal coherence properties of the signal and multipaths caused by sea surface roughness, the difficulties encountered in applying the algorithm to actual sea data, and the solutions and lessons learned.;Blind separation of interference for synthetic aperture sonar and the lessons learned from real data;Not health related;Not health related;0
"F. B. Gelderblom; Y. Liu; J. Kvam; T. A. Myrvoll";2021;This paper investigates the use of different room impulse response (RIR) simulation methods for synthesizing training data for deep neural network-based direction of arrival (DOA) estimation of speech in reverberant rooms.Different sets of synthetic RIRs are obtained using the image source method (ISM) and more advanced methods including diffuse reflections and/or source directivity. Multi-layer perceptron (MLP) deep neural network (DNN) models are trained on generalized cross correlation (GCC) features extracted for each set. Finally, models are tested on features obtained from measured RIRs.This study shows the importance of training with RIRs from directive sources, as resultant DOA models achieved up to 51% error reduction compared to the steered response power with phase transform (SRP-PHAT) baseline (significant with p << .01), while models trained with RIRs from omnidirectional sources did worse than the baseline. The performance difference was specifically present when estimating the azimuth of speakers not facing the array directly.;Synthetic Data For Dnn-Based Doa Estimation of Indoor Speech;Not health related;Not health related;0
"N. Hou; D. Zhang; G. Du; Y. Song";2014;Often graphic processing units (GPGPUs) are used for data processing as the range-Doppler algorithm is computationally expensive and highly parallel. However, GPGPUs may not be an appropriate solution for applications with strictly constrained space and power requirements. In this paper, we implemented a FPGA-based multi-core system for SAR data processing with Range-Doppler algorithm. Our system consists of 4 Xilinx Virtex-6-550T devices with speed grade -1 and several off-chip memory devices. We design a scalable multi-core processor consisting of 16 processing elements and a 2D-Mesh network-on-chip on the FPGAs. Our system calculates real-time SAR raw data in pipeline, and can acquire a 256 level gray SAR images containing 2048_4096 pixels in 12.03 second at the speed of 130MHz. We measure the power of the whole system is about 85 watt.;An FPGA-based multi-core system for synthetic aperture radar data processing;Not health related;Not health related;0
"V. -E. Neagoe; A. -D. Ciotec; L. Bruzzone";2019;"This paper proposes a novel approach to improve accuracy of weakly-supervised change detection in Synthetic Aperture Radar (SAR) imagery. The method is based on the idea to use an initial small size labeled dataset for synthetic training data generation (STDG) using an Ensemble of Self-Organizing Maps (ESOM). The resulting synthetic data set substitutes the initial small set of labeled authentic data and it is used to train a simple deep neural network (DNN) classifier. The proposed DNN architecture consists of 6 layers: an input layer, two Long Short-Term Memory (LSTM) layers, and three Fully Connected (FC) layers. The proposed method is evaluated on a TerraSAR-X image acquired in the Fukushima region, Japan, before and after tsunami. We have used only 200 labeled data (100 for the ""change"" class and 100 for the ""no-change"" class). As benchmark method, we have considered a Support Vector Machine (SVM) classifier. The experimental results confirmed the effectiveness of the proposed approach.";A Weakly-Supervised Change Detection Technique for SAR Images Based on Deep Learning and Synthetic Training Data Generated by an Ensemble of Self-Organizing Maps;Not health related;Not health related;0
N. G. Kasapo_lu;2011;Synthetic aperture radar (SAR) ScanSAR data has advantages on oceanographic remote sensing applications regarding its large coverage and sufficient resolution. However terrestrial downlink bandwidth is limited and therefore up to dual polarized (e.g., HH and HV) ScanSAR data can be achieved today's spaceborne systems (e.g., RadarSAT-2). In this study grey level co-occurrence matrix was employed to extract SAR features for both HH and HV channels. Additionally some of band math products such as HH/HV and HH-HV were used as candidate SAR features. Selection of optimum SAR features is crucial and application dependent. In this study, selection strategies based on SAR data assimilation was introduced and relation of conventional separability criterions on SAR data assimilation and pattern recognition were discussed.;Synthetic aperture radar feature selection for dual polarized ScanSAR data;Not health related;Not health related;0
"M. Andulkar; J. Hodapp; T. Reichling; M. Reichenbach; U. Berger";2018;As Convolutional Neural Network based models become reliable and efficient, two questions arise in relation to their applications for industrial purposes. The usefulness of these models in industrial environments and their implementation in these settings. This paper describes the autonomous generation of Region based CNN models trained on images from rendered CAD models and examines their applicability and performance for part handling application. The development of the automated synthetic data generation is detailed and two CNN models are trained with the aim to detect a car component and differentiate it against another similar looking part. The performance of these models is tested on real images and it was found that the proposed approach can be easily adopted for detecting a range of parts in arbitrary backgrounds. Moreover, the use of syntheic images for training CNNs automates the process of generating a detector.;Training CNNs from Synthetic Data for Part Handling in Industrial Environments;Not health related;Not health related;0
"S. Bhattacharya; O. Mazumder; D. Roy; A. Sinha; A. Ghose";2020;Synthetic data generation has recently emerged as a substitution technique for handling the problem of bulk data needed in training machine learning algorithms. Healthcare, primarily cardiovascular domain is a major area where synthetic physiological data can be used improve accuracy of machine learning algorithm. This paper presents a novel approach of generating synthetic Photoplethysmogram (PPG) data using statistical explosion. Synthetic data is subsequently used to classify Coronary Artery Disease (CAD) using a two stage cascaded classifier. Proposed classifier along with synthetic data removes class bias and provides better accuracy compared to state of art. The proposed data generation and cascaded classifier is generic enough to be used to improve machine learning algorithm on any time series signal.;Synthetic Data Generation Through Statistical Explosion: Improving Classification Accuracy of Coronary Artery Disease Using PPG;health related;health related;1
"Z. Li; R. M. Narayanan";2006;Although techniques for resolution enhancement in single-aspect radar imaging have made rapid progress in recent years, it does not necessarily imply that such enhanced images will improve target identification or recognition. However, when multiple looks of the same target from different aspects are obtained, the available knowledge base increases allowing more useful target information to be extracted. Physics based image fusion techniques can be developed by processing the raw data collected from multiple ISAR sensors, even if these individual images are at different resolutions. We derive an appropriate data fusion rule in order to generate a composite image containing increased target shape characteristics for improved target recognition. The rule maps multiple data sets collected by multiple radars with different system parameters on to the same spatial-frequency space. The composite image can be reconstructed using the inverse 2-D Fourier transform over the separated multiple integration areas. An algorithm called the matrix Fourier transform is created to realize such a complicated integral. This algorithm can be regarded as an exact interpolation, such that there is no information loss caused by data fusion. The rotation centers need to be carefully selected in order to properly register the multiple images before performing the fusion. A comparison of the IAR (Image Attribute Rating) curve between the fused image and the spatial-averaged images quantifies the improvement in the detected target features. The technique shows considerable improvement over a simple spatial averaging algorithm and thereby enhances target recognition.;Data Level Fusion of Multilook Inverse Synthetic Aperture Radar (ISAR) Images;Not health related;Not health related;0
N. Gökhan Kasapo_lu;2011;Data analysis based on a specific sensor has some limitations related to individual sensor properties and imaging geometry. Combining information from different sources diminishes these limitations and improves analysis. In this study zero dimensional variational (OD-Var) data assimilation method was used first time to combine passive microwave based analysis and synthetic aperture radar (SAR) data to reach improved sea ice analysis. For this purpose a simple geophysical model (forward model) were developed to map sea ice features into SAR feature space. Only passive microwave based analysis were utilized as background state and proper observed SAR features were employed to update background state by minimizing a cost function which depends on ice concentration as a sea ice feature. Manual image analysis charts were used for validation and improved sea ice analysis results were obtained by using the proposed method.;Synthetic aperture radar data assimilation for sea ice analysis;Not health related;Not health related;0
"J. Huh; K. Lee; I. Lee; S. Lee";2018;Environment recognition has been an important topic ever since the emergence of augmented reality (AR). For better experience in AR applications, environment recognition should be provided fast in real-time, where real-time object detection technologies could fulfill this requirement. However, training object detectors for AR specific scenarios are often troublesome. The real-time nature of AR produces visual degradations such as motion blur or occlusion by interaction, which make detectors trained with plain data difficult to detect objects exposed in such complex situations. Also, since gathering and labeling training data from scratch is a heavy burden, we need to resort to synthesized training data but previous synthetic data generation frameworks do not consider the aforementioned issue. Therefore, in this paper, we propose a new synthetic data generation framework which includes visual variations such as motion blur and occlusion occurred by distractors. By this simple modification, we show that including such variated data to the training dataset could dramatically improve realtime performance of object detectors by a high margin. Also, we stress that synthesizing training data with no more than three objects per image can achieve competitive performance compared to detectors trained with over four present in a single image. Experimental results both quantitatively and qualitatively supports our statements and shows the superiority of our method.;A Simple Method on Generating Synthetic Data for Training Real-time Object Detection Networks;Not health related;Not health related;0
"S. Yang; Y. Zhou; Y. Guo; R. A. Farneth; I. Marsic; B. S. Randall";2017;Process mining techniques have been applied to the visualization, interpretation, and analysis of medical processes. However, only a very limited amount of process data necessary for these analyses is publicly available, especially in the medical field because of patients' privacy. This limits novel medical process research to using insufficiently large or randomly-generated synthetic datasets. Our goal in this study is to train a model (using a limited amount of observed process data) that can generate large amounts of semi-synthetic process data. This generated data has characteristics similar to those of real-world process data, and could potentially be observed in reality.;Semi-Synthetic Trauma Resuscitation Process Data Generator;health related;Not health related;1
"B. Bhanu; G. Jones";2000;This paper outlines an approach and experimental results for synthetic aperture radar (SAR) object recognition using the MSTAR data. With SAR scattering center locations and magnitudes as features, the invariance of these features is shown with object articulation (e.g., rotation of a tank turret) and with external configuration variants. This scatterer location and magnitude quasi-invariance is used as a basis for development of a SAR recognition system that successfully identifies articulated and non-standard configuration vehicles based on non-articulated, standard recognition models. The forced recognition results and pose accuracy are given. The effect of different confusers on the receiver operating characteristic (ROC) curves are illustrated along with ROC curves for configuration variants, articulations and small changes in depression angle. Results are given that show that integrating the results of multiple recognizers can lead to significantly improved performance over the single best recognizer.;Object recognition results using MSTAR synthetic aperture radar data;Not health related;Not health related;0
"A. A. Nielsen; H. Skriver; K. Conradsen";2007;Multi-look, polarimetric synthetic aperture radar (SAR) data are often worked with in the so-called covariance matrix representation. For each pixel this representation gives a 3 times 3 Hermitian, positive definite matrix which follows a complex Wishart distribution. Based on this distribution a test statistic for equality of two such matrices and an associated asymptotic probability for obtaining a smaller value of the test statistic are given and applied to change detection, edge detection and segmentation in polarimetric SAR data. In a case study EMISAR L-band data from 17 April 1998 and 20 May 1998 covering agricultural fields near Foulum, Denmark, are used. Soon the Japanese ALOS, the German TerraSAR-X and the Canadian RADARSAT-2 will acquire space-borne, polarimetric data making analysis based on these methods important.;Complex Wishart Distribution Based Analysis of Polarimetric Synthetic Aperture Radar Data;Not health related;Not health related;0
"L. Dabbiru; J. V. Aanstoos; M. Mahrooghy; W. Li; A. Shanker; N. H. Younan";2012;This research presents results of applying the NASA JPL's Uninhabited Aerial Vehicle Synthetic Aperture Radar (UAVSAR) quad-polarized L-band data to detect anomalies on earthen levees. Two types of problems / anomalies that occur along these levees which can be precursors to complete failure during a high water event are slough slides and sand boils. The study area encompasses a portion of levees of the lower Mississippi river in the United States. Supervised and unsupervised classification techniques have been employed to detect slough slides along the levee. RX detector, a training-free classification scheme is introduced to detect anomalies on the levee and the results are compared with the k-means clustering algorithm. Using the available ground truth data, a supervised kernel based classification technique using a Support Vector Machine (SVM) is applied for binary classification of slides on the levee versus the healthy levee and the performance is compared with a neural network classifier.;Levee anomaly detection using polarimetric synthetic aperture radar data;Not health related;Not health related;0
"B. Jeli_; R. Grbi_; M. Vranje_; D. Miji_";2023;Deep learning models require vast amounts of annotated training data. Gathering and annotating the data from the real world is an expensive and time-consuming process. Thus, synthetically generated data are being researched more and more. This article tries to answer the question of whether and to what extent synthetically generated data can help in developing the advanced driver-assistance system algorithms for autonomous vehicles, for the object detection task based on deep learning. Two state-of-the-art deep learning object detectors were trained on various combinations of real-world and synthetic data. A total of 12 detectors were tested on real-world test images. Results show that synthetic data can contribute to better detector performance until a certain ratio between real-world and synthetic data is reached.;Can We Replace Real-World With Synthetic Data in Deep Learning-Based ADAS Algorithm Development?;Not health related;Not health related;0
"S. Tripathi; R. Prakash; N. Melkani; S. Kumar";2015;In this paper we describe a probabilistic approach of image classification based on bayes classifier for SAR (Synthetic Aperture Radar) data. It classifies the images of different polarizations as well as the combination of these polarizations. The classification is carried out on the HH, HV and VV polarized image, their combination HH-HV, HH-VV and HV-VV and considering all the three polarized images, i.e., HH-HV-VV. It is an supervised classification approach which obtains the training data set from the region under observation. The results obtained are compared on the basis of better distinguishing ability of different classes by the classifier. The proposed classifier incorporates the use of baysian function for the evaluation of PDF (Probability Density Function) and with the consideration of PDF the decision function allocates a particular class to a pixel which better suits to that pixel.;Study of Bayes Theorem for Classification of Synthetic Aperture Data;Not health related;Not health related;0
"D. Shen; B. Liu; X. Li";2019;The sea surface wind at low and moderate speed (< 20 m/s) can be well derived from Synthetic Aperture Radar (SAR) data by geophysical model functions (CMOD5, XMOD, CMOD-X0, or others). However, wind retrieval bias is usually large at high wind speed (> 20 m/s). In this study, we explore to use a novel deep convolutional neural networks (DCNN) architecture, U-Net, to retrieve the sea surface wind at high speed. The Sentinel-1B SAR cross-polarized (HV and VH) Normalized Radar Cross Section (NRCS), SAR incidence angle and the ancillary Global Forecast System (GFS) model wind directions are used as the U-Net input data. The GFS model wind speed is the ground-truth data. The results suggest that the U-Net wind retrieval model has a capability to retrieve the high wind speed from SAR data with the physical model-based data. The accuracy of physical model data directly affects the U-Net model results. Meanwhile, U-Net model has a better continuity in the joint area of two strips.;Sea Surface Wind Retrieval from Synthetic Aperture Radar Data by Deep Convolutional Neural Networks;Not health related;Not health related;0
"G. Papamakarios; D. Giakoumis; K. Votis; S. Segkouli; D. Tzovaras; C. Karagiannidis";2014;In-house automatic activity detection is highly important toward the automatic evaluation of the resident's cognitive state. However, current activity detection systems suffer from the demand for on-site acquisition of large amounts of ground truth data for training purposes, which poses a major obstacle to their real-world applicability. In this paper, focusing on resident location trajectory-based activity recognition through limited amount of low-cost cameras, we introduce a novel scheme for automatic ground truth data generation, via simulation of resident trajectories based on formal descriptions of activities. Additionally, we present an activity detection scheme capable of learning activity patterns from such synthetic ground truth data. Experimental results show that our methodology achieves activity detection performance that is comparable to state-of-art methods, while suppressing the need for any actual ground truth recordings, thus boosting the real-world applicability of practical activity detection systems.;Synthetic ground truth data generation for automatic trajectory-based ADL detection;Not health related;Not health related;0
"R. Liu; B. Fang; Y. Y. Tang; P. P. K. Chan";2016;A standard data set is useful to empirically evaluate classification rules learning algorithms. However, there is still no standard data set which is common enough for various situations. Data sets from the real world are limited to specific applications. The sizes of attributes, the rules and samples of the real data are fixed. A data generator is proposed here to produce synthetic data set which can be as big as the experiments demand. The size of attributes, rules, and samples of the synthetic data sets can be easily changed to meet the demands of evaluation on different learning algorithms. In the generator, related attributes are created at first. And then, rules are created based on the attributes. Samples are produced following the rules. Three decision tree algorithms are evaluated used synthetic data sets produced by the proposed data generator.;Synthetic Data Generator for Classification Rules Learning;Not health related;Not health related;0
"J. Laviada; F. Las-Heras";2015;Off-axis holography techniques, which are commonly used to perform phase retrieval from amplitude-only data, require a full-characterization of the reference signal in terms of amplitude and phase. In this letter, it is demonstrated that the recently proposed version of the off-axis holography, working with a broadband synthetic aperture radar, can bypass the need for this calibration. The strategy is based on employing a calibration object to adjust the system. Furthermore, certain properties of the reflectivity calculation are exploited to correct potential misalignment errors. The performance of the approach is demonstrated by reconstructing the profile of several objects at K_-band.;Scalar Calibration for Broadband Synthetic Aperture Radar Operating with Amplitude-Only Data;Not health related;Not health related;0
"J. Chen; J. S. Kim; C. Rabiti";2017;This paper focuses on probabilistic analysis of hybrid energy systems (HES), which integrate multiple energy inputs and multiple energy outputs for effective management of variability in renewable energy and grid demand. To characterize the volatility, a statistical model combining Fourier series and autoregressive moving average (ARMA) is used to generate synthetic weather condition (e.g., wind speed) and grid demand data. Specifically, Fourier series is used to model the seasonal trends in historical data, while ARMA is applied to characterize the autocorrelation in residue time series (e.g., measurements with seasonal trends subtracted). The synthetic data is shown to have same statistic characteristics with historical measurements, but possesses different temporal profile. The probabilistic analysis of a particular HES configuration is then performed, which consists of nuclear power plant, wind farm, battery storage, and desalination plant. Requirements on component ramping rate, and the effects of deploying different sizes of batteries in smoothing renewable variability, are all investigated.;Probabilistic analysis of hybrid energy systems using synthetic renewable and load data;Not health related;Not health related;0
"M. Miltner; P. P. Duan; M. U. de Haag";2014;System degradation due to sensor failures has been a contributing factor in many aircraft accidents. For example, an autopilot (A/P) disconnect due to incorrect airspeed data was a primary factor in the events leading to the crash of Air France Flight 447 (AF447) into the Atlantic Ocean. Similarly, on Turkish Airlines Flight 1951 (TK1951) a faulty radio altimeter lead to an unexpected retard of the autothrottle (A/T), and consequent lack of energy state and flight mode awareness by the flight crew resulted in a crash on approach to Amsterdam. This paper addresses a method to increase the continuity of the sensor information that drives the aircraft automation modes and human-machine interfaces by utilizing dissimilar sensors and systems to synthesize critical data in case of sensor failure. The two main topics in this paper are data synthesis methods in the event of sensor failures and the modelling of the automation mode logic based on the available literature. The latter topic provides the information requirements for continued operation of the automation and, therefore, identifies what information would require a synthesized replacement in the event of failure. All topics will form an important automation logic component of a simulation tool that is being built to evaluate predictive altering methods that are developed to improve flight crew's aircraft state awareness.;Modeling and utilization of synthetic data for improved automation and human-machine interface continuity;Not health related;Not health related;0
"A. Abubakar; M. Gemignani; C. F. Meschini Almeida";2019;This paper presents a methodology for optimal Battery Energy Storage System (BESS) sizing in photovoltaic (PV) systems. The method is based on an established mathematical relationship between generated energy, demand and storage, allowing one to determine energy deficit and supply interruption periods. Autoregressive and time series models were used to generate daily synthetic series data from a set of hourly estimated global irradiance for the city of Petrolina in Brazil. The proposed method facilitates the assessment of the performance of several possible BESS, indicate the risk of energy deficit and the possible frequency of energy interruption.;Battery Storage System Sizing Using Synthetic Series Data;Not health related;Not health related;0
"K. T. Hjelmervik; H. Berg; D. H. S. Stender; T. S. Såstad";2015;Modern anti-submarine warfare sonars are often designed with narrow beamwidths and wide frequency bandwidths in order to maximize spatial resolution and sonar performance. A known issue for high-resolution sonars in littoral environments, is the occurence of high false alarm rates. Increased false alarm rates increase the workload of sonar operators and also reduces the usefullness of automatic systems such as autonomous underwater vehicles, since their limited communication abilities hinder them from sharing large amounts of contacts. The false alarm rate may be reduced simply by increasing the threshold used in the detection process. However, this also reduces the probability of detecting actual targets. Automatic classification algorithms provide more sophisticated alternatives for false alarm reduction. The work presented here demonstrates an automatic classification algorithm on a data set collected in a littoral environment. The data set contains a large amount of false alarms, particularly close to the coast, but does not contain any submarine target detections. Synthetic submarine echoes are therefore added to the sonar data set. Six features are extracted from the hybrid synthetic-recorded data set. The features are fed into supervised machine learning schemes. The performance of each scheme is presented as receiver operating characteristic curves.;A hybrid recorded-synthetic sonar data set for validation of ASW classification algorithms;Not health related;Not health related;0
N. H. W. Eklund;2006;Avoidance of unscheduled downtime and costly secondary damage make the accurate prediction of equipment remaining useful life of enormous economic benefit to industry. The detection of faults is an important first step in building a prognostic reasoner. This paper describes an approach for improving the performance of fault detection systems that operate on time series data. The method generates synthetic data that closely matches the characteristics of the raw data. These synthetic data are used to develop and evaluate classification systems in the common situation where real labeled data is quite scarce. The real data can then be preserved for use as a test set to assess the performance of the classification system. Data collected from aircraft engine/airframe systems is used to assess the performance of the resulting classification system;Using Synthetic Data to Train an Accurate Real-World Fault Detection System;Not health related;Not health related;0
"C. J. Read; D. V. Arnold; D. M. Chabries; P. L. Jackson; R. W. Christiansen";1988;A convolution technique is proposed that allows direct reconstruction of the processed synthetic-aperture radar (SAR) image from the digitally-sampled, block-encoded raw data. This computational compression technique reduces the number of arithmetic operations from that required by fast Fourier transform (FFT) convolution for SAR processing. SAR phase histories are block encoded and directly processed into an image where only arithmetic additions are required for the processing. For SAR data previously block encoded, the processing time is reduced by a factor of 100 or more. A speed-up of three times over SAR processing by FET convolution has been demonstrated when both computation of the block encoding and subsequent direct processing are included in the time. SAR image quality measurements for a method of block encoding called vector quantization at compression ration ranging from 5:1 to 50:1 show image degradation proportional to the compression ratio. For a 5:1 compression radio, image quality measurements show minimal degradation.<>;Synthetic aperture radar image formation from compressed data using a new computation technique;Not health related;Not health related;0
"R. Hori; R. Hachiuma; H. Saito; M. Isogawa; D. Mikami";2021;In this paper, we propose a framework for 3D human pose estimation with a single 360° camera mounted on the user’s wrist. Perceiving a 3D human pose with such a simple setting has remarkable potential for various applications (e.g., daily-living activity monitoring, motion analysis for sports enhancement). However, no existing work has tackled this task due to the difficulty of estimating a human pose from a single camera image in which only a part of the human body is captured and the lack of training data. Therefore, we propose an effective method for translating wrist-mounted 360° camera images into 3D human poses. We also propose silhouette-based synthetic data generation dedicated to this task, which enables us to bridge the domain gap between real-world data and synthetic data. We achieved higher estimation accuracy quantitatively and qualitatively compared with other baseline methods.;Silhouette-Based Synthetic Data Generation For 3D Human Pose Estimation With A Single Wrist-Mounted 360° Camera;Not health related;Not health related;0
"K. O. Babalola; O. B. Jennings; E. Urdiales; J. A. DeBardelaben";2018;This document outlines and demonstrates an approach to generating a synthetic email dataset using the Enron email dataset as a reference and the Synthetic Transaction Data Generator (STDG) simulator application. With statistical measures extracted from the Enron dataset, we generate synthetic email threads within the confines of a fictitious corporate structure. Our approach extrapolates the network structure of who communicates with whom and the timing of these communications, but it ignores semantic content. To this end, we first harvest the statistical network and temporal features of the Enron reference dataset. With these features, we use the agent-based STDG application to stochastically generate corporations of arbitrary sizes and email transactions within the corporations over a specified time period. We evaluate our methodology by comparing features of the synthetically-generated datasets with those of the reference dataset.;Statistical Methods for Generating Synthetic Email Data Sets;Not health related;Not health related;0
"S. Parashar; G. J. Wessels";1989;;Synthetic Aperture Radar Data Simulation Software Package At Ccrs;Not health related;Not health related;0
F. Biondi;2016;The problem of chirped Synthetic Aperture Radar (SAR) is the high vulnerability of the received information, under Electromagnetic (EM) Corruption. This paper proposes a valid recovery solution of SAR Single Look Complex (SLC)1 images observed with low band signals. The recovery of high resolution images is made by Super-Resolution (SR) Signal Processing (SP), based on Spectrum Extrapolation (SE), implemented by Convex Programming (CP).;Super resolution of synthetic aperture radar data by convex optimization;Not health related;Not health related;0
"Y. Wang; J. Yang; J. -w. Li";2017;"The application of the spaceborne stripmap range sweep synthetic aperture radar (SS-RSSAR) is limited by its fixed azimuth beam pointing. In this paper a more advanced spaceborne azimuth-range sweep synthetic aperture radar (ARSSAR) is proposed to illuminate the region of interest (ROI) by simultaneously sweeping the beam in range and in azimuth. In this case, more balances of the azimuth resolution and the azimuth swath can be flexibly realized. The basic data acquisition problem for the spaceborne ARSSAR is analyzed from two aspects: firstly, an optimized beam illumination strategy is achieved by minimizing the width of the echoes from the ROI-strip; secondly, a continuous varying pulse interval (CVPI) method is suggested to avoid the transmission blockage and maximize the allowable width of the ROI-strip. The presented approach is evaluated by the simulations.";Data acquisition for a novel spaceborne azimuth-range sweep synthetic aperture radar;Not health related;Not health related;0
"S. J. Sanabria; T. Brevett; R. Ali; A. Telichko; J. Dahl";2022;Speed-of-sound (SoS) reconstruction in pulse-echo ultrasound can reduce aberrations in imaging and provide quantitative biomarkers for diagnostics. State-of-the-art SoS imaging requires beamformed data for time-delay estimation. This translates either into multi-layered models, where absolute delays $\mathrm{t}_\mathrm{i}$ at multiple depths are fitted to axial SoS variations, or a perturbation approach, where differential delays between adjacent paths $\tau_\mathrm{i}$ are used to map SoS heterogeneities. In this work, we remove the beamforming constraint and propose a dual regularization method to combine absolute and differential delays into a generalized SoS reconstruction. Our approach provides quantitative reconstructions in both layered distributions and focal lesions with bias < 2.3 m/s and rmse < 6 m/s. Moreover, it allows scatterer localization with sub-micron resolution. We show experimental results in SoS reconstruction through heterogeneous aberrating layers.;Direct Speed of Sound Reconstruction from Full-Synthetic Aperture Data with Dual Regularization;Not health related;Not health related;0
"H. Gao; B. Heyde; J. D'hooge";2013;"In recent years, echo particle image velocimetry (EPIV) has been proposed to characterize intra-cardiac flow fields noninvasively. To date, these methods remain two dimensional (2D). As volumetric cardiac ultrasound has become more readily available and as this may avoid some of the problems inherent to 2D imaging (i.e. out-of-plane flow), the aim of this study was to test whether EPIV could be extended to three dimensions using synthetic data sets. A typical inflow velocity profile and a dynamic LV volume change profile combined with a three-dimensional (3D) anatomical model of the LV were used to build a realistic computational fluid dynamics (CFD) flow vector field (Fluent 12.1, ANSYS). 3D B-mode image sequences were acquired using a fast ultrasound simulator (COLE) of virtual contrast bubbles moving along this 3D flow field (frequency = 4.5MHz; sampling frequency = 50MHz; 50_50 degrees opening angle; frame rate = 113 Hz; image size 120_60_60mm). Inter-frame motion during diastasis was estimated by volumetric speckle tracking (kernel 0.65_6.70_6.70mm; search region 4.96_9.71_8.85mm; overlap 40%_90%_90%) using 3D normalized cross-correlation as a similarity metric. Cubic spline interpolation was embedded for subsample motion estimation prior to median filtering (6.34_4.73_4.75mm) of the resulting velocity vector field. Regression and correlation coefficients as well as the estimate error of the tracking velocity field with the CFD reference ground truth were calculated. The relative point-wise velocity vector error averaged over the period of E wave, A wave and systole were 14.66 ± 2.66%, 12.61 ± 1.15%, 14.00 ± 7.60% respectively. The flow field obtained through speckle tracking agreed well with the ones measured from CFD when the velocity amplitude was below 0.35 m/s. However, for larger velocities, the regional motion was underestimated.";3D Intra-cardiac flow estimation using speckle tracking: A feasibility study in synthetic ultrasound data;Not health related;Not health related;0
"F. Koltuk; E. G. Schmidt";2020;Cloud data center workloads have time- dependencies and are hence non-i.i.d (independent and identically distributed). In this paper, we propose a new model-based method for creating synthetic workload traces for cloud data centers that have similar time characteristics and cumulative distributions to those of the actual traces. We evaluate our method using the actual resource request traces of Azure collected in 2019 and the well-known Google cloud trace. Our method enables generating synthetic traces that can be used for a more realistic evaluation of cloud data centers.;A Novel Method for the Synthetic Generation of Non-I.I.D Workloads for Cloud Data Centers;Not health related;Not health related;0
"F. Romanelli; F. Martinelli";2023;In recent years, deep learning techniques have revolutionized the field of data generation, including the creation of synthetic sensor data. The ability to generate large quantities of diverse, high-quality data have significant implications in fields such as robotics and computer vision. Synthetic sensor data generation using deep learning techniques involves training a model to generate data that closely resembles real-world sensor data. This is achieved by feeding the model large amounts of real-world data and using it to learn the underlying patterns and structures in the data. Once trained, the model can generate data that are similar in quality and complexity to the original data, but with added variations and noise to increase diversity and realism. Several deep learning techniques such as generative adversarial networks (GANs), variational autoencoders (VAEs), recurrent neural networks (RNNs) have shown impressive results in generating synthetic data for a range of sensors. In this letter, deep learning techniques based on autoregressive convolutional recurrent neural networks (CRNNs) for multivariate time series prediction have been exploited to generate synthetic data for ultrawide band (UWB) and for ultrahigh frequency radio frequency identification (UHF-RFID) sensors. The neural network presented here incorporates measurements from sensors and heterogeneous information, such as the position of the antennas and tags in the environment, to generate synthetic data that can be used to augment real-world data, increasing diversity and robustness of datasets. The deep generation approaches presented here can help researchers generate datasets to validate algorithms without the need for expensive and time-consuming data collection.;Synthetic Sensor Data Generation Exploiting Deep Learning Techniques and Multimodal Information;Not health related;Not health related;0
"R. W. Lowdermilk; F. J. Harris";2003;Synthetic instruments apply the flexibility and computational capabilities of a digital signal-processing platform to synthesize a wide variety of synthetic instruments. The emphasis presented in traditional descriptions of a synthetic instrument is the architecture and the capabilities of the digital signal processing (DSP) platform and the user interfaces to the same. In this paper we highlight and discuss considerations of the signal conditioning and signal collection tasks. The focal point of the signal collection process is the analog-to-digital converter (ADC), the element that defines the precision and the bandwidth of the sampled data representation of the signal processed by the DSP platform. Analog signal conditioning prior to the ADC performs the task of limiting the input signal bandwidth and possible translation of the spectral band center. Digital signal conditioning following the ADC continues to perform the same tasks by further limiting the bandwidth of the digitized input signal as well as performing spectral translation with appropriate sample rate changes. In addition, the post conversion process can correct gain, phase, and time delay imbalances between input signal paths as well as gain and phase distortion encountered in the analog signal path. We identify and address performance constraints of existing ADCs and present a number of signal processing-based options to enhance and extend the operating regimes of the analog signal conditioning and the ADC conversion process.;Signal conditioning and data collection in synthetic instruments [ATE systems];Not health related;Not health related;0
"K. Y. Lee; T. R. Bretschneider";2011;This paper addresses ship target detection in dual-frequency single polarization and fully polarimetric SAR data by using a global thresholding approach. The corresponding threshold values were determined from the gamma distribution (for single-polarization SAR data) and the chi-squared distribution (for fully polarimetric SAR data). Based on the experimental results, which were obtained from nine-look NASA/JPL AIRSAR POLSAR data, the complementary use of both the C- and L-band was found to remove ghost ships (due to azimuth ambiguity) and improve the resolution of ship targets. Moreover, it was observed that the use of fully polarimetric SAR information reduced undesired artifacts, which were particularly noticeable in the C-band single-polarization outputs.;Improved ship detection using dual-frequency polarimetric synthetic aperture radar data;Not health related;Not health related;0
"P. Hill; A. Achim; D. Bull";2010;The detection of underwater targets, such as mines, from sonar returns is a difficult task which is compounded by the complex and variable backgrounds found on the seabed. The developed system employs a classical training and classification structure giving a statistical characterisation of the background together with domain knowledge of typical target types. A set of ground truth labels have been produced for three given seabed test regions which contain a range of target types. The method identifies the centre of targets using log-Gabor, matched and shaped filters together with a Support Vector Machine (SVM) classifier. Subjective testing enabled the comparison of our automatic detection methods with the performance of expert operators. The automatic target detection method was found to offer performance at least as good as human operators on identical data (based on a small operator data set).;Underwater target detection in synthetic aperture sonar data;Not health related;Not health related;0
"K. Warman; K. Chick; E. Chang";2001;A key supporting technology for future naval capabilities (FNCs) in long-range search and survey will be synthetic aperture sonar (SAS). The few existing, R&D, SAS systems tend to have relatively narrow bandwidth and beamwidth. In order to attain FNC goals of high resolution at long ranges, system designs will be driven toward both larger bandwidth and wider beam-width. In this study we emulate broadband and wide-beam data in order to analyze their implications for SAS processing. We specifically break the processing into the three steps: motion estimation, motion compensation, and image formation. The modeled data include beam patterns that are wide and frequency dependent as expected for widebeam/broad-band systems. For such systems, the interplay between beam-width and bandwidth will cause the frequency content of the received waveforms to depend on both the range and the bearing of the target. This complicates motion estimates that are based on correlating signals from sequential pings. Because data are linear combinations of signals from potentially very different bearings, the motion compensation step is more complicated as well. Both the motion estimation and motion compensation steps are more naturally addressed after the data are transformed to range-bearing space.;Synthetic aperture sonar processing for widebeam/broadband data;Not health related;Not health related;0
"D. Horn; S. Houben";2018;Most modern computer vision techniques rely on large amounts of meticulously annotated data for training and evaluation. In close-to-market development, this demand is even higher since numerous common and-more important-less common situations have to be tested and must hence be covered datawise. However, gathering the necessary amount of data ready-labeled for the task at hand is a challenge of its own. Depending on the complexity of the objective and the chosen approach, the required amount of data can be vast. At the same time, the effort to capture all possible cases of a given problem grows with their variability. This makes recording new video data unfeasible, even impossible at times. In this work, we regard parking space classification as an exemplary application to target the imbalance of cost and benefit w.r.t. image data creation for machine learning approaches. We rely on a fully-fledged park deck simulation created with Unreal Engine 4 for data creation and replace all conventionally recorded and hand-labeled training data by automatically-annotated synthetic video data. We train several of-the-shelf classifiers with a common choice of feature inputs on synthetic images only and evaluate them on two realworld sequences of different outdoor car parks. We reach a classification performance that matches our previous work on this task in which all classifiers were developed solely with real-life video data.;Evaluation of Synthetic Video Data in Machine Learning Approaches for Parking Space Classification;Not health related;Not health related;0
"G. R. Chandra; K. Rajiv; B. B. Rao";2019;"In ground penetration radar (GPR) B-scan images the various features from the target object reflections. These features are used for locating the buried pipes accurately. Detecting the radius of the pipe is required to identify the fraudulent water connections by the various water board authorities. In this paper a method is proposed to detect the radius of the pipe from scanned GPR images. The proposed pipe radius detection method has four steps. First step is to identify the regions of 3-D GPR data. Here the important features to be considered are hyperbolic curves. In second step the pipe location is identified by connecting all hyperbolic signatures. The third step is to identify the path location and keep on increasing the II value until perfect circle is reached. The final step is pipe radius detection; here a midpoint of the expanded circle is calculated, then the radius is calculated using standard radius calculations. A synthetic 3-D pipe model is created and applied the proposed algorithms on this data to identify the radius of the pipe. The same proposed algorithm is applied on the actual GPR data and compared against the 3-D synthetic data.";Detecting Radius of Pipe in GPR Images and Comparing with 3D Synthetic Data;Not health related;Not health related;0
"A. Saepuloh; K. Wikantika; M. Urai";2015;Detecting ground surface changes at active volcanoes is crucial for better hazard mitigation. Ground based measurements are commonly used to detect surface changes especially on the flanks near to the summit. However, the selection of observation points is limited by field conditions such as rough terrain and topographical barrier. The few number of observation points may lead to miss-interpretation when the displacements related to magma ascent occurred beneath unobserved flanks. This classical problem is common for dormant volcanoes. Overcoming the problem, we used the Synthetic Aperture Radar (SAR) data to observe surface roughness changes at the summit of active volcanoes. This paper discussed the potential capability of the SAR backscattering intensity to observe ground surface changes in view point of surface roughness around the summit. We presented two study cases at Mt. Sinabung in North Sumatra and Mt. Merapi in Central Java - Indonesia. Mt. Merapi was selected for comparison following published references. Mt. Sinabung is currently active since the first phreatic eruption has been occurred in August 2010. Time series of lava dome roughness on SAR (drSAR) method were used to detect ground surface changes prior to the eruption. Based on this method, the ground surface at the summit of Mt. Sinabung changed from 5121 to 6584 m2. The temporal pattern slightly agreed to the ground surface changes at summit of Mt. Merapi prior to the 2010 eruptions. Observing ground surface changes related to surface roughness at the summit might be used as new tools for observing volcanic activity.;Observing lava dome roughness on synthetic aperture radar (SAR) data: Case study at Mt. Sinabung and Merapi — Indonesia;Not health related;Not health related;0
"T. Kubota; N. Yoshida; S. Shimizu; R. Oki; H. Hanado; T. Iguchi";2011;We have developed the GPM/DPR synthetic data from the TRMM/PR data for contribution to higher level algorithm development. The frequency of the PR is 13.8 GHz and it is necessary to estimate radar reflectivity factor (Z-factor) at the DPR frequencies. In this paper, Z-factors from rainfall signals are estimated at the DPR frequencies. The detectable rain range of the DPR is verified using the synthetic data from the PR observations on January and July 1998, which suggests that it is important to consider differences of the sensitivity between the KuPR-matched beams and the interlaced high-sensitivity beams of the KaPR. The estimation of the Z-factors at the DPR frequencies is applied to an experimental data on 15th March 2007 when the PR was operated according to the scanning geometry of the KaPR, and then the synthetic data are obtained with distinction of 2 kinds of KaPR beams.;Development of synthetic GPM/DPR data using KaPR sampling experiment of the TRMM/PR;Not health related;Not health related;0
"C. Boryan; Z. Yang; P. Willis; A. Sandborn";2019;"Early season crop identification is important for food security and economic stability. The USDA NASS uses optical data to provide acreage estimates, each June, to the NASS Agricultural Statistics Board. However, early season crop identification is difficult using optical data alone, because imagery is frequently cloudy during the spring. The purpose of this study is to determine whether using SAR and SAR texture can improve early season winter wheat identification compared to optical data alone. Study areas in the Missouri ""Bootheel"" (2017 growing season) and Northwest Texas (2018 growing season), United States (U.S.) are selected. The SAR data used in this study are Sentinel-1. Optical data include: Landsat 8, Disaster Monitoring Constellation, and Sentinel-2. Study results show that optical data with SAR achieved the highest winter wheat accuracies, 7.7% higher than optical data alone, in Missouri. Optical with SAR and SAR texture resulted in improved accuracies over optical alone, but only marginally, in Texas. These results indicate that optical and SAR, used together, can potentially improve early season crop identification.";Early Season Winter Wheat Identification Using Sentinel -1 Synthetic Aperture Radar (Sar) and Optical Data;Not health related;Not health related;0
"V. Guglielmi; F. Castanie; P. Piau";1995;"To increase the spatial resolution of synthetic aperture radar (SAR) images, the authors process the received data through super-resolution methods instead of conventional pulse compression. Since the problem of SAR processing is ""ill-posed"", ""regularization"" techniques must be applied by incorporating prior knowledge about the imaged scene.";Application of super-resolution methods to synthetic aperture radar data;Not health related;Not health related;0
"R. Haeusler; R. Klette";2012;We comparatively discuss a set of confidence measures for stereo analysis by testing them on semi-global matching (SGM) cost functions. The aim is a prediction of (potentially) erroneous areas in calculated disparity maps. The evaluation is done by using the sparsification technique which provides more information than commonly used RMS or NCC measures. We also present an approach for combining different confidence measures. This allows us to perform a quantisation of confidence estimates in terms of disparity errors.;Evaluation of stereo confidence measures on synthetic and recorded image data;Not health related;Not health related;0
E. H. Attia;2004;"In this paper, we discuss the application of the spatial correlation algorithm (SCA) (Attia and Steinberg, 1989; Attia, 1993; Attia, 1994), and some of its variants, to synthetic aperture LADAR (SAL) imaging. These algorithms (which derive the required phase corrections from range-resolved LADAR clutter received by the SAL sensor) are very robust, computationally efficient, perform very well in the presence of large uncorrelated phase errors, and do not require dominant scatterers of opportunity. Performance and limitations of the basic SCA are discussed. Dependence of the accuracy of the resulting phase corrections on the SNR per pulse and number of range cells available for estimating the corrections is presented in considerable detail. The impact of residual uncorrected phase errors on image SNR and dynamic range is statistically evaluated taking into account the total number of pulses coherently combined along the imaging baseline. Also, a method for exploiting special features of 3D interferometric SAL imaging for the purpose of improving phase correction accuracy is introduced.";Data-adaptive motion compensation for synthetic aperture LADAR;Not health related;Not health related;0
N. G. Kasapo_lu;2011;Data analysis only based on a specific sensor has always some limitations related to sensor properties and imaging geometry. Data analysis fusion is a novel method which was used in this study to diminish disadvantages of making analysis only an information source. In this work, zero dimensional variational (0D-Var) data assimilation method were used first time to combine analysis results of passive microwave and synthetic aperture radar (SAR) to make automated and reliable sea ice analysis. Simple geophysical models were developed to map sea ice features in to SAR features space. The manual image analysis charts were employed for validation.;Synthetic aperture radar and passive microwave data analysis fusion;Not health related;Not health related;0
"R. Karpinski; A. Belaïd";2019;This paper proposes a fully automatic new method for generating semi-synthetic images of historical documents to increase the number of training samples in small datasets. This method extracts and mixes background only images (BOI) with text only images (TOI) issued from two different sources to create semi-synthetic images. The TOIs are extracted with the help of a binary mask obtained by binarizing the image. The BOIs are reconstructed from the original image by replacing TOI pixels using an inpainting method. Finally, a TOI can be efficiently integrated in a BOI using the gradient domain, thus creating a new semi-synthetic image. The idea behind this technique is to automatically obtain documents close to real ones with different backgrounds to highlight the content. Experiments are conducted on the public HisDB dataset which contains few labeled images. We show that the proposed method improves the performance results of a semantic segmentation and baseline extraction task.;Semi-Synthetic Data Augmentation of Scanned Historical Documents;Not health related;Not health related;0
"L. Liang; J. Zhong; J. Liu; P. Li; C. Zhan; Z. Meng";2013;Wind power fluctuation is a major concern of large scale wind power grid integration. To test methods proposed for wind power grid integration, a large amount of wind data with time series are necessary and will be helpful to improve the methods. Meanwhile, due to the short operation history of most wind farms as well as limitations of data collections, the data obtained from wind farms could not satisfy the needs of data analysis. Consequently, synthetic generation of wind data series could be one of the effective solutions for this issue. In this paper, a method is presented for generating wind data series using Markov chain. Due to the high order Markov chain, the possibility matrix designed for a wind farm could cost a lot of memory, which is a problem with current computer technologies. Dynamic list will be introduced in this paper to reduce the memory required. Communication errors are un-avoidable on long way signal transmission between the control centre and wind farms. Missing of data always happens in the historical wind data series. Using these data to generate wind data series may result in some mistakes when searching related elements in the probability matrix. An adaptive method will be applied in this paper to solve the problem. The proposed method will be verified using a set of one-year historical data. The results show that the method could generate wind data series in an effective way.;An implementation of synthetic generation of wind data series;Not health related;Not health related;0
C. A. Robinson;2008;"Two NSF-funded studies (OISE-0417704; OISE-0513379) carried out by the author over SW Egypt (between 20-24.5degN and 25-32degE, in the East Oweinat and Tushka regions, respectively) used C-band Radarsat-1 SAR images and topographic data to understand groundwater distributions in the local aquifers that underlie the study areas. In these studies, areas with enhanced groundwater represent the best locations for groundwater development in order to address the water and food pressures that exist in the regions. In the East Oweinat area, small-scale agricultural development is already underway, while in Tushka there are plans for development in the near future. Radar waves are uniquely able to image beneath the desert sand in the eastern Sahara to reveal groundwater-related near-surface features, that is, the courses of ancient rivers and streams and faults and fractures. The depth of near-surface imaging for the datasets used is on the order of half a meter. The distribution of groundwater-recharge near-surface features was analyzed and the variation in groundwater storage was appraised. Three new areas have been identified as promising sites for agricultural development.";Understanding the Distribution of Groundwater Resources using Synthetic Aperture Radar Data over Southwest Egypt;Not health related;Not health related;0
"Z. Chen; A. Georgiadis";2018;Based on different projection geometry, a fisheye image can be presented as a parameterized non-rectilinear image. Deep neural networks(DNN) is one of the solutions for extracting the parameters for fisheye image feature expression. However, a number of images are required for training a reasonable prediction model for DNN. In this paper, we propose to extend the scale of the dataset using parameterized synthetic images which effectively boost the diversity of samples and avoid the limitation on the scale. To simulate different viewing angles and distances, we adopt controllable parameterized projection processes on transformation. The reliability of the proposed method is tested with the image captured by a fisheye camera. The synthetic fisheye image dataset is the first dataset that is developed by existing labeled perspective images. It is accessible via: http://www2.leuphana.de/misl/fisheye-data-set/.;Parameterized Synthetic Image Data Set for Fisheye Lens;Not health related;Not health related;0
"M. Rudorfer; L. Neumann; J. Krüger";2019;Deep Learning-based approaches for 3d object detection and 6d pose estimation typically require large amounts of labeled training data. Labeling image data is expensive and particularly the 6d pose information is difficult to obtain, as it requires a complex setup during image acquisition. Training with synthetic data is therefore very attractive. Large amounts of synthetic, labeled data can be generated, but it is not yet fully understood how certain aspects of data generation affect the detection and pose estimation performance. Our work therefore focuses on creating synthetic training data and investigating the effects on detection performance. We present two methods for data generation: rendering object views and pasting them on random background images, and simulating realistic scenes. The former is computationally simpler and achieved better results, but the detection performance is still very sensitive to small changes, e.g. the type of background images.;Towards Learning 3d Object Detection and 6d Pose Estimation from Synthetic Data;Not health related;Not health related;0
"D. Huang; M. Li; R. Zheng; S. Xu; J. Bi";2017;There are a few studies on Manchu recognition, and the existing methods are mainly based on segmentation on characters or strokes. Thus, their performances are strongly dependent on segmentation accuracy. In this paper, a whole word recognition method for segmentation-free Manchu word is proposed to avoid the mis-segmentation of Manchu word. Firstly, we build an initial Manchu word image dataset, and then augment it with synthetic data, which are harvested via structural distortions on Manchu word image. Secondly, the support vector machine classifier with polynomial kernel function combined with directed acyclic graph is used for classification of Manchu words from 2 to 100 classes. The experiment results show that the precise is 78% for the 100-way classification problem, even above 90% for classes less than 40. The synthetic data method proposed in this paper is an effective way to augment the training and test dataset for Manchu word recognition.;Synthetic Data and DAG-SVM Classifier for Segmentation-Free Manchu Word Recognition;Not health related;Not health related;0
"P. Azalov; F. Zlatarova";2003;The synthetic generation of data is appropriate in many cases. The data generated can be of different nature: program texts, tables, XML documents, etc. The application area of such data comprises not only the scientific research and analysis of the real program system features, but the teaching activity, too. The generation of texts of different problems, belonging to the same type, represents such a case. The problems could be used for self-preparation as after-class training, for solving exercises in class, or for assessing the students' knowledge in a given discipline. This paper discusses the philosophy and development of a system generated sets of synthetic program texts of the same type. The examples considered are for teaching purpose.;SDG - a system for synthetic data generation;Not health related;Not health related;0
"B. C. Knoll; G. B. Melton; H. Liu; H. Xu; S. V. S. Pakhomov";2016;The accuracy of part of speech (POS) tagging reported in medical natural language processing (NLP) literature is typically very high when training and testing data sets are from the same domain and have similar characteristics, but is lower when these differ. This presents a problem for clinical NLP, where it is difficult to obtain large corpora of training data suitable for localized tasks. We experimented with implementing the TnT POS tagger and training it on a manually tagged small corpus of publicly available synthetic clinical reports supplemented with widely used public corpora (GENIA and Penn Treebank). We describe this implementation and report the evaluation results on MiPACQ, a large corpus of manually tagged clinical text. Our tagger achieves accuracy comparable to POS taggers trained on large amounts of real clinical data (91-93%). This demonstrates that medical NLP developers do not need to rely on large restricted resources for POS tagging.;Using synthetic clinical data to train an HMM-based POS tagger;health related;health related;1
"J. Gemignani; J. Gervain";2021;The use of a large and diversified ground-truth synthetic fNIRS dataset enables researchers to objectively validate and compare data analysis procedures. In this work, we describe each step of the synthetic data generation workflow and we provide tools to generate the dataset.;A practical guide for synthetic fNIRS data generation;Not health related;Not health related;0
"E. J. MacKie; D. M. Schroeder";2020;The topography beneath ice sheets is an important parameter in numerous ice sheet analyses including sea level rise prediction. Subglacial topography is measured with airborne ice-penetrating radar. Radar coverage of ice sheets is incomplete so gaps must be interpolated. However, traditional interpolation techniques such as kriging create unrealistically smooth surfaces. Alternatively, topography can be geostatistically simulated such that the interpolated topography has similar spatial statistics to the data. The direct sampling technique simulates topography by sampling from a training dataset, or exposed topography. However, bed measurements are often hundreds or thousands of kilometers away from exposed topography, so selecting a representative training dataset is difficult. Therefore, we propose a technique that uses radar bed measurements to generate synthetic training data. This approach uses a Fourier transform to simulate topography based on the frequency content of the available bed measurements. The simulated training data can then be used to perform a direct sampling simulation. We demonstrate this concept for exposed bed topography in Greenland.;Geostatistically Simulating Subglacial Topography with Synthetic Training Data;Not health related;Not health related;0
"M. Pyne; B. J. Yurkovich; S. Yurkovich";2019;In this article an approach is presented based on the use of measured experimental data from conventional battery packs to generate synthetic operational data for subsequent use in monitoring, predicting and controlling battery pack state of health. Generally speaking, experimentation-based synthetic data is effective in extensive simulation models possessing many varied operating conditions. The results presented in this article focus on proof of concept and are part of a comprehensive study into general capacity estimation and capacity fade prediction in battery packs. Experimental data is derived from scaled operational cycles with multiple charge and discharge pulses applied repetitively on a commercially available battery pack. The resulting synthetically generated data, using Markov chain approaches, has the flexibility of matching user-imposed conditions and can be of any length. Therefore, the focus in this article is the generation of sufficient training data for models built from machine learning techniques, utilizing only a relatively limited amount of actual data. In the context of the overall ongoing study, the behavior of the battery pack is characterized by features and a supervised learning approach is adopted in order to estimate capacity fade during real-time operation without the use of specific capacity tests.;Generation of Synthetic Battery Data with Capacity Variation;Not health related;Not health related;0
"N. J. Rodriguez-Fernandez; P. Richaume; Y. H. Kerr; F. Aires; C. Prigent; J. -P. Wigneron";2017;This paper discusses a methodology to construct a synthetic dataset using realistic geophysical data and the L-MEB model to compute synthetic brightness temperatures (Tb's) and to train a Neural Network (NN) for global retrievals of soil moisture (SM). The trained NNs are applied to real Tb's measured by the Soil Moisture and Ocean Salinity (SMOS) satellite (L-MEB NN). The objective is twofold. First, to compare and provide feedback to the operational algorithm. Second, to evaluate this approach in the context of pre-launch algorithm development. The performance of the L-MEB NN dataset was evaluated by comparing with time series of in situ measurements in North America. The correlation, standard deviation and bias of NN SM and in situ SM are similar to those obtained with the SMOS L3 SM product and with ECMWF models. The L-MEB NN dataset was also compared globally to the SMOS Level 3 SM and SM from ECMWF models. The L-MEB NN dataset is in general wetter than SMOS L3 SM, closer to ECMWF models. Some possible reasons are briefly discussed.;Global retrieval of soil moisture using neural networks trained with synthetic radiometric data;Not health related;Not health related;0
"F. Galland; F. Tupin; J. . -M. Nicolas; M. Roux";2005;;Registering of synthetic aperture radar and optical data;Not health related;Not health related;0
"R. De Paulis; C. Prati; F. Rocca; S. Scirpoli; S. Tebaldini";2009;Synthetic Aperture Radar (SAR) and Sonar (SAS) systems provide high resolution reflectivity maps of the imaged scene by coherently combining the echoes collected along a virtual array of receivers. A peculiarity of SAS systems is that the echoes are often collected by moving a short real array of hydrophones to avoid range ambiguity. In this paper we present a modification of the standard wavenumber focusing algorithm widely used in SAR data processing to make it suitable for focusing bi-static SAS data. An autofocusing technique is then exploited to estimate and compensate for the deviation of the platform trajectory from the rectilinear one.;Focusing Synthetic Aperture Sonar (SAS) data with the Omega-K technique;Not health related;Not health related;0
"E. A. Olson; C. Barbalata; J. Zhang; K. A. Skinner; M. Johnson-Roberson";2018;In this paper, we present a new methodology to generate synthetic data for training a deep neural network (DNN) to estimate depth maps directly from stereo images of underwater scenes. The proposed method projects real underwater images onto landscapes of randomized heights in a 3D rendering framework. This procedure provides a synthetic stereo image pair and the corresponding depth map of the scene, which are used to train a disparity estimation DNN. Through this process, we learn to match the underwater feature space using supervised learning without the need to capture extensive real underwater depth maps for ground truth. In our results, we demonstrate improved accuracy of reconstruction compared to traditional computer vision feature matching methods and state-of-the-art DNNs trained on synthetic terrestrial data.;Synthetic Data Generation for Deep Learning of Underwater Disparity Estimation;Not health related;Not health related;0
"J. Drozdowicz; P. Samczynski; M. Wielgo; D. Gromek; K. Klincewicz";2015;Experiments on Synthetic Aperture Radar require a data acquisition and storage system. Custom-made systems are well suited to this specific application, but the development of such a system is both time and resource consuming. The use of an off-the-shelf FPGA evaluation board with interchangeable analogue to digital converter cards as a data acquisition and pre-processing system was proposed. Such an approach allows not only for operation with different analogue front-ends, but also for the implementation of data pre-processing, such as modulation and decimation. The modulation makes further processing easier and the filtration-decimation reduces the size of the data stream. A complete system was tested thoroughly during a ground-based experiment and is now ready for airborne testing. As there are unused resources in the FPGA, they will be utilized in the future for the implementation of real-time processing.;The use of FPGA evaluation board as data acquisition and pre-processing system for Synthetic Aperture Radar;Not health related;Not health related;0
"K. -H. Le Minh; K. -H. Le";2021;The past decade has seen a notable increase in air pollution that directly damages health, animals, and plants worldwide. To mitigate such negative effects, several research groups have been working on predicting air quality using deep learning. However, the lack of high-quality air quality datasets is a major obstacle encountered to achieve high accuracy prediction. In this paper, we introduce an air monitoring data generator powered by learning distributed real sequences using the generative adversarial network (GAN), namely AirGen. An unsupervised adversarial loss is also employed in the network to minimize the difference between generated synthetic and original data in the training process. Experiments on real datasets indicate that the data generated by Airgen could significantly increase the prediction accuracy performed by deep learning models. The mean square error (MSE) is remarkably reduced from 0.024 to 0.015.;AirGen: GAN-based synthetic data generator for air monitoring in Smart City;Not health related;Not health related;0
"K. Yang; W. Gao; R. Fan; T. Yin; J. Lian";2021;"High impedance faults (HIFs) have always been significant challenges in the power grids. Researchers have developed some advanced protective methods to detect the HIFs. To test and validate these methods, large amounts of HIF data are required. This paper presents a synthetic HIF data generating method using the deep convolutional generated adversarial network (DCGAN). The DCGAN includes a generator module to create synthetic HIF waveform from random noises; and a discriminator module to identify the flaws of those synthetic data, which ultimately helps improve the quality of the synthetic data created by the generator. To test the fidelity of the generated synthetic HIF data, two different HIF-detection methods have been applied. Extensive simulation results have validated the effectiveness of using the DCGAN to create synthetic HIF data.";Synthetic High Impedance Fault Data through Deep Convolutional Generated Adversarial Network;Not health related;Not health related;0
A. Elgarem;1989;The characterization of the size, shape, and location of subsurface discontinuities in metals using multidepth synthetic aperture processing of ultrasonic data is considered. A technique is proposed to increase the lateral resolution and signal-to-noise ratio when metals of large thickness are tested. The first technique to achieve multidepth was dynamic focusing. The aperture in the proposed technique is simultaneously focused at several depths by means of a computer program, thus avoiding the fixed hardware required in the dynamic focusing technique. Experimental evidence is presented to indicate the improvement in resolution, due to multidepth processing, over the single-focus technique.<>;Multidepth synthetic aperture processing of ultrasonic data;Not health related;Not health related;0
"C. Öztürk; M. G. Drahor";2010;The synthetic GPR modelling studies are quite important to evaluate the interpretation of shallow geological properties such as faulting, bedding, weathering etc. This study aims to simulate some shallow geological structures by GPR technique. To obtain a successful interpretation, we performed the synthetic modelling studies for paleoseismology, sedimentary and geotechnical problems using different values of electrical properties that affect the radar signals. During the modelling studies, we produced a lot of models for various subsurface conditions. Also, real GPR data was collected on some geological problems. As a result, the combined usage of the real GPR data and synthetic modelling could be contributed the joint interpretation on shallow geological structures.;Synthetic GPR modelling studies on shallow geological properties and its comparison with the real data;Not health related;Not health related;0
"G. A. Tammana; Y. F. Zheng; R. L. Ewing";2005;Performing some form of compression on synthetic aperture radar (SAR) data is necessary and important to curtail the downlink capacity and on-board storage requirements of a SAR system. In this paper, we present a transform coding approach for compression of SAR raw data using wavelet packet transform. Trellis coded quantization (TCQ) techniques are used to quantize the transform coefficients. A variation of TCQ implementation using a significance test is investigated. Simulation results on real SAR raw data show the improved performance of the proposed algorithm over standard block adaptive quantization (BAQ) technique.;Synthetic aperture radar raw data compression using wavelet packet transform and trellis coded quantization;Not health related;Not health related;0
"C. Wang; Y. Tang; H. Zhang; B. Zhang; F. Wu; L. Xu";2021;To well keep track of progress towards the 17 Sustainable Development Goals and the corresponding 169 targets of the 2030 Agenda for Sustainable Development signed in 2015, the UN Statistical Commission proposed a Global Indicator Framework (GIF) of 232 SDG indicators to help countries to implement development strategies and monitor the progress toward the SDG Targets. In the past years, the contributions of EO-based data to the GIF are recognized and regarded as it will play more important role in the next 10 years of action to realize the SDGs of 2030. In this paper the contributions of Synthetic aperture radar (SAR) to the 232 SDG Indicators are assessed, which can monitor our planet day and night as an important part of EO. And in order to better support the SDGs in the future, the specifications of the SAR-based dataset (products) and the corresponding requirements for the new generation SAR are discussed, which is helpful for the proposed SAR mission designated to SDGs.;Synthetic Aperture Radar for SDGs in Big Data Era;Not health related;Not health related;0
"M. P. Anand; A. D. Rajapakse; B. Bagen";2018;In this paper, some of the existing solar radiation prediction models are reviewed initially and two suitable stochastic long-term solar radiation prediction models were identified. These models were implemented to generate synthetic solar radiation data. The simulated radiation data were compared with measured data in terms of time correlation and statistical characteristics. Most importantly, the paper proposed a new property, namely the probability distributions of the first order differences in solar hourly radiation and show that these distributions computed for a particular hour of the day remain identical from year to year. The paper further shows that the studied solar radiation models fail to preserve this property and some of the implications associated with these limitations are briefly discussed.;Analysis of the Quality of Long-term Synthetic Solar Radiation Data Generated from Stochastic Models;Not health related;Not health related;0
"F. K. Dankar; N. Madathil";2022;Federated Learning (FL) is a hot new topic in collaborative training of machine learning problems. It is a privacy-preserving distributed machine learning approach, allowing multiple clients to jointly train a global model under the coordination of a central server, while keeping their sensitive data private. The problem with FL systems is that they require intense communication between the server and clients to achieve the final machine learning model. Such complexity increases with the number of clients participating and the complexity of the model sought. In this paper, we introduce synthetic data generation into FL systems with the intention of reducing the number of iterations required for model convergence. In this novel method, clients generate synthetic datasets modeling their private data. The synthetic datasets are then sent to the central server and are used to generate a cognizant initial model. Our experiments show that such conscious method for generating the initial model lowers the number of iterations by a factor of more than 4 without affecting the model accuracy. As such it enhances the overall efficiency of FL systems.;Using Synthetic Data to Reduce Model Convergence Time in Federated Learning;Not health related;Not health related;0
"F. Uçar; Ö. F. Alçin; B. Dandil; F. Ata";2017;Electrical grid has lots of changes in its morphology and managing style since first installed nearly two hundred years ago. Today, smart grid structure plays a crucial role when creating a sustainable and reliable operation. In smart grid context, power quality issues are monitored and required measures are obtained from smart meters. Power quality term include voltage quality. When it is about voltage quality, sags take the lead among other disturbances. System operators have to track voltage sags to provide a better service quality. In this study, a fast and simple algorithm called Hilbert Transform is used to detect voltage sags in synthetic dataset. Then, a voltage sag table is built considering related IEEE and IEC standards to identify site indices SARFI-X and SIARFI-X. Purpose of the study is being a first step to voltage sag detection and defining indices with real data. Obtained results denote and feed this aim.;Hilbert transform based simple detection and indice analyze of voltage sags using synthetic data;Not health related;Not health related;0
"F. Shang; X. Huang; H. Liu; A. Hirose";2020;This letter proposes a data arrangement for fully polarimetric synthetic aperture radar (PolSAR). It is an essential novel method in the use of the rotation transformation in data interpretation. The key point of the proposal is employing a single pixel-based and selective rotation transformation for each pixel before the speckle filtering. The experimental results with ALOS2-PALSAR2 data show that the proposed data arrangement has much higher performance in recognizing double-bounce scattering in the man-made target area. At the same time, it is effective in avoiding the overestimation of double-bounce and/or surface scattering in natural target areas.;Data Arrangement With Rotation Transformation for Fully Polarimetric Synthetic Aperture Radar;Not health related;Not health related;0
"Cetin; Moses";2006;"We consider the problem of wide-angle synthetic aperture radar (SAR) imaging from data with arbitrary frequency-band omissions. We propose an approach that involves composite image formation through combination of subaperture images, as well as point-enhanced, super-resolution image reconstruction. This framework provides a number of desirable features including preservation of anisotropic scatterers that do not persist over the full wide-angle aperture; robustness to bandwidth limitations and frequency-band omissions; as well as a characterization of the aspect dependence of scatterers. We demonstrate the effectiveness of the proposed approach through experiments based on electromagnetically simulated realistic data.";Synthetic Aperture Radar Imaging from Wide-Angle Data with Frequency-Band Omissions;Not health related;Not health related;0
N. Sano;2020;In statistical disclosure control, releasing synthetic data implies difficulty in identifying individual records, since the value of synthetic data is different from original data. We propose two methods of generating synthetic data using principal component analysis: orthogonal transformation (linear method) and sandglass-type neural networks (nonlinear method). While the typical generation method of synthetic data by multiple imputation requires existence of common variables between population and survey data, our proposed method can generate synthetic data without common variables. Additionally, the linear method can explicitly evaluate information loss as the ratio of discarded eigenvalues. We generate synthetic data by the proposed method for decathlon data and evaluate four information loss measures: our proposed information loss measure, mean absolute error for each record, mean absolute error of mean of each variable, and mean absolute error of covariance between variables. We find that information loss in the linear method is less than that in the nonlinear method.;Synthetic Data by Principal Component Analysis;Not health related;Not health related;0
"A. V. Sosnovsky; V. G. Kobernichenko";2018;An experimental study of the efficiency of the radar data interferometric processing algorithms is complicated by the fact that a quantitative result can be obtained only after the entire processing procedure is finished, so it is difficult to estimate the effectiveness of the first processing stages - the multilooking and of phase noise suppression. A method for accuracy estimation interferometric data processing at such steps based on the reference digital elevation model inverse transformation is proposed. The results of the method implementation for ALOS PALSAR data with different baselines are presented.;An Efficiency Estimation for Multilooking and Phase Noise Suppression Methods for Spaceborne Interferometric Synthetic Aperture Radars Data Processing;Not health related;Not health related;0
"M. Tao; J. Li; J. Su; Y. Fan; L. Wang; Z. Zhang";2020;Radio frequency interference (RFI) is a critical issue to synthetic aperture radar (SAR), which would cause great distortions to amplitude and phase information of the received echoes. Most of the existing literatures deal with the interference separation problem in time domain, frequency domain, or time-frequency domain using the matrix representation and matrix optimization tools, without further exploiting the correlation among multiple dimensional measurements. This paper proposes an interference separation for SAR data using tensor representation by formulating a novel time-frequency azimuth tensor. Then, the low-rank property of the interference is utilized and the interference contribution is estimated using low rank tensor approximation. Experimental results demonstrate that the interference components is effectively extracted, and well imaging results could be recovered.;Interference Mitigation for Synthetic Aperture Radar Data using Tensor Representation and Low-Rank Approximation;Not health related;Not health related;0
O. Ikeda;1992;In a previous article by the author (ICASSP 1991 p.2613-16), a synthetic-aperture array imaging method was presented. The method first collects array data by illuminating the object with laser light sequentially at each of a number of angles and by detecting the reflected field with an array sensor. Then, on a computer, the eigenvector with the largest eigenvalue for the correlation of the array data is derived, and a beam steering operation is applied to the eigenvector to reconstruct the object. The eigenvector, however, does not always backpropagate to focus at a single point on the object, when it has several parts with similar reflectivities, resulting in blurred reconstructed images. The array data are selected based on the aperture region to improve the focusing characteristic, where two criteria measuring the single-point focusing are used. Basic experimental results are given.<>;Synthetic-aperture optical array imaging system: selection of array data for single-point focusing;Not health related;Not health related;0
"F. M. Ngai; R. E. Carande";1996;This paper describes automated algorithms that generate map products in a rapid fashion with interferometric synthetic aperture radar (IFSAR) data set. The input IFSAR data set consists of backscatter, elevation and correlation images and the automatically extracted map elements include land-use polygons, transportation networks, buildings/man-made structures, bare soil elevation contours and hydrology networks. A calculator computes five feature images from the IFSAR data set and the five feature images serve as inputs to a hybrid unsupervised classification algorithm which segments the scene into five classes. The five classes include fields, trees, urban area/man-made structure, water and unknown. The classification results allows a bare soil terrain model to be extracted from the IFSAR terrain model using a novel algorithm. The difference between the bare soil elevation and the original IFSAR elevation can be used in conjunction with the classification map to identify and estimate the heights of buildings, man-made structures and trees. Elevation contours and hydrology networks derived front the bare soil model can be shown to be more accurate than that derived from the IFSAR model. Transportation networks are identified using a road extraction technique based on rotational energy summation. Sample map products are shown to demonstrate the efficiency and accuracy of the automated algorithms.;Automated rapid mapping with interferometric synthetic aperture radar data;Not health related;Not health related;0
"J. Stuckman; K. Wills; J. Purtilo";2013;Source code metrics have been used in past research to predict software quality and focus tasks such as code inspection. A large number of metrics have been proposed and implemented in consumer metric software, however, a smaller, more manageable subset of these metrics may be just as suitable for accomplishing specific tasks as the whole. In this research, we introduce a mathematical model for software defect counts conditioned on product metrics, along with a method for generating synthetic defect data that chooses parameters for this model to match statistics observed in empirical bug datasets. We then show how these synthetic datasets, when combined with measurements from actual software systems, can be used to demonstrate how sets of metrics perform in various scenarios. Our preliminary results suggest that a small number of source code metrics conveys similar information as a larger set, while providing evidence for the independence of traditional software metric classifications such as size and coupling.;Evaluating Software Product Metrics with Synthetic Defect Data;Not health related;Not health related;0
"K. Y. Lee; Soo Chin Liew; Leong Keong Kwoh";2002;The use of C- and L-band polarimetric synthetic aperture radar (POLSAR) data for classifying land cover features in a tropical area is investigated in this study. The POLSAR data were acquired during the NASA/JPL PACRIM-1 mission over the northern part of Peninsular Malaysia on 3rd December 1996. Prior to classification, the Lee polarimetric filter was applied to the complex covariance matrix for speckle suppression. In unsupervised classification, the scattering mechanism of each pixel in the speckle-suppressed images was analyzed and grouped into one of the three categories: (1) odd-bounce, (2) even-bounce, or (3) diffuse scattering. Training samples were then generated from the outputs of the unsupervised classification, to be used in subsequent supervised classifications of various frequency and polarization combinations. The Kappa statistics computed for classification using single-frequency fully polarized C- and L-band data were 0.69 and 0.73, respectively. An improvement to 0.79 was achieved by using the dual-frequency (combined C and L bands), fully polarized data in the classification.;Land cover classification of polarimetric synthetic aperture radar (POLSAR) data based on scattering mechanisms and complex Wishart distribution;Not health related;Not health related;0
"C. -S. Lee; Y. M. Lui; Sung Yong Chun";2011;This paper presents performance evaluation of silhouette-based action recognition from virtual human silhouette(VIHASI), which is publicly available. We especially evaluate action recognition performance based on tensor analysis. We applied tensor analysis for the silhouette data itself after sampling the same number of silhouette frames from given sample frames. Geometric manifold embedding based on tangent bundle is applied and shows better performance compared with baseline performance.;Human action silhouette recognition based on tensor analysis using synthetic silhouette data;Not health related;Not health related;0
"P. C. Madhusudana; S. -J. Lee; H. R. Sheikh";2022;Deep neural networks targeting stereo disparity estimation have recently surpassed the performance of hand-crafted traditional models. However, training these networks require large labeled databases for obtaining accurate disparity estimates. In this letter, we address the large data requirement by generating synthetic data using natural image statistics. Images generated using dead leaves model have been shown to share many statistical characteristics commonly seen in natural images. In this work, we created a synthetic dataset using the 3D dead leaves model consisting of occluding spheres, and projected them onto parallel camera planes to obtain stereo image pairs along with ground-truth disparity map. This generated data was subsequently used to train a deep neural network in a supervised manner to estimate disparity. Through experiments we show that this trained model achieves competitive performance across real-world and synthetic stereo datasets, even without any additional fine-tuning. The proposed method for dataset generation is simplistic in nature, computationally inexpensive and can be easily scaled for large scale data generation.;Revisiting Dead Leaves Model: Training With Synthetic Data;Not health related;Not health related;0
"M. Kalkuhl; P. Droste; W. Wiechert; H. Nies; O. Loffeld; M. Lambers";2007;For modern SAR data acquisition, bi- and multistatic SAR missions become increasingly important. Established methods for processing monostatic SAR signals need to be adapted to new algorithms for signal processing. In order to support the evolution and development of these new algorithms simulated SAR raw data of arbitrary bi- and multistatic SAR scenarios are essential. This paper refers to a modular SAR simulator, which is able to simulate complex bi- and multistatic SAR scenarios. It focuses on the geometrical simulation approach of the simulator and the computationally intensive synthesis of SAR raw data. The main part descripts the parallel implementation of the radar lobe footprint scan and of the succeeding SAR raw data generation executed on a compute cluster. An example will be shown, which compares the simulation of the same SAR scenario on three different compute systems and their runtimes.;Parallel computation of synthetic SAR raw data;Not health related;Not health related;0
"Y. Zhao; V. Sark; M. Krstic; E. Grass";2022;Hand gesture recognition has attracted a lot of attention recently. However, gesture recognition based on machine learning (ML) requires a huge training data set in order to achieve high recognition accuracy. Creating this training data set requires a significant effort. In order to solve this problem, a synthetic frequency modulated continuous waves (FMCW) RADAR data generator for hand gestures is proposed. This generator can produce a large amount of data that can be used for training the ML model, without collecting real data involving multiple people. For evaluation, 3600 synthetic samples for six hand gestures are generated with rich variations in hand size, speed and position. Those synthetic data are utilized to train the ML models for hand gesture recognition. Further, the models are tested by real data set acquired by an AWR 1642 RADAR. The results indicate that a convolutional neural network with 19 layers (VGG19) pre-trained model in conjunction with the XGBoost classifier can achieve an average accuracy of 87.53% on the test set. If only 2% of the real data is used for training, the XGBoost classifier alone achieves an average accuracy of 56.31%. But if the synthetic data set and 2% of the real data set are combined, the average recognition accuracy of the XGBoost classifier on the test data set is increased to 94.63%.;Synthetic Training Data Generator for Hand Gesture Recognition Based on FMCW RADAR;Not health related;Not health related;0
"E. Rignot; R. Chellappa";1990;;SEGMENTATION OF SYNTHETIC APERTURE RADAR COMPLEX DATA;Not health related;Not health related;0
"T. I. Lukowski; R. K. Hawkins; P. S. Daleman; L. M. H. Wander";1990;;CCRS Synthetic Aperture Radar Data Calibration - Status Report;Not health related;Not health related;0
"Chen Jie; Zhou Yin-qing; Li Chun-sheng";2001;Spaceborne synthetic aperture radar (SAR) raw data simulation of a natural terrain is a very useful tool for interpretation of SAR images, optimization of SAR system design scenario and evaluation of interferometric processing algorithms, as well as testing the performance of SAR payload hardware via EGSE (electronic ground support equipment). A method for simulation spaceborne SAR raw data starting from digital elevation map (DEM) data of natural terrain is proposed, based on fractal theory, the facet model and clutter scattering statistic model, as well as the Earth model and orbit propagator. Digital elevation data of a natural terrain is used in computer simulation. The simulated raw data is processed by means of the chirp scaling algorithm. Special phenomena in spaceborne SAR images of natural terrain are reflected in the final images, such as shadowing, layover and foreshortening, as well as the dynamic range of the radar. It is illustrated that the method is suitable for verification and optimization of a spaceborne SAR system scenario.;Spaceborne synthetic aperture radar raw data simulation of three dimensional natural terrain;Not health related;Not health related;0
G. E. Carlson;1984;A method for estimating wavenumber and propagation direction for the dominant wave component in an ocean wave field from a few scans of synthetic aperture radar data is described and analyzed. The use of just a few radar scans rather than a complete image reduces data storage requirements significantly. The analysis shown uses actual synthetic aperture radar data and provides parameter tradeoffs and statistical performance results. While reasonable estimates of wavenumber and propagation direction are achieved in some cases, the estimates are not sufficiently consistent to be satisfactory over a wide range of cases. The primary problem is one of low signal-to-noise ratio of the radar scan data.;Estimation of ocean wave wavenumber and propagation direction from limited synthetic aperture radar data;Not health related;Not health related;0
"A. Radius; P. Marques";2012;"The maritime environment observed by SAR is very complex, in which different perturbations overlap in the same appearance in surface roughness. In these conditions, moving ship detection can require advanced vessel detection techniques, that should be validated with ground truth data. In this context the data simulators can provide useful data for validation purposes. The simulator aims to be a useful tool for the recreation of realistic conditions, being able to simulate the raw SAR data of different surfaces and targets; particularly, it is able to characterize the marine surface depending on the sea state and the wind sea. The main purpose of the simulator is to provide data for the validation of focusing techniques, Doppler parameter algorithms and moving ships detection and classification methodologies.";Synthetic aperture radar raw data simulator for sea environment dedicated to moving ship detection;Not health related;Not health related;0
"R. Horn; E. Meier";1992;;A Refined Procedure To Generate Calibrated Im44gery From Airborne Synthetic Aperture Radar Data;Not health related;Not health related;0
"D. Horn; L. Janssen; S. Houben";2021;The utilization of automatically generated image training data is a feasible way to enhance existing datasets, e.g., by strengthening underrepresented classes or by adding new lighting or weather conditions for more variety. Synthetic images can also be used to introduce entirely new classes to a given dataset. In order to maximize the positive effects of generated image data on classifier training and reduce the possible downsides of potentially problematic image samples, an automatic quality assessment of each generated image seems sensible for overall quality enhancement of the training set and, thus, of the resulting classifier. In this paper we extend our previous work on synthetic traffic sign images by assessing the quality of a fully generated dataset consisting of 215,000 traffic sign images using four different measures. According to each sample's quality, we successively reduce the size of our training set and evaluate the performance with SVM and CNN classifiers to verify the approach. The comparability of real-world and synthetic training data is investigated by contrasting several classifiers trained on generated data to our baseline w.r.t. actual misclassifications during testing.;Automated Selection of High-Quality Synthetic Images for Data-Driven Machine Learning: A Study on Traffic Signs;Not health related;Not health related;0
"V. Eranki; N. Yee; H. Y. Wong";2022;In this letter, a novel variation of Generative Adversarial Network (GAN) is proposed and used to predict device and circuit characteristics based on design parameters. Unlike regular GAN which takes white noise as inputs, this modified GAN uses device or circuit parameters as inputs. Unlike regular Physics-informed GAN (PI-GAN) which incorporates differential equations in the training process, this modified GAN learns physics through the inputs and has one extra step of supervised learning. FinFET is used as a device example and Technology Computer-Aided-Design (TCAD) is used to generate its current-voltage ( ${I}_{D}{V}_{G}$ ,  ${I}_{D}{V}_{D}{)}$  and capacitance-voltage ( ${C}_{G}{V}_{G}$ ) curves as the training data by varying the gate length ( $\text{L}_{\text {G}}{)}$ , fin top width ( $\text{W}_{\text {TOP}}{)}$ , and gate metal workfunction (WF). A CMOS inverter with source contact defects is used as a circuit example and a SPICE simulator is used to generate its Voltage Transfer Characteristics (VTC) by varying the source contact resistances. We show that 1) the GAN model is able to generate both the device and circuit electrical characteristics based on the input parameters, 2) it can predict the characteristics of the device and circuit out of the training range (in a testing volume 3.7x to 4.6x larger than the training volume), and 3) it is further verified on experimentally measured data in the inverter case that it does not overfit and has learned the underlying physics.;Out-of-Training-Range Synthetic FinFET and Inverter Data Generation Using a Modified Generative Adversarial Network;Not health related;Not health related;0
"A. Juven; M. -H. Aumeunier; R. Brunet; M. L. Bohec; M. Adel; R. Miorelli; X. Artusi; C. Reboud";2022;Infrared (IR) imaging systems are common diagnostics for monitoring in-vessel components in thermonuclear fusion devices (tokamak). Nevertheless, IR interpretation in fully metallic environment is complex due to the presence of multiple reflections and the change of optical properties of materials as the fusion operation progresses. This causes high errors on the surface temperature measurement which is a risk for machine protection. The paper presents a first demonstration of simulation-assisted machine learning method for retrieving the surface temperature from IR measurement on metallic targets with unknown properties. The technique relies on the training of a convolutional neural network on a synthetic dataset generated by a deterministic ray tracer. The performances of such an approach is first proven on tokamak prototype considering pure specular surfaces.;Temperature Estimation in Fusion Devices using Machine Learning techniques on Infrared Specular Synthetic Data;Not health related;Not health related;0
"M. Hosseini; I. Becker-Reshef; R. Sahajpal; L. Fontana; P. Lafluf; G. Leale; E. Puricelli; M. Varela; C. Justice";2020;"In this study, double-bounce parameter derived from Sentinel-1 was integrated with Difference vegetation index (DVI) derived from Landsat-8 for prediction of soybean yield at field level over central Argentina. Artificial Neural Network (ANN) model was trained using time series of Synthetic Aperture Radar (SAR) and optical features during the growing season. For comparison of SAR versus optical versus their integration for soybean yield prediction, the ANN model was trained and tested for three scenarios of SAR-only, optical-only and SAR-optical integration. Accuracies of yield prediction including correlation of determination (R2), root mean square error (RMSE) and mean absolute error (MAE) are 0.80, 0.589 t/ha, 0.445 t/ha for SAR-only; 0.65, 0.800 t/ha, 0.546 t/ha for optical-only; and 0.85, 0.554 t/ha, 0.389 t/ha for SAR-optical integration scenarios, respectively. These accuracies demonstrate of high potential of SAR and SAR-optical integration for soybean yield prediction at field level.";Crop Yield Prediction Using Integration Of Polarimteric Synthetic Aperture Radar And Optical Data;Not health related;Not health related;0
"O. Gozes; H. Greenspan";2020;In this paper, we present a deep learning-based image processing technique for extraction of bone structures in chest radiographs using a U-Net FCNN. The U-Net was trained to accomplish the task in a fully supervised setting. To create the training image pairs, we employed simulated X-Ray or Digitally Reconstructed Radiographs (DRR), derived from 664 CT scans belonging to the LIDC-IDRI dataset. Using HU based segmentation of bone structures in the CT domain, a synthetic 2D “Bone x-ray” DRR is produced and used for training the network. For the reconstruction loss, we utilize two loss functions- L1 Loss and perceptual loss. Once the bone structures are extracted, the original image can be enhanced by fusing the original input x-ray and the synthesized “Bone X-ray”. We show that our enhancement technique is applicable to real x-ray data, and display our results on the NIH Chest X-Ray-14 dataset.;Bone Structures Extraction and Enhancement in Chest Radiographs via CNN Trained on Synthetic Data;Not health related;Not health related;0
"A. B. Mattos; D. A. Borges Oliveira; E. Da Silva Morais";2018;Visual Speech Recognition is the ability to interpret spoken text using video information only. To address such task automatically, recent works have employed Deep Learning and obtained high accuracy on the recognition of words and sentences uttered in controlled environments, with limited head-pose variation. However, the accuracy drops for multi-view datasets and when it comes to interpreting isolated mouth shapes, such as visemes, the values reported are considerably lower, as shorter segments of speech lack temporal and contextual information. In this work, we evaluate the applicability of synthetic datasets for assisting recognition of visemes in real-world data acquired under controlled and uncontrolled environments, using GRID and AVICAR datasets, respectively. We create two large-scale synthetic 2D datasets based on realistic 3D facial models - with near-frontal and multi-view mouth images. We perform experiments that indicate that a transfer learning approach using synthetic data can get higher accuracy than training from scratch using real data only, on both scenarios.;Towards View-Independent Viseme Recognition Based on CNNS and Synthetic Data;Not health related;Not health related;0
"M. Janik; N. Gard; A. Hilsmann; P. Eisert";2021;We present a network architecture which compares RGB images and untextured 3D models by the similarity of the represented shape. Our system is optimised for zero-shot retrieval, meaning it can recognise shapes never shown in training. We use a view-based shape descriptor and a siamese network to learn object geometry from pairs of 3D models and 2D images.Due to scarcity of datasets with exact photograph-mesh correspondences, we train our network with only synthetic data.Our experiments investigate the effect of different qualities and quantities of training data on retrieval accuracy and present insights from bridging the domain gap. We show that increasing the variety of synthetic data improves retrieval accuracy and that our system’s performance in zero-shot mode can match that of the instance-aware mode, as far as narrowing down the search to the top 10% of objects.;Zero in on Shape: A Generic 2D-3D Instance Similarity Metric Learned from Synthetic Data;Not health related;Not health related;0
"W. Gong; F. Meyer";2012;After more than a decade of continuous improvements, Persistent SAR Interferometry (PSI) has become an accepted tool for long-term monitoring of geophysical phenomena such as volcanoes, earthquakes, and city deformation. To extract surface deformation from PSI observations, different phase component are separated based on their physical or stochastic properties. Atmospheric signals, one of the main error contributions, are usually mitigated using a low-pass filter in time. This approach provides unsatisfactory results if the temporal sampling of the surface deformation signal is close to Nyquist, and if there are sampling gaps in the time series. Hence, in many cases, a more sophisticated way to mitigate atmospheric signals is required. This paper proposes an adaptive filter design that is utilizing knowledge of the stochastic properties of the atmosphere at SAR acquisition times. The concept of the filter is presented and its performance is tested on simulated signals. In these studies, the adaptive filter design performed favorably and we are expecting this approach to be of major benefit for PSI processing. In current work, the performance of the adaptive filter is tested on real data. For this study, the pre-known statistics of atmospheric delay is provided by Numerical Weather Forecast Models.;Optimized filter design for irregular acquired data stack in persistent scatterers synthetic aperture radar interferometry;Not health related;Not health related;0
"X. Pic; E. G. S. Antonio; M. Dimopoulou; M. Antonini";2023;Over the past years, the ever-growing trend on data storage demand, has motivated research for alternative systems of data storage. Because of its biochemical characteristics, synthetic DNA molecules are considered as potential candidates for a new storage paradigm. Because of this trend, several coding solutions have been proposed over the past years for the storage of digital information into DNA. Despite being a promising solution, DNA storage faces two major obstacles: the large cost of synthesis and the noise introduced during sequencing. Additionally, this noise increases when biochemically defined coding constraints are not respected: avoiding homopolymers and patterns, as well as balancing the GC content. This paper describes a novel entropy coder which can be embedded to any block-based image-coding schema and aims to robustify the decoded results. Our proposed solution introduces variability in the generated quaternary streams, reduces the amount of homopolymers and repeated patterns to reduce the probability of errors occurring. While constraining the code to better satisfy the constraints would degrade the compression efficiency, in this work, we propose an alternative method to further robustify an already-existing code without affecting the compression rate. To this end, we integrate the proposed entropy coder into four existing JPEG-inspired DNA coders. We then evaluate the quality —in terms of biochemical constraints— of the encoded data for all the different methods by providing specific evaluation metrics.;Rotating labeling of entropy coders for synthetic DNA data storage;Not health related;Not health related;0
"Q. Song; F. Xu; H. Wang";2020;Recently, with the introduction and development of deep learning based detection and classification methods, various applications in optical images have been put into practice. However, few automatic target recognition (ATR) approaches in Synthetic Aperture Radar (SAR) images can be used practically. Two reasons underlying it are the complicated imaging mechanism of SAR and limited sample data for optimizing the models. This paper focus on target representation and classification with limited data based on zero-shot learning (ZSL) and fewshot learning (FSL), and provides a comprehensive investigation of existing ZSL/FSL algorithms.;Target Representation and Classification with Limited Data in Synthetic Aperture Radar Images;Not health related;Not health related;0
"J. W. Owens; M. W. Marcellin; B. R. Hunt; M. Kleine";1997;Synthetic aperture radar (SAR) is a remote-sensing technology which uses the motion of the radar transmitter to synthesize an antenna aperture much larger than the actual antenna aperture to yield high spatial resolution radar images. Trellis coded quantization (TCQ) techniques are shown to provide a high performance, low bit-error sensitivity solution to the problem of downlink data rate reduction for SAR systems. Trellis coded vector quantization (TCVQ) and universal trellis coded quantization (UTCQ) coding systems are implemented and compared with other data compression schemes (block adaptive quantization (BAQ or BFPQ) and vector quantization (VQ)) that can be used to compress SAR phase history data.;Compression of synthetic aperture radar phase history data using trellis coded quantization techniques;Not health related;Not health related;0
"D. R. Sagi; S. J. Ranade; A. Ellis";2005;This paper explores a method for load composition estimation using fuzzy regression. The method is examined using synthetic data to test the accuracy and robustness. The results obtained show that aggregate load composition can be estimated with acceptable accuracy using assumed load component parameters.;Evaluation of a load composition estimation method using synthetic data;Not health related;Not health related;0
"G. Christie; Z. Kurtz; K. Huber; J. Massey; I. Courtney; C. Gifford; J. Humphreys; A. Mayalu; R. Williams; J. Hunnell; B. Collins";2018;Training accurate object detection models often requires a large amount training data. In some cases, limited imagery, from drastically different perspectives than the desired target view positions and angles, may be available for specific objects of interest. Training accurate models with this imagery may not be possible and require a lot of performance-limiting assumptions. However, it may be possible to use this limited imagery to create a 3D model of the targets and their surrounding area. In this paper, we explore training an object detector using only synthetic imagery to detect rooftop stacks for UAV sampling tasks. We show that this detector performs well on real imagery, and enables autonomous UAV sampling. We also note that this approach is general, and should extend to other objects.;Training Object Detectors with Synthetic Data for Autonomous UAV Sampling Applications;Not health related;Not health related;0
"J. Schulz-Stellenfleth; T. Konig; S. Lehner";2006;Spacebore Synthetic Aperture radar (SAR) is still the only instrument providing continuous two-dimensional (2-D) ocean wave measurements on a global basis. For more than a decade the European satellites ERS-1 and ERS-2 have acquired SAR data over the open ocean operating in wave mode. The ERS acquisitions are currently continued by the ENVISAT ASAR wave mode. It is well known that the derivation of ocean wave parameters from SAR data is not straightforward and different approaches have been proposed. It this study we present a new technique, which is based on an empirical SAR imaging model. The method has the calibrated SAR image as the only input. A data set of 6000 globally distributed ERS-2 wave mode image spectra and collocated ocean wave spectra computed with the numerical model WAM are used to fit a linear model, which relates the SAR spectrum to integral wave parameters like, e.g., the significant wave height. This model is then used for ocean wave parameter retrieval. The radar cross section and the azimuthal cut-off wavelength estimated from the wave mode images are used as additional input variables. The method takes into account the coupling of the different parameters and is based on a least-square minimisation approach. The resulting coupled linear system of equations is solved using a singular value decomposition technique. Disjunct subsets of the collocated data set are used for fitting the model and retrieving ocean wave parameters. Scatterplots and global maps with the derived parameters are presented. It is shown that the standard deviation of the retrieved significant waveheight with respect to the WAM waveheight is in the order of 0.62 m. Other wave parameters, which are of practical relevance like mean wave periods are investigated as well.;An Empirical Approach for the Retrieval of Ocean Wave Parameters from Synthetic Aperture Radar Data;Not health related;Not health related;0
"K. Brunham; A. El Boustani; W. Kinsner";2002;The compression of raw SAR data has been a topic of interest to researchers for many years. The goal of such compression is to transmit the highest fidelity data from the radar satellite within the downlink bandwidth constraints. The necessity of new compression techniques is further emphasized as advances in the design of radar technologies are close to surpassing the capabilities of current compression techniques. Developing only a theoretical compression technique is not a complete solution to the problem as the technique must be realizable in the hardware available for placement on the radar satellite. The implementation must not only be low in computational complexity, but must also be able to handle the ever increasing throughput demands of the radar. Bit-planes have been used successfully with several other compression techniques, and are extremely amenable to hardware due to their inherent parallelism. This paper shows that bit-plane segmentation and simple coding transformation of raw SAR data can reduce the entropy of a single plane by 59%.;A study of bit planes for the compression of raw synthetic aperture radar data;Not health related;Not health related;0
"O. Noakoasteen; J. Vijayamohanan; A. Gupta; C. Christodoulou";2022;In this paper, we propose the use of GANs as learned, data-driven knowledge database that can be queried for rapid synthesis of suitable antenna designs given a desired response. As an example, we consider the problem of designing the Log-Periodic Folded Dipole Array (LPFDA) antenna for two non-overlapping ranges of Q-factor values. By representing the antenna with the vector of its structural parameters and considering each desirable range of the Q-factor as a class, we transform our problem to that of generating new samples from a given class. We develop two alternative models, a Conditional Wasserstein GAN and a label-switched library of vanilla Wasserstein GANs and train them with a dataset of features and their associated labels (parameter vectors and Q-factor range). The main component of these models is a generator network that learns to map a normally distributed noise vector along with a binary label to the vector of parameters of candidate structures. We demonstrate that in inference mode, these models can be relied upon for fast generation of suitable designs.;Antenna Design Using a GAN-Based Synthetic Data Generation Approach;Not health related;Not health related;0
"I. D. Gerg; D. P. Williams; V. Monga";2020;"Deep learning has been recently shown to improve performance in the domain of synthetic aperture sonar (SAS) image classification over existing shallow learning solutions. Given the constant resolution with range of a SAS, it is no surprise that deep learning techniques perform so well; the image of the seafloor produced by a SAS system is almost photographic in quality. Despite the image quality benefits of SAS, there is still room for classification improvement particularly in reducing the number of false alarms. This work addresses this by tackling one facet of the classification pipeline: image enhancement. Specifically, we ask and address the following question: Can we train a deep neural network to simultaneously enhance and classify a SAS image? We will respond in the affirmative as we introduce a new deep learning architecture tackling the problem, Data Adaptive Enhancement and Classification Network (DA-ECNet). DA-ECNet is a deep learning architecture which combines image enhancement as part of the classification procedure eliminating the need for a fixed state-of-the-art despeckling algorithm or enhancement module. Additionally, we train both image enhancement and classification jointly resulting in data adaptive image enhancement. Experiments on a challenging real-world dataset reveal that the proposed DA-ECNet outperforms state of the art deep learning as well as traditional feature based methods for SAS image classification.";Data Adaptive Image Enhancement and Classification for Synthetic Aperture Sonar;Not health related;Not health related;0
"P. Addabbo; M. L. Bernardi; F. Biondi; M. Cimitile; C. Clemente; N. Fiscante; G. Giunta; D. Orlando";2022;One of the greatest limitations of Synthetic Aperture Radar imagery is the capability to obtain an arbitrarily high spatial resolution. Indeed, despite optical sensors, this capability is not just limited by the sensor technology. Instead, improving the SAR spatial resolution requires large transmitted bandwidth and relatively long synthetic apertures that for regulatory and practical reasons are impossible to be met. This issue gets particularly relevant when dealing with Stripmap mode acquisitions and with relatively low carrier frequency sensors (where relatively large bandwidth signals are more difficult to be transmitted). To overcome this limitation, in this paper a deep learning based framework is proposed to enhance the SAR image spatial resolution while retaining the complex image accuracy. Results on simuated and real SAR data demonstrate the effectiveness of the proposed framework.;Super-Resolution of Synthetic Aperture Radar Complex Data by Deep-Learning;Not health related;Not health related;0
"J. D. Pasternak; J. Shelton; J. Gilmore";2011;"With the continuing development of synthetic ropes and real-world experience gained through their use in offshore mooring operations, it is evident that previous reservations about the use of synthetics in soils are no longer warranted. This is proven through the extensive documentation of field service history, coupled with rope and fiber testing of a jointly developed High Modulus Polyethylene (HMPE) “mud rope,” which is presented to industry in this paper. The ""mud rope"" examined in this paper is of 8_3 open-braid HMPE construction. This open-braid construction facilitates visual inspections of each rope, allowing for significant time savings during expensive offshore operations. Each of the subropes within the main rope has its own separate filter barriers and jackets. These individually jacketed subropes allow for more residual strength in the rope should a breach in the jacket occur and cause premature fiber wear and internal abrasion. As the ropes are comprised of lighter HMPE material, they are easier to handle offshore on the deck of an Anchor Handling Vessel (AHV), reducing the likelihood of accidents and injuries. This low density also allows the ropes minimal interference in the freefall trajectory of the gravity-installed anchor to which they are directly connected. The ropes have initial lengths of ~335-ft, and due to their direct connection to the gravity-installed anchor, much of the rope is under cyclic tension under the mudline. Given the excellent fatigue and abrasion resistance of HMPE, in addition to all of the aforementioned benefits, it is an excellent choice for ropes that are under continual use under harsh conditions. The service life of this rope spans from its first installation into Gulf of Mexico soils on 3/12/08 to the final retrieval date of 1/20/10 (~679 days). The actual total time in the mud is documented to be ~544 days. During that service life, the rope was placed on two semi-submersible MODUs (Mobile Offshore Drilling Units) and in six different mooring system configurations. In late August of 2008, the rope was connected to a MODU during hurricane Gustav, experiencing storm loads calculated to be in excess of ~1,050-kips (~1,050,000-lbs). The rope was inspected and measured to determine if any permanent damage was done to it. As the rope did not experience loading near its MBL (Minimum Break Load) of 800 metric tones (~1764-kips), the rope was deemed safe for further use. After minor jacket repairs, the rope was placed back into service, where it remained until January of 2010. After being retired from service, the rope underwent several destructive tests. Testing was performed on the rope fibers to confirm fiber creep, static stiffness, and post-service MBL (Minimum Break Load). A similar testing scope was performed on the rope to examine overall rope creep, rope static stiffness, and post-service MBL. This paper documents the detailed records of an HMPE ""mud rope"" throughout its service life, along with the rope and fiber testing techniques and results that were used to verify its integrity after the rope was retired.";Synthetic “mud ropes” for offshore mooring applications - field history and testing data;Not health related;Not health related;0
"F. Khan; S. Basak; H. Javidnia; M. Schukat; P. Corcoran";2020;In this paper, we explore how synthetically generated 3D face models can be used to construct a high-accuracy ground truth for depth. This allows us to train the Convolutional Neural Networks (CNN) to solve facial depth estimation problems. These models provide sophisticated controls over image variations including pose, illumination, facial expressions and camera position. 2D training samples can be rendered from these models, typically in RGB format, together with depth information. Using synthetic facial animations, a dynamic facial expression or facial action data can be rendered for a sequence of image frames together with ground truth depth and additional metadata such as head pose, light direction, etc. The synthetic data is used to train a CNN-based facial depth estimation system which is validated on both synthetic and real images. Potential fields of application include 3D reconstruction, driver monitoring systems, robotic vision systems, and advanced scene understanding.;High-Accuracy Facial Depth Models derived from 3D Synthetic Data;Not health related;Not health related;0
"R. Thomanek; C. Roschke; F. Zimmer; T. Rolletschke; R. Manthey; M. Vodel; B. Platte; M. Heinzig; M. Eibl; C. Hosel; R. Vogel; M. Ritter";2020;Current research in the domain of video activity detection focuses on real-time activity detection. This includes multiple approaches in the mobile environment, such as the detection of correct motion sequences in the sports and health area or in safety-relevant environments. Current trends focus on the use of 3D CNNs. This work describes a approach to combine the results of a human skeleton point detector with an LSTM on mobile devices. Frameworks for pose detection are combined with LSTMs for activity detection with sensor data, optimized for the mobile area. The resulting system allows the direct detection of a person pose and the classification of activities in a video recorded with a smartphone. The successful application of the system in several field tests shows that the described approach works in principle and can be transferred to a resource-limited mobile environment by optimization.;Real-Time Activity Detection of Human Movement in Videos via Smartphone Based on Synthetic Training Data;Not health related;Not health related;0
"M. Burgin; M. Moghaddam";2014;"The focus for geophysical parameter retrieval from space, such as soil moisture, is shifting towards lower frequencies allowing better penetration through vegetation (rendering it less visible) and into the soil (allowing soil moisture sensing into depth). But the ionosphere becomes increasingly opaque at lower frequencies, which intensifies the need for mitigation of ionospheric effects to allow the use of spaceborne radar signals. This work focuses on a radar-only method to predict and mitigate the Faraday rotation effect, which is considered to be the remaining dominant effect. A novel method to retrieve the Faraday rotation angle by using three different optimization techniques in the presence of other system distortion terms with no external targets is presented. The retrieval performance with synthetic spaceborne radar data is very promising; it allows a successful retrieval of the Faraday rotation angle if it is initiated within ±25° of the ground truth.";Mitigation of Faraday rotation effect for long-wavelength synthetic spaceborne radar data;Not health related;Not health related;0
A. J. Lewis;1998;The 4000 offshore oil production facilities in the Gulf of Mexico and many other partially submerged structures are obstacles to navigation as well as being of commercial importance. Oil spills from these production or transportation facilities could cause extensive damage to the coastal ecosystem and an economic collapse of the fisheries industry. The same production facilities are also navigational hazards to private and commercial navigation. The risk of damage to the environment and economy is ameliorated by current accurate maps of offshore oil production facilities. National Ocean Service (NOS) Nautical Charts published by the National Oceanic and Atmospheric Administration (NOAA) map the location of navigational hazards. Oil production platforms and other navigational hazards are indicated on these NOAA charts within an accuracy tolerance of 50 meters at a scale of 1/80000. The frequency of map revision is dependent on the rate of coastal change and user needs. However, NOS Nautical Charts may not be revised for several years or even a decade because of cost. This study evaluates the potential and limitations of using airborne SAR for detecting and mapping navigation hazards, primarily offshore oil production facilities, in coastal Louisiana. However, the results are extendable to other coastal zones. Accuracy assessments indicate the importance of target and system parameters in target detection of navigational hazards.;Detecting and mapping offshore navigation hazards using synthetic aperture radar data;Not health related;Not health related;0
"I. D. Cameron; I. H. Woodhouse; N. Walker";2007;Synthetic aperture radar (SAR) provides a promising method for offshore wind field estimation, particularly in the context of important for offshore wind farm development. This paper introduces an iterative maximum aposteriori probability (MAP) method for combining meteorological model output with synthetic aperture radar for offshore wind field estimation. The MAP approach is demonstrated for 40 ENVISAT ASAR scenes collected for 2004–2006 over the UK Irish Sea. Both the CMOD4 and CMOD5 geophysical model functions are implemented and retrievals using MAP and a simpler direction based windspeed algorithm are validated against insitu mast observations. The CMOD5 MAP algorithm in particular shows promising results with an estimates on average within 2 ms_1 of the insitu observations.;A novel method for estimating offshore wind fields using synthetic aperture radar and meteorological model data;Not health related;Not health related;0
"D. Lee; M. Cho; S. Lee; J. Song; C. Choi";2021;Post-training quantization is a representative technique for compressing neural networks, making them smaller and more efficient for deployment on edge devices. However, an inaccessible user dataset often makes it difficult to ensure the quality of the quantized neural network in practice. In addition, existing approaches may use a single uniform bit-width across the network, resulting in significant accuracy degradation at extremely low bit-widths. To utilize multiple bit-width, sensitivity metric plays a key role in balancing accuracy and compression. In this paper, we propose a novel sensitivity metric that considers the effect of quantization error on task loss and interaction with other layers. Moreover, we develop labeled data generation methods that are not dependent on a specific operation of the neural network. Our experiments show that the proposed metric better represents quantization sensitivity, and generated data are more feasible to apply to mixed-precision quantization.;A Novel Sensitivity Metric For Mixed-Precision Quantization With Synthetic Data Generation;Not health related;Not health related;0
"R. Techentin; D. Foti; P. Li; E. Daniel; B. Gilbert; D. Holmes; S. Al-Saffar";2014;We have developed a large semi-synthetic, semantically rich dataset, modeled after the medical record of a large medical institution. Using the highly diverse data.gov data repository and a multivariate data augmentation strategy, we can generate arbitrarily large semi-synthetic datasets which can be used to test new algorithms and computational platforms. The construction process and basic data characterization are described. The databases, as well as code for data collection, consolidation, and augmentation are available for distribution.;Development of a Semi-synthetic Dataset as a Testbed for Big-Data Semantic Analytics;health related;Not health related;1
"R. Ali; J. J. Dahl; N. Bottenus";2019;Retrospective Encoding For Conventional Ultrasound Sequences (REFoCUS) enables recovery of the full-synthetic aperture (FSA) dataset from focused transmits while avoiding the drawbacks of single- and virtual-element transmissions. It was recently shown that a regularized inversion approach significantly improves the accuracy of the recovered FSA dataset over the REFoCUS method when applied to a walking-aperture transmit sequence. However, this approach becomes computationally burdensome when applied to walking aperture sequences on larger linear arrays. We present an iterative form of REFoCUS that improves FSA dataset recovery to better handle these cases.;Iterative Retrospective Recovery of Full Synthetic Aperture Data from Focused Transmissions;Not health related;Not health related;0
"A. Chougule; K. Agrawal; V. Chamola";2023;In recent years, significant research has occurred on developing various protocols for communication within an autonomous vehicle. Due to the simplicity and trustworthiness of a Controller Area Network (CAN) bus, it has become trendy and widely employed for in-vehicle communication. However, research indicates numerous network-level threats are possible owing to the CAN bus's lack of defense mechanisms. Messages are prone to attacks from third-party sources threatening the correctness of the CAN bus messages. In the last few years, machine learning and deep learning algorithms have effectively improved CAN security and developed various misbehavior, intrusion prevention, and detection systems. However, a large amount of data is required to train these algorithms. There are currently very few CAN datasets available, which has become a major barrier for researchers when developing new CAN security algorithms. Also, the nature of the data in question is tedious to accumulate, especially if there is a need for specific features. In this work, we proposed SCAN-GAN (Synthetic CAN), a generative adversarial Network (GAN) based technique to generate data using existing collected data and presented a synthetic CAN dataset. We also compared the original and generated dataset based on various parameters as well as on well-known classification algorithms, showing that various previous models deliver improved results on the generated dataset over the original dataset. The results exhibit the efficiency of using GANs for data production, which is on par with real data. The results of this work also suggest the adaptability of the GAN to work with varied datasets.;SCAN-GAN: Generative Adversarial Network Based Synthetic Data Generation Technique for Controller Area Network;Not health related;Not health related;0
"M. Mahdianpari; M. Motagh; V. Akbari";2012;This paper proposes a novel speckle reduction method that combines an advanced statistical distribution with spatial contextual information for SAR data. The method for despeckling is based on a Markov random field (MRF) that integrates a K-distribution for the SAR data statistics and a Gauss-MRF model for the spatial context. These two pieces of information are combined based on weighted summation of pixel-wise and contextual models. This not only preserves edge information in the image, but also improves signal-to-noise ratio (SNR) of the despeckled data. Experiments on real SAR data demonstrate the effectiveness of the algorithm compared with well-known despeckling methods.;Speckle reduction and restoration of synthetic aperture radar data with an adoptive markov random field model;Not health related;Not health related;0
"P. Torrione; L. Collins";2008;As ground penetrating radar phenomenology continues to improve, more advanced statistical signal processing approaches become applicable to subsurface inference in GPR data. Despite the wide body of literature exploring the applications of various approaches to processing GPR data, statistical modeling of realistic soil responses is a difficult task, and the algorithms developed for real-time fielded GPR processing are rarely directly motivated by statistical models of GPR data. In this work, we present a tractable spatial statistical model for volumetric GPR data which can be used to motivate the application of various signal processing approaches to solving problems of interest in GPR data like pre-screening, feature extraction, and air/ground response tracking.;Statistical Models for Landmine Detection in Ground Penetrating Radar: Applications to Synthetic Data Generation and Pre-Screening;Not health related;Not health related;0
"P. G. Menshih; S. D. Erokhin; M. G. Gorodnichev";2020;Ensembles of neural networks are used to improve the performance of the model and show themselves well in the case of using deep recurrent and convolutional networks on serial [1] and graphic data [2]. The article discusses the advisability of using a snapshot ensemble of not deep feedforward neural networks on a synthetic dataset.;Efficiency Analysis of Neural Networks Ensembles Using Synthetic Data;Not health related;Not health related;0
M. V. Hall;2004;"The May 2001 Geoacoustic Inversion Techniques Workshop provided synthetic transmission loss (TL) data for four cases with range-dependent shallow-water all-liquid environments. In two of these cases (""0"" and ""1""), the sea floor has constant slope and the geoacoustic model (GAM) is range independent. Cost functions have been computed using a new adiabatic-mode TL algorithm (which uses an exact velocity boundary condition at the sloping sea floor), as one parameter in the GAM is varied. Two frequencies (80 and 220 Hz) were selected. In case 0, the sea-floor slope is 0.0183 and the GAM comprises an inhomogeneous layer over a basement. The sea-floor sound-speed was selected as the variable parameter. The resulting cost minima at 80 and 220 Hz are displaced from the actual sound speed by 2.3 and 3.4 m/s, respectively. In case 1, the sea-floor slope is 0.012 and the GAM comprises one homogenous layer, five inhomogeneous layers, and a basement. The selected parameter was the sound-speed in the homogeneous layer. The corresponding cost minima are displaced by -1.2 and +1.1 m/s. The relative values of these four errors indicate that mode coupling increases with sea-floor slope and that there may be a dependence on frequency at the greater slope.";Preliminary analysis of the applicability of adiabatic modes to inverting synthetic acoustic data in shallow water over a sloping sea floor;Not health related;Not health related;0
"C. Tan; R. Behjati; E. Arisholm";2019;Having access to high-quality test data is an important requirement to ensure effective cross-organizational integration testing in the Norwegian public sector. Evogent is a PhD project that aims to provide model-based solutions for generating synthetic test data that is statistically representative of real (reference) population, and is dynamic in the same way that the actual population is. This project is carried out in collaboration with the Modernization of the National Registry project (MF) within the Norwegian Tax Department, which serves as our case study. In this paper, we present a conceptual model and related algorithms for event generation.;A Model-Based Approach to Generate Dynamic Synthetic Test Data: A Conceptual Model;Not health related;Not health related;0
"A. Damian; C. Filip; A. Nistor; I. Petrariu; C. Mariuc; V. Stratan";2023;This paper investigates a new method for generating high-quality synthetic training data for object detection (OD) algorithms. Utilizing photogrammetry, we transform real-world objects into 3D digital replicas in Unreal Engine 5 (UE5). We then simulate diverse scenarios, capturing images of these objects at varying angles, backgrounds, and textures. To evaluate the data quality, we experiment with multiple configurations and train OD models using YOLOv8. Our findings highlight the effectiveness of the proposed image generation approach in UE5 and establish its potential as a cost-efficient solution for generating extensive volumes of training data for OD algorithms.;Experimental Results on Synthetic Data Generation in Unreal Engine 5 for Real-World Object Detection;health related;health related;0
"X. Ceamanos; S. Douté";2010;The Compact Reconnaissance Instrument Spectrometer for Mars (CRISM) is a hyperspectral imager that is affected by optical distortions onto its spatial-spectral detector matrix that is also non-uniform in responsivity. In particular, central wavelength of the spectral response varies along the spatial dimension of the array, thus giving rise to artifacts in the data. An accurate knowledge of the point spread function (PSF) of each individual detector of the matrix is fundamental to correct for these effects. In this work, CRISM images from the poles of Mars are noticed to suffer from wavelength shifts that do not agree with pre-launch calibration data. The source of this instrumental anomaly is identified as a faulty flat fielding in the CRISM data pipeline and a method that is based on synthetic spectra is introduced for wavelength calibration. In the experiments, a polar CRISM image is calibrated by estimating its apparent wavelengths. Results are finally compared with the same test image after undergoing a prototype calibration that does not present the wavelength anomaly.;Calibration of CRISM/MRO apparent wavelengths using synthetic data;Not health related;Not health related;0
"R. Adam; P. Janciauskas; T. Ebel; J. Adam";2022;Today industries strive toward using data-driven machine learning wherever applicable. Consequently, they re-quire manually or automatically labeled training data sets. Currently, synthetically generating labeled training data sets belongs to the open challenges in machine learning across multiple application fields. In this paper, we propose employing a procedural pipeline combining BlenderProc with domain randomization to create prelabeled training data sets synthetically. Randomizing the domain using uncorrelated random background images, we ensure that the neural network applied for object detection purely learns the object features and is background-independent. Our proposed pipeline yields a solution to create sizeable prelabeled training data sets. We assess the pipeline performance for the application of cone object detection for the formula student driverless competition using no real training and a small real-world training data set for fine-tuning: We show that using the synthetically generated training data fine-tuned with a limited real training data set performs best for object detection. This transfer learning-based, fine-tuned solution also outperforms the benchmark training data set in detecting knocked-ver cones that are neither present in the real nor the synthetic training data set. Consequently, by combining BlenderProc and domain randomization, we provide a solution for formula student teams to generate extensive training data for cone detection and other detection problems relevant to driverless.;Synthetic Training Data Generation and Domain Randomization for Object Detection in the Formula Student Driverless Framework;Not health related;Not health related;0
"E. Strelcenia; S. Prakoonwit";2022;Deep learning-based classifiers for object classification and recognition have been utilized in various sectors. However according to research papers deep neural networks achieve better performance using balanced datasets than imbalanced ones. It's been observed that datasets are often imbalanced due to less fraud cases in production environments. Deep generative approaches, such as GANs have been applied as an efficient method to augment high-dimensional data. In this research study, the classifiers based on a Random Forest, Nearest Neighbor, Logistic Regression, MLP, Adaboost were trained utilizing our novel K-CGAN approach and compared using other oversampling approaches achieving higher F1 score performance metrics. Experiments demonstrate that the classifiers trained on the augmented set achieved far better performance than the same classifiers trained on the original data producing an effective fraud detection mechanism. Furthermore, this research demonstrates the problem with data imbalance and introduces a novel model that's able to generate high quality synthetic data.;Generating Synthetic Data for Credit Card Fraud Detection Using GANs;Not health related;Not health related;0
"M. A. Farooq; P. Corcoran";2021;Thermal imaging has played a dynamic role in the diversified field of consumer technology applications. To build artificially intelligent thermal imaging systems, large scale thermal datasets are required for successful convergence of complex deep learning models. In this study, we have highlighted various techniques for generating large scale synthetic facial thermal data using both public and locally gathered datasets. It includes data augmentation, synthetic data generation using StyleGAN network, and 2D to 3D image reconstruction using deep learning architectures. Training and validation accuracy of Wide ResNet CNN for binary gender recognition task is improved by 4.6% and 4.4% using original and newly generated synthetic data with an overall test accuracy of 83.33%.;Proof-of-Concept Techniques for Generating Synthetic Thermal Facial Data for Training of Deep Learning Models;Not health related;Not health related;0
"O. Dogan; M. Kartal; S. Kent";2007;In this paper we proposed a new strip mode synthetic aperture radar (SAR) raw data simulator. The computation of environmental backscattering characteristics are followed by range-time domain integration regarding the SAR strip mode geometry. Finally the simulation results are verified by visually comparing with a real ASAR image.;A New Strip Mode Synthetic Aperture Radar (SAR) Data Simulator;Not health related;Not health related;0
"N. Oddershede; J. A. Jensen";2005;;Synthetic aperture flow angle estimation on in-vivo data from the carotid artery;Not health related;Not health related;0
"J. Wetzel; S. Zeitvogel; A. Laubenheimer; M. Heizmann";2020;In this work an approach for wide-area indoor people detection with a network of depth sensors is presented. We propose an end-to-end multi-view deep learning architecture which takes three foreground segmented overlapping depth images as input and predicts the marginal probability distribution of people present in the scene. In contrast to classical data-driven approaches our method does not make use of any real image data for training but uses a randomized generative scene model to generate synthetic depth images which are used to train our proposed deep learning architecture. The evaluation shows promising results on a publicly available data set.;People Detection in a Depth Sensor Network via Multi-View CNNs trained on Synthetic Data;Not health related;Not health related;0
"K. Park; H. Lee; H. Yang; S. -Y. Oh";2020;Despite the advances in deep learning, training instance segmentation models like convolutional neural networks still tend to depend on enormous training data that are expensive and require labor to annotation. To avoid labor-intensive procedure, synthetic data can be an alternative because it is easy to generate and automatically segmented. However, it is challenging to train instance segmentation model that perform well at real world using only synthetic data because of domain gap. It is wrong direction to put a lot of effort into solving these problems by making synthetic data more photorealistic. In this paper, we suggest how to learn the instance segmentation model using synthetic data with artificial distractors. The performance has been improved about 7% by adding flying distractors compared to original synthetic data.;Improving Instance Segmentation using Synthetic Data with Artificial Distractors;Not health related;Not health related;0
"M. Rajendran; C. T. Tan; I. Atmosukarto; A. B. Ng; S. See";2022;The task of classifying or predicting the activity performed by hu-mans is called human activity recognition. Many existing models aim to solve the problem of activity recognition in this field, but we recognise the lack of real data can have a great impact on the effectiveness of such models. In this paper, we introduce SynDa, a first-of-its-kind streamlined semi-automated pipeline for synthetic data generation built using photorealistic rendering and AI pose estimation, that harvests existing real-life video datasets to create new large-scale datasets. The synthetic data can augment real data to train models robustly and overcome the lack of and associated costs to acquire more real data. Preliminary work indicates that combining real data and synthetic video data generated using this pipeline to train models presents a mAP of 32.35%, while a model trained on real data presented 29.95%.;SynDa: A Novel Synthetic Data Generation Pipeline for Activity Recognition;Not health related;Not health related;0
"A. E. Robertson; D. V. Arnold; D. G. Long";1998;A computer simulation of SAR data is discussed. A method is outlined for simulating a distributed scene which is capable of modeling SAR data, under moderate motion of the radar platform, and a wide range of incidence angles. Arbitrary terrain profiles and reflectivities can be modeled. Interferometric pairs of data can be generated. Inputs and outputs to the simulator and the steps required in the simulation algorithm are outlined.;Computer simulation of synthetic aperture radar data;Not health related;Not health related;0
"G. H. F. Tam; Y. S. Hung; C. Chang";2013;Evaluation of gene regulatory network (GRN) discovery methods relies heavily on synthetic time series. However, synthetic data generated by traditional method deviate a lot from real data, making such evaluation questionable. Guiding by decaying sinusoids, we propose a new method that generates synthetic data resembling human (HeLa) cell-cycle gene expression data. Using the new synthetic data, a simple comparison between four GRN discovery methods reveals that Granger causality (GC) methods substantially outperform Pearson correlation coefficient (PCC), while time-shifted PCC can give comparable performance as GC methods. The new synthetic data generation would also be useful for generating other kinds of cell-cycle time series. Using data generated by our proposed method, evaluation of GRN discovery methods should be more trustworthy for real-data applications.;Synthetic Time Series Resembling Human (HeLa) Cell-Cycle Gene Expression Data and Application to Gene Regulatory Network Discovery;Not health related;Not health related;0
"C. Cimino; G. Franceschetti; A. Iodice; A. Mazzeo; N. Mazzocca; E. Napoli; D. Riccio; P. Spirito; A. Strollo; M. Tesauro";1999;Real time processing for SAR data is a very useful tool in civilian and military applications. However, in order to avoid resolution degradation, a very short time must be required for each single multiplication. This condition can be verified by using one-bit coded data and reference function: in fact, in this case operations can be performed by XNOR gates. In this paper, a VLSI architecture for real time one-bit processing is presented, and first performance tests are reported.;A novel VLSI architecture for real time operations of one-bit coded synthetic radar imaging data;Not health related;Not health related;0
"T. Sarkar; S. Basu; J. Wong";2013;Testing techniques for database applications typically include generation of database states (synthetic data) along with automatic generation of test cases. The quality of such test cases is evaluated on the basis of structural coverage of the host language (e.g., Java), whereas, the quality of test cases for the embedded language (e.g., SQL) is evaluated separately using mutation testing. In mutation testing, several mutants or variants of the original SQL query are generated and mutation score is calculated. It is the percentage of mutants that can be differentiated in terms of their results using the given test cases. Higher mutation score indicates higher quality of the test cases. In existing approaches the generated test cases achieve high structural coverage with respect to the generated synthetic data, but suffer from low mutation score with respect to the same data. We present a novel framework called \textit{SynConSMutate} for test case and synthetic data generation for database applications. The generated test cases with respect to the newly generated synthetic data ensure high quality not only in terms of coverage of code written in the host language, but also in terms of mutant detection of the queries written in the embedded language.;SynConSMutate: Concolic Testing of Database Applications via Synthetic Data Guided by SQL Mutants;Not health related;Not health related;0
"J. E. Pereira-Pires; J. M. N. Silva; J. M. Fonseca; R. Guida; A. Mora";2023;Canopy Height (CH) is an important variable in any forest inventory, not only by its own information, but also as a proxy variable to estimate other parameters as the above-ground biomass. The CH information can also be helpful to understand the climate change trends, for forest management, and in decision support systems related to wildfires. The growing availability of Remote Sensing observations acquired from different sensors, create an alternative for the CH mapping to field campaigns and Airborne Laser Scanning (ALS) missions. Here a comparison between using Multispectral and Synthetic Aperture Radar sensors for CH estimation is presented. Both used the same Regression Methodology, being achieved a R2/RMSE between 43.71%-72.85%/0.85-4.03m for Multispectral and 42.12%-62.62%/0.96m-4.49m for SAR, for a total of 17 regions of interest. It is concluded that Multispectral data revealed to be more suitable for the CH mapping.;Multispectral vs Synthetic Aperture Radar Data for Canopy Height Estimation;Not health related;Not health related;0
"E. Kim; K. Park; H. Yang; S. -Y. Oh";2020;In tandem with growing deep learning technology, vehicle detection using convolutional neural network is now become a mainstream in the field of autonomous driving and ADAS. Taking advantage of this, lots of real image datasets have been produced in spite of the painstaking work of data collection and ground truth annotation. As an alternative, virtually generated images are introduced. This makes data collection and annotation much easier, but a different kind of problem called `domain gap' is announced. For instance, in off-road vehicle detection, there is a difficulty in producing off-road image dataset not only by collecting real images, but also by synthesizing images sidestepping the domain gap. In this paper, focusing on the off-road army tank detection, we introduce a synthetic image generator using domain randomization on off-road scene context. We train a deep learning model on synthetic dataset using low level features form feature extractor pre-trained on real common object dataset. With proposed method, we improve the model accuracy to 0.86 AP@0.5IOU, outperforming naïve domain randomization approach.;Training Deep Neural Networks with Synthetic Data for Off-Road Vehicle Detection;Not health related;Not health related;0
"E. Knapp; M. Battaglia; T. Stadelmann; S. Jenatsch; B. Ruhstaller";2021;The optimization of organic semiconductor devices relies on the determination of material and device parameters. However, these parameters are often not directly measurable or accessible and may change depending on the neighboring materials in the layered stack. Once the parameters are known, devices can be optimized in order to maximize a certain target, e.g. the brightness of a LED. Here, we combine the use of machine learning and a semiconductor device modelling tool to extract the material parameters from measurements. Therefore, we train our machine learning model with synthetic training data originating from a semiconductor simulator. In a second step, the machine learning model is applied to a measured data set and determines the underlying material parameters. This novel and reliable method for the determination of material parameters paves the way to further device performance optimization.;XGBoost Trained on Synthetic Data to Extract Material Parameters of Organic Semiconductors;Not health related;Not health related;0
"M. B. Kagalenko; W. H. Weedon";1996;The synthetic aperture imaging (SAI) and backpropagation imaging (BPI) algorithms are compared for reconstructing a representation of scattering objects from time-domain ground-penetrating radar data. Synthetic data produced by a FDTD forward solver is used as a basis for the comparison. We assume initially a homogeneous, lossless host medium. The BPI algorithm, which includes waveform diffractions, is shown to result in a better focused image than SAI, with fewer artifacts. The superiority of the BPI algorithm is particularly evident at lower frequencies, where the scattering objects are on the order of the wavelength of the incident field.;Comparison of backpropagation and synthetic aperture imaging algorithms for processing GPR data;Not health related;Not health related;0
"S. Kent; M. Kartal; N. G. Kasapoglu; S. Kargin";2007;To make proper feature extraction from SAR imagery is strictly deal with not only to understand imaging mechanism of SAR sensor, but also to take into account scattering mechanism of targets. In this work, a point scatterer model and a facet model based on synthetic aperture radar (SAR) simulators are presented.;Synthetic Aperture Radar Raw Data Simulation for Microwave Remote Sensing Applications;Not health related;Not health related;0
"L. J. Gonzalez-Soler; C. Rathgeb; D. Fischer";2023;Tattoos have been successfully employed to assist law enforcement in the identification of criminals and victims. Due to various privacy issues in acquiring images containing tattoos, only a limited number of databases exist. This lack of databases has slowed down the development of new tattoo segmentation and retrieval methods. In our work, we propose a new unsupervised generator that allows generating a large number of semi-synthetic images with tattooed subjects. To successfully generate realistic images, a database including the respective skin segmentation map is also proposed. Using this new generator and the skin database, 5,500 semi-synthetic images were created and evaluated for the tattoo segmentation use case. Experimental results on real data show the usefulness of using semi-synthetic images to train semantic segmentation algorithms: several manually mislabelled real samples were successfully corrected. The tattoo generator code, the skin database and generated images have been made available at https://dasec.h-da.de/hda-sstd/.;Semi-synthetic Data Generation for Tattoo Segmentation;Not health related;Not health related;1
"A. Esmaili; S. Farzi";2021;Nowadays, with the pervasiveness of social networks among the people, the possibility of publishing incorrect information has increased more than before. Therefore, detecting fake news and users who publish this incorrect information is of great importance. This paper has proposed a system based on combining context-user and context-network features with the help of a conditional generative adversarial network for balancing the data set to detect users who publish incorrect information in the Persian language on Twitter. Moreover, by conducting numerous experiments, the proposed system in terms of evaluation metrics compared to its competitors, has produced good performance results in detecting fake users.;Effective synthetic data generation for fake user detection;Not health related;Not health related;0
"M. P. Pepin; J. J. Sacchini";1996;The motivation for this research is the recognition of objects from the relative locations of radar point scatterers on the object. Point scatterers are modeled as damped exponential signals. Parametric estimation of the exponential parameters determines the point scatterer locations. Maximum likelihood estimation of exponential parameters in white Gaussian noise provides good results in many exponential estimation problems. In fact, the exponential parameters are estimated with touch better than Fourier resolution. In the case of SAR, however, the white noise is estimated to be quite large even in relatively noiseless images. This is due to energy that is not well modeled by damped exponentials. This unmodeled energy distorts the estimated point scatterer locations. This paper proposes that the unmodeled energy be modeled as unknown colored noise and that maximum likelihood estimation of scatterer locations in unknown colored noise will provide more accurate scatterer locations. New methods for estimating exponential parameters in unknown colored noise are applied to SAR data. These methods efficiently model the points scatterers in SAR data. The new methods achieve a higher estimation accuracy than maximum likelihood estimation of exponentials in white noise.;Maximum likelihood estimation of point scatterers in synthetic aperture radar data;Not health related;Not health related;0
"S. R. Silva; S. Cunha; A. Matos; N. Cruz";2008;High frequency synthetic aperture sonar systems require demanding tolerances in motion errors and medium phase stability. This article proposes a new method that mitigates the problems associated with small wavelength related errors. By dividing the received signal bandwidths in to several smaller ones and conjugate complex multiplying them, a new signal is obtained with longer effective wavelength, thus reducing the impact of motion errors and medium phase fluctuations.;Sub-band processing of synthetic aperture sonar data;Not health related;Not health related;0
"D. Pototzky; A. Sultan; L. Schmidt-Thieme";2022;Lack of labeled data is an omnipresent issue in deep learning for computer vision. Generating synthetic data seems to offer a simple solution for this problem. Once a data generator is set up, arbitrary amounts of fully-labeled data can be created. This data is then used to train a neural network which finally solves a problem on real data, e.g. by detecting cars.We argue that leveraging synthetic data like that rarely works in practice. Synthetic images often have a significant domain gap to real data, leading to reduced performance on the target domain. In experiments on several synthetic-to-real benchmarks including Sim10k to CityScapes, we show that state-of-the-art domain adaptation methods trained on thousands of synthetic images are usually outperformed by ordinary supervised learning on 14 to 70 images from the target domain.;Parting With Illusions About Synthetic Data;Not health related;Not health related;0
"E. F. S.; O. E. Ramos; S. P. G.";2021;Deep learning (DL) models applied to computer vision have made great progress for image-based plant phenotyping in recent years, mostly for quality control process automation in the agroindustry. On the one hand, these models are able to detect objects in complex and noisy images as fast as human observations, but on the other hand, they are trained with a large amount of labeled data for parameter tuning. This turns the training process into an expensive, repetitive, and time-consuming labor. In this work, a synthetic data generator based on robotic process automation (RPA) and Lindenmayer systems (L-Systems) named RPASD is designed and implemented to train a DL model that detects artichoke seedlings in images captured by a robot. First, the growth artichoke seedling is modeled in L+C language using the LStudio software. Second, the RPASD is developed in Python to produce labeled images of grouped synthetic artichoke seedlings that alongside manually labeled images of real artichoke seedlings, taken by a robot, form the PlantiNet database. Third, a YOLOv3 model is trained with the previously built databases forming three datasets: 1) real and synthetics seedlings, 2) only synthetic seedlings, and 3) only real seedlings. The results show a 55% of Mean Intersection over the Union (mIoU) when training only with the second dataset and testing with the third one, which allows us to conclude that our proposed method could adequately boost DL model training reducing costs and time.;RPA and L-System Based Synthetic Data Generator for Cost-efficient Deep Learning Model Training;Not health related;Not health related;0
"O. R. Fogle; B. D. Rigling";2012;Recently, the use of micro-Doppler radar signatures for classification has become an area of focus, in particular for the case of dynamic targets where many components are interacting over time. One specific target of interest is the dismount. Dismount detection, feature extraction, and classification offers some unique challenges. For instance, humans have small radar cross-sections and slow range-rates making discrimination from clutter difficult. Extracted features may be utilized to distinguish dismounts from other objects, however. In this paper, range-Doppler radar data collected from an airborne circular synthetic aperture radar is analyzed to determine extractable dismount characteristics. The results are compared against high signal-to-clutter ratio results.1 2;Dismount feature extraction from circular synthetic aperture radar data;Not health related;Not health related;0
"F. O. Hocaoglu; M. Fidan; O. N. Gerek";2009;In this paper, a novel Mycielski based approach for wind speed data generation is developed and presented. The efficiency of the proposed approach is tested using hourly wind speed data obtained from Izmir region. To test the efficiency of the approach, the four year-long measured data are seperated into two parts: data belonging to first three years are used for training whereas the remaining one-year data are used for testing and accuracy comparison purposes. In order to compare the efficiency of the proposed generation method, the same data are used in artificial wind speed generation with the classical method of first order Markov chains. Results indicate that the proposed Mycielski based algorithm produces better quality artificial data as compared to previous methods.;Mycielski approach for synthetic wind speed data generation;Not health related;Not health related;0
"N. Sravani; R. Mitravinda; P. R. Kumar; N. Neelima; K. L. Sailaja";2023;Agriculture appears to play a significant role in both a nation’s food security and economic development. Image processing techniques can be applied to Synthetic Aperture Radar (SAR) images with high spatial resolution. These images are backscattered to overcome the noises like clouds, smoke, rain, trees, etc. Traditionally Optical images are used for crop monitoring which has less brightness of pixel as it depends only on one variable, the amount of light reflected by the earth’s surface. Whereas in SAR images the brightness of each pixel primarily depends on at least 8 variables which give high resolution than optical images. The main aim of our project is to develop a model in Agriculture Field for Crop Growth Monitoring using SAR data. We used Sentinel 2 imagery data and processed them with Advanced Remote Sensing techniques. For crop monitoring analytics, there are different types of Vegetation Indices available. These indices help in remote sensing by providing useful insights of crops. In general, the Normalized Difference Vegetation Index (NDVI) is the most widely Vegetation Index to report biomass, density due to its versatility and credibility and it gives better accuracy during the whole crop production time. Using this parameter, we develop a methodology for Agricultural Crop Mapping/Retrieval using SAR images.;ACMapping: Agricultural Crop Mapping/Retrieval Using Synthetic Aperture Radar (SAR) Data;Not health related;Not health related;0
"F. de Wet; W. Van der Walt; N. Dlamini; A. Govender";2017;Creating synthetic voices that are natural and intelligible is a daunting challenge for well-resourced languages. The challenge is much bigger for languages in which the speech and text resources required for voice development are not available. Previous studies have suggested audiobooks as an alternative source of speech data. This paper reports on a comparison between voices derived from audiobook data and voices based on professional voice artist data. Two sets of voices were evaluated: male voices built using very small amounts of both data types (around 3 hours, representing a severely resource constrained scenario) and female voices trained on almost 10 hours of audiobook and professional speech data. The results of subjective listening tests indicate that, while the majority of the listeners preferred the voice artists' voices over the audiobook voices, the difference in naturalness was not perceived to be substantial. Results also showed that the artists' voices outperform the audiobook voices in terms of intelligibility, especially if a limited amount of training data is available. Although additional training data improves the intelligibility of audiobook voices, the results seem to indicate that a smaller quantity of professional data yields a better voice than large volumes of especially old audiobook data.;Building synthetic voices for under-resourced languages: The feasibility of using audiobook data;Not health related;Not health related;0
"S. Wiehle; A. Pleskachevsky";2018;This paper presents the use of Sentinel-1 Synthetic Aperture Radar (SAR) satellite data to derive bathymetry, the topography of the sea floor. With the growing efforts in global shipping and offshore constructions like wind parks, the knowledge of bathymetry becomes increasingly important. An automatic algorithm is used to retrieve the peak wavelengths of long swell waves from SAR acquisitions of coastal seas and calculate the bathymetry using the shoaling effect, which leads to waves becoming shorter when approaching shallower waters. The peak wave period, required for solving the dispersion relation, is also automatically retrieved by comparison to existing datasets.;Bathymetry derived from Sentinel-1 Synthetic Aperture Radar data;Not health related;Not health related;0
"J. Ramirez; G. Hickman";2016;In this paper, we consider the use of synthetic aperture processing for developing completely virtual arrays designed for an operational wavelength other than that of a physical array. In towed array scenarios, we show it is possible to exploit existing array motion to mitigate the effects of main-lobe widening and spatial aliasing when source wavelengths are different from the operational wavelength of the towed array. In particular, we demonstrate the advantage of using a class of thinned arrays know as co-prime linear sensor arrays in conjunction with synthetic aperture processing for creating virtual uniform linear arrays derived form physical channel data.;Synthetic Aperture Processing for creating completely virtual arrays from towed co-prime sensor array data;Not health related;Not health related;0
"Q. Lu; J. Zhu; Q. Li; K. Du; P. Li; X. Yu";2021;Traditional synthetic aperture radar (SAR) processing algorithms are incapable of exactly implementing the aperture-variant motion compensation due to the superposition of the synthetic apertures of several targets. Thus, a reference range bin (for example, range center of the sub-scene) and reference azimuth time (for example, the center of synthetic aperture) are applied, resulting in the range envelopes distortions and residual azimuth phase errors that impact the focusing, geometric fidelity of the processed SAR image. In this paper, an improved precise tomography- and aperture-dependent compensation fashion is developed to reach the precise and complete compensation of these space-variant motion errors. To this end, numerical computation is designed for the accurate azimuth frequency-time mapping in the presence of the residual phase errors. In the experimental part, some simulations are used to demonstrate the superiority of the proposed approach.;Processing of Blurred Image Data from Numerical Computation for Synthetic Aperture Radar;Not health related;Not health related;0
"A. Sanghi; J. R. Haritsa";2023;A critical need for enterprise DBMS vendors is to generate synthetic databases for testing their engines and applications in a range of environments. These synthetic databases are targeted toward capturing the desired schematic properties, and the statistical profiles of the data hosted on these schemas.Several data generation frameworks have been proposed for OLAP over the past three decades. The early efforts focused on ab initio generation based on standard mathematical distributions. Subsequently, there was a shift to database-dependent regeneration, which aims to create a database with similar statistical properties to a specific client database. This client-specific perspective has been taken further in recent times through workload-dependent database regeneration, where the databases generated ensure similar query executions to those observed at the client site.In this tutorial, we present a holistic coverage of synthetic data generation, highlighting the strengths and limitations of the above-mentioned framework classes. At the end, a suite of open technical problems and future research directions are enumerated.;Synthetic Data Generation for Enterprise DBMS;Not health related;Not health related;0
"Shiyong Li; Hongbin Huang; Bailing Ren; Houjun Sun";2013;Nowadays, due to the need of high resolution imaging, huge amount of data are needed to be collected base on Nyquist sampling theorem. However, the acquisition platform cannot afford the computation requirement to process on board, so those data must be sent to the ground so that it can be processed. This paper is focused on the compression of the Synthetic Aperture Radar (SAR) raw data to send as less data as we can ease the burden on the system and reduce the time for transmission. In this paper, we compressed the SAR raw data using compressive sensing method, and we train the sparse basis through K-SVD method. First we use the raw data that we collected to train the sparse basis using K-SVD method, when we get the trained sparse basis, we only need to send part of the raw data with the basis to the ground and the raw data can be recovered perfectly. The result of recovered data and imaging results are given. This method can help us to perform further application research of SAR imaging.;Raw data compress method of Synthetic Aperture Radar based on compressive sensing;Not health related;Not health related;0
"C. Rudiger; J. -C. Calvet; A. Brut; J. -P. Wigneron; B. Berthelot; A. Chanzy; S. Cros; M. Berger";2007;For the preparation of the Soil Moisture and Ocean Salinity (SMOS) mission, due for launch in 2008, a synthetic study for the aggregation and disaggregation of L-band brightness temperature fields is currently being undertaken for a 5-year period (2000–2005) over south-western France. The observed soil moisture is derived from offline simulations obtained from the Météo-France land surface model ISBA-A-gs. The radiative transfer model L-MEB is used to estimate the corresponding brightness temperature at 8km resolution, with a large number of possible incidence angles for each time step. Aggregation of the distributed information is then undertaken by simulating overpasses of SMOS over the region. Finally, the disaggregation method is an extension of the approach presented by Merlin et al. (2005).;Aggregation and Disaggregation of Synthetic L-band Soil Moisture Data over South-western France in Preparation of SMOS;Not health related;Not health related;0
"PengZhi Cao; RongQing Xu; RongTan Liu";1997;The C-SAR raw data simulator is a fundamental component of the C-band spaceborne Synthetic Aperture Radar (SAR) ground demonstration system being developed by China Aerospace Corporation (CASC). The simulator provides the demonstration system with the radar raw signal in real time, and further forms a complete data stream together with the onboard radar equipment and the ground signal processor. It will be used to verify and test the SAR system equipment during the system integration testing before practical launching. Besides of this, the simulator can also be used to evaluate the performance of the imaging algorithm. This paper introduces the design and implementation of the C-SAR raw data simulator in the context of the whole demonstration system. The computer simulation models for the raw data simulator are described. Finally, some results of the simulation experiment are given and their effective manipulation for verifying and test purposes are shown.;C-SAR spaceborne synthetic aperture radar raw data simulator;Not health related;Not health related;0
"K. Hill; C. Macon; B. Kent; T. Van";2005;For future Shuttle missions, a radar system consisting of a new wideband C-band radar and two Weibel continuous pulse Doppler X-band radars has been implemented for adequate detection of debris during launch and ascent. Three radars will digitally record tracking data of the Shuttle from launch until signal is lost with the primary timeframe of interest being launch to launch plus 150 seconds. In this paper, radar cross sections and range profiles of the Shuttle are computed by Xpatch for these three radars.;Analysis of the RCS and High Resolution Synthetic Radar Track Data for the Space Shuttle Mission STS-114;Not health related;Not health related;0
"Y. Ding; W. Fan; F. Zhou";2020;The existence of wideband interference (WBI) would seriously reduce the SAR imaging quality and the following image interpretation accuracy. However, it is difficult to mitigate WBI owing to its large bandwidth and severe overlapping with useful signal. This paper proposes a WBI mitigation algorithm based on variational Bayesian inference. Firstly, a low-rank matrix factorization model for WBI is established according to the low rank characteristics of WBI in time-frequency domain. Then, we build the Bayesian posterior probability model for the low rank matrix factorization. Finally, the variational Bayesian inference is utilized to estimate the model parameters and reconstruct the WBI. The experimental results of WBI mitigation using measured WBI data acquired by the sentinel-1 satellite have verified the effectiveness of the proposed algorithm.;Wideband Interference Mitigation for Synthetic Aperture Radar Data Based on Variational Bayesian Inference;Not health related;Not health related;0
"Q. H. Nguyen; K. Y. Lee; M. T. Aung; T. Bretschneider; I. McLoughlin";2009;From the literature review, there are two constant false alarm rate detectors for detecting edges in multi-look fully polarimetric synthetic aperture radar (POLSAR) imagery, namely the likelihood ratio edge detector and the Roy's largest eigenvalue-based edge detector. In the latter approach, one major restriction is the computation complexity, i.e. in the context of the chosen C language-based implementation. Thus, in this paper, a novel hardware-based architecture is presented to improve the processing time for the Roy's largest eigenvalue-based edge detection. The algorithm was implemented in a field-programmable gate array (FPGA) with an accelerated solution targeting data rates of up to 1 Gb/s. Its performance was examined using nine-look NASA/JPL C-band data and evaluated in terms of processing speed and accuracy as compared to the C language-based implementation on a personal computer (PC) with a Core¿ 2 Duo processor clocked at 2.2 GHz.;Hardware-accelerated edge detection for polarimetric synthetic aperture radar data;Not health related;Not health related;0
"T. Berger; S. -E. Hamran; N. Ødegaard; M. -J. Øyan; L. Damsgård";2015;An autofocus scheme for stripmap synthetic aperture radar (SAR) data using dominant scatterers is given. The focus of the paper is on assessing the performance on real data from an experimental Ku-band gated frequency modulated continuous wave (FMCW) system. The measurements are performed from a rooftop rail of 33 m length. An inertial navigation system consisting of a satellite navigation receiver and an inertial measurement unit (IMU) produces input to a post processing software. The satellite receiver only gives centimeter relative accuracy, while including the IMU data we get millimeter relative accuracy. The result when applying the millimeter accuracy navigation data serves as a reference for the proposed autofocus scheme. A time domain backprojection is applied to the data, with the centimeter accuracy navigation data as input. At Ku-band this will not be sufficient to get focused SAR images. Dominant scatterers are chosen from the partly focused image, and spatial filters are applied to the image before an inverse wavenumber algorithm is applied to get clean range profiles. The phase history of the clean range profiles are then used in the autofocus algorithm. The result from the autofocus scheme is compared to the image produced when including the IMU data in the navigation post processing software.;Stripmap autofocus of short range Ku-band synthetic aperture radar data;Not health related;Not health related;0
"J. Kun; W. Bingfang; L. Qiangzi; T. Yichen";2009;Remote sensing for the monitoring of agricultural crops has been widely used in the past. Synthetic aperture radar (SAR) system, for its characteristics of all-weather, all-day image obtain capacity, is an attractive source of information for agriculture crop classification applications, particularly in regions where cloud cover is a problem. The accuracy with which crops can be classified is dependent on a range of sensor properties, including the SAR operating configuration. This paper focuses on the effect of integrating C- and X-band SAR data on the improvement of classification accuracy. The study was carried out on Yucheng Ecological Experimental Station (China). Radarsat-2 and TerraSAR-X data were acquired, and during the satellite overpass, the ground investigation was implemented. Support vector machine classifier was used to classify the image based on the backscattering coefficients and texture features. The classification was conducted separately on Radarsat-2, TerraSAR-X and the integrating of the two. The performance of single band SAR was not acceptly good, but the integrating of the two had a great increase on classification accuracy (more than 10%). With different frequency we would get more information about the earth surface. Integrating of multiband of SAR data was a dependable way to improve classification accuracy.;Improvement of classification accuracy integrating C- and X-band synthetic aperture radar data;Not health related;Not health related;0
"S. A. Kislyy; I. A. Kuzmin; R. N. Kolesnikov; V. K. Tsvetkov; A. A. Zatonskaya; K. S. Lyalin";2022;Modern satellite and aircraft synthetic-aperture radars (SAR) require high pulse rate frequency as well as high data sample rate to obtain better range and azimuth resolution images without distortion. These properties lead to significant data streams transferring from a data sampling device to a storage device or to a remote ground station. Thus, two main characteristics have to be considered: the data storage device volume and the data write/transfer rate. The capacity of the storage devices develops rapidly, although it may still be a problem for space devices due to the lack of radiation hardened ICs. However, the data transfer rate is also a problem and it is restrained by some physical limitations. The paper proposes an approach to compress the amount of data by combining responses from several probing pulses with orthogonal binary sequences. Presented simulation results and compressed radar images show the solid application potential of the proposed technique.;Synthetic-Aperture Radar Observation Data Amount Compression Technique Using Orthogonal Binary Sequences;Not health related;Not health related;0
"R. Lv; P. Li; G. Song; Y. Li; H. Lu; X. Yang; P. Dang";2017;This paper briefly discusses the working principle of synthetic aperture microwave radiometer system. Through the system sensitivity and repeatability test, The sensitivity and repeatability of the system are obtained. Based on the test, a correction algorithm for calibration error of calibration network sensitivity and receiver repeatability is proposed, which can be used to calibrate the data in different environments on the satellite and provide the basis for the future onboard calibration.;Study on data processing method of synthetic aperture microwave radiometer;Not health related;Not health related;0
"M. D. Wigh; T. M. Hansen; A. Døssing";2021;We investigate if it is theoretically possible to discriminate between unexploded ordnance (UXO) and non-UXO sources by modelling the magnetic dipole moment for ferrous objects of different shapes and sizes. This is carried out by approximating the volumetric demagnetization factors of rectangular prisms, representing shapes similar to a long rod or flat steel plate. By modelling different UXO as prolate spheroids the demagnetization factors can be determined which can be compared with the magnetic response of a prism. The inversion is carried out in a probabilistic framework, where the UXO forward model and the non-UXO forward model are assigned individual prior models in terms of shape, size, orientation and remanent magnetization of the object. 95 independent realizations of the prism prior model are generated to make 95 synthetic anomalies exemplifying non-UXO objects, which are inverted for using the UXO model. It is investigated if an identical magnetic moment can be produced between the two models and how well resolved the magnetic moment is in terms of the measured anomaly. The case study is carried out in two steps where we first have little prior information of expected UXO properties and another where a UXO prior is introduced with expected values of aspect ratio and size of 24 different UXO, that are often encountered in the North Sea. With no prior information of expected UXO, discrimination is at many times implausible, unless elongated rod prism objects are considered, where the magnetic moment often can not be reproduced by a spheroid. Introducing the UXO prior we achieve a much better discrimination rate when using the list of expected UXO properties. By using the UXO prior we can account for a much higher remanent magnetization allowed in the prior, and still achieve high discrimination capabilities in comparison to a case with no UXO prior.;Synthetic case study: discrimination of unexploded ordnance (UXO) and non-UXO sources with varying remanent magnetization strength using magnetic data;Not health related;Not health related;0
"A. Balenzano; G. Satalino; F. Lovergine; F. Mattia; O. Cartus; M. Davidson; M. Al-Khaldi; J. Johnson";2018;The objective of this study is to cross-compare three algorithms for retrieving surface soil moisture (SSM) from ESA's Sentinel-1 (S-1) data. The context is provided by the large scientific and application interest in SSM products at high resolution and regional/continental scale that can be retrieved from S-l data alone or in combination with other missions such as NASA/SMAP and ESA/SMOS. Of the three investigated algorithms, one inverts a scattering model exploiting a Bayesian approach, whereas the other two are change detection approaches. The cross-comparison is carried out by using both simulated and experimental data. Strengths and weaknesses of the three algorithms are identified and discussed.;Cross-Comparison of Three SAR Soil Moisture Retrieval Algorithms Using Synthetic and Experimental Data;Not health related;health related;0
"W. Naranjo Lourido; L. E. Munoz; J. E. Pereda";2015;This paper proposes a methodology to obtain a synthetic driving cycle by using GPS data as input. The aim is to obtain a driving cycle that is representative of a given condition when used for energy consumption studies. The methodology begins with a data acquisition phase, where an adequate sampling frequency is proposed. Velocity raw data are processed and clustered to capture a mobility global pattern. A synthetic driving cycle is generated from the clustered information using a two level energy model optimization. For a case study, a representative synthetic driving cycle was determined for a Bus Rapid Transit (BRT) route.;A Methodology to Obtain a Synthetic Driving Cycle through GPS Data for Energy Analysis;Not health related;Not health related;0
"M. Belke; P. Blanke; S. Storms; W. Herfs";2022;The handling of objects is a crucial robotic skill for the automation of the production industry. The trend to use machine learning to estimate the 6D pose of objects is driven by higher robustness and faster processing times. Machine-learning based 6D pose estimation algorithms are available with varying estimation performance, robustness and flexibility. Suitable algorithms have to be selected based on use-case specific production requirements. A concept to evaluate these algorithms is presented. The generation of synthetic data based on the production requirements is proposed, followed by an evaluation of the algorithms to assess the generalization performance from generic benchmark datasets to custom industrial datasets. The overall pipeline is presented, realized and discussed.;Object pose estimation in industrial environments using a synthetic data generation pipeline;Not health related;Not health related;0
"M. Pergeorelis; M. Bazik; P. Saponaro; J. Kim; C. Kambhamettu";2022;Semantic segmentation is a critical component of scene understanding in underwater environments. Deep learning models have shown the best performance in semantic segmentation tasks. Deep learning benefits from large amounts of labeled imagery for training. However, there is a significant gap in size between the above-ground and underwater datasets. This work addresses the dataset size and class imbalance problems of underwater datasets by creating synthetic images to increase effective dataset size. Using the semantic segmentation mask, we can cut and paste objects from one image into another. A synthetic dataset created this way could also address class imbalances by automatically adding more instances of underrepresented classes. Doing this can create a subpixel boundary around inserted objects that networks will learn to detect instead of the intended targets. To solve this problem, we compare two different blurring methods, Gaussian and Poisson, to reduce the impact of this effect. We evaluate different methods, such as Gaussian and Poisson blurring and the number of targets added to synthetic images, to find the best way to blend inserted objects in underwater imagery. In addition, we assess the different methods to see which one produces the most realistic training set by comparing the performance metrics of various semantic segmentation algorithms after training on synthetic datasets employing each method.;Synthetic Data for Semantic Segmentation in Underwater Imagery;Not health related;Not health related;0
"K. U. Rani; G. N. Ramadevi; D. Lavanya";2016;Classification, one of the important data mining techniques plays an important role in classifying the data for knowledge discovery. The data sets contain some redundant and unnecessary attributes which mislead the classifiers. Feature Extraction techniques such as Principal Component Analysis (PCA) can be used to overcome high-dimensionality problem. Class imbalance is another problem which leads difficulty in learning for classifiers. Hence imbalanced datasets should be restructured using the resampling techniques. Synthetic Minority Oversampling Technique (SMOTE) can control the number of examples and distribution to achieve the purpose of balancing the dataset through synthetic new examples. In this study the performance of SMOTE with five classifiers on breast cancer data sets are analyzed.;Performance of synthetic minority oversampling technique on imbalanced breast cancer data;health related;health related;0
"N. R. Schwartz; A. I. Zaghloul";2010;"Ground penetrating radar (GPR) is widely studied for detection of landmines and mine-like targets; GPR is particularly useful for detecting minimum metal mines which are harder to detect using traditional metal detection devices alone. In order to expand the phenomenology for GPR, careful measurements of homogeneous and heterogeneous soils with and without targets (landmine simulants and metal objects) are performed in a controlled environment. These measurements will aid in the development of models for soil response beyond the initial air-ground interface. Such explicit models for soil will increase the effectiveness of algorithms designed to discriminate between returns from targets and from naturally occurring geologic materials and interfaces. Furthermore, careful measurement of soil characteristics will enable comparison and refinement of models of soil with embedded targets developed in electromagnetic simulations (using finite-difference time-domain codes (FDTD), e.g.).";Ground penetrating radar measurements: Applications to synthetic data generation and target characterization;Not health related;Not health related;0
"H. Wang; X. Meng; C. Zhang; J. Li";2021;Objective: this study is aimed at exploring an approach to extend algorithm assessment of artificial intelligence medical device software. Method: clinical fundus photos are collected with approval from ethical review boards, as a baseline test set. Mathematical models are developed to augment the test set and simulate several types of image variation, including detector modification, focusing adjustment and illumination fluctuation. The synthetic test sets applied in the testing of artificial intelligence medical device software, using sensitivity and specificity as major metrics. Result: in case of detector modification, the sensitivity and specificity both drop 2% during the test. In case of focusing variation, the sensitivity and specificity change 25% and 15% respectively. In case of illumination variation, the maximum fluctuation of sensitivity and specificity is 15% and 7.5% respectively. Conclusion: in this paper, fundus photos are augmented through white box manners to simulate image variation in the real world. The algorithm performance on the augmented test sets shows significant fluctuation. This testing approach may help better reveal the weakness of artificial intelligence medical device software and understand its robustness.;Performance Assessment of Artificial Intelligence Medical Device Software Using Synthetic Data;health related;health related;1
"S. Medasani; G. U. Reddy";2018;Speckle noise, a granular noise, occurs in synthetic aperture radar (SAR) data due to the interference of reflected signals with several scatterers in a resolution cell. One of the simplest techniques to suppress speckle noise from polarimetric SAR data is to use local statistics. The Lee filter employs sample mean and variance of pixels of data degraded by speckle noise, which can be multiplicative, additive, or a mixture of both, in a searching window. The refined Lee method utilizes directional windows with local minimum mean square error (LMMSE), which ensures superior maintenance of spatial resolution and features. The Lee-Sigma filter is constructed on the basis of the two-sigma probability, which effectively suppresses the speckle noise. However, insufficiencies were observed in the generation of a biased evaluation, blurring of edges, and suppression of point targets. To eliminate these insufficiencies, the improved Lee-Sigma filter was developed, which employs the minimum mean square error estimate as a priori mean. Thus, an excellent maintenance of point targets and subtle details is displayed. In this study, the Lee, refined Lee, Lee-Sigma, and improved Lee-Sigma filters were evaluated using full polarimetric data. The evaluated results indicated that the improved Lee-Sigma filter performed better than other local statistics filters.;Analysis and Evaluation of Speckle Filters by Using Polarimetric Synthetic Aperture Radar Data Through Local Statistics;Not health related;Not health related;0
"S. Adhikary; S. P. Tiwari; S. Banerjee";2022;"Marine transportation has high economic importance because the majority of the physical goods trades occurs via sea routes. However, minor and major oil spills can occur for a variety of natural and man-made reasons, resulting in partial or complete leaks from ships. Because of the limited opportunities for rescue in the mid-ocean, this situation has the potential to be fatal. Therefore real-time monitoring of the oil spills from ships is a huge challenge. Due to the availability of advanced satellites equipped with Synthetic Aperture Radar (SAR), it is now possible to monitor oil spills in oceans in real-time. Earlier, deep learning was widely used for this purpose; however, it requires a large amount of training data as well as computational power. We propose a novel image processing technique based on blob detection that considerably reduces computational resource usage while maintaining consistent detection accuracy. We achieved detection accuracy of up to 95.3% and detected an oil spill patch within 6.9 milliseconds. The proposed model outperforms deep learning methods by 20 times faster detection with a tradeoff of only 0.8% accuracy. The proposed modelling approach will enable automated real-time detection ship oil spills at low cost in terms of infrastructure and maintenance.";Realtime Oil Spill Detection By Image Processing of Synthetic Aperture Radar Data;Not health related;Not health related;0
"K. Van Leemput; T. Van den Bulcke; T. Dhollander; B. De Moor; K. Marchal; P. van Remortel";2008;The development of structure-learning algorithms for gene regulatory networks depends heavily on the availability of synthetic data sets that contain both the original network and associated expression data. This article reports the application of SynTReN, an existing network generator that samples topologies from existing biological networks and uses Michaelis-Menten and Hill enzyme kinetics to simulate gene interactions. We illustrate the effects of different aspects of the expression data on the quality of the inferred network. The tested expression data parameters are network size, network topology, type and degree of noise, quantity of expression data, and interaction types between genes. This is done by applying three well-known inference algorithms to SynTReN data sets. The results show the power of synthetic data in revealing operational characteristics of inference algorithms that are unlikely to be discovered by means of biological microarray data only.;Exploring the Operational Characteristics of Inference Algorithms for Transcriptional Networks by Means of Synthetic Data;Not health related;Not health related;0
"P. Imperatore; A. Pepe; P. Berardino; R. Lanari";2015;A general-purpose parallel scheme for efficiently focusing synthetic aperture radar (SAR) data on multicore-based shared-memory architectures is presented. The proposed parallel solution is based on a canonical processing pattern that exploit a segmented-block-based approach, and it works successfully on data acquired by different SAR platforms. Insofar as significant portion of the focusing algorithm is amenable to tiling, our approach decomposes the problem into simpler subproblems of the same type, also providing a suitable mechanism to explicitly control the granularity of computation through the proper specification of the tiling. Relevant implementation makes use of multithreading and high-performance libraries. Achievable performances are then experimentally investigated by quantifying the benefit of the parallelism incorporated into the prototype solution, thus demonstrating the validity of our approach.;A segmented block processing approach to focus synthetic aperture radar data on multicore processors;Not health related;Not health related;0
"T. Berger; S. -E. Hamran";2012;In this paper we compare the resulting images using three autofocusing techniques and navigation data from measurements performed from a rooftop rail system with a C-band radar developed at FFI. We describe the system and its characteristics, and we apply the autofocusing techniques to images of a scene containing a corner reflector and other targets. The results show that all autofocusing techniques improve the quality of the images, and the methods using a single prominent point give better results than the more general phase gradient autofocus method. The experiments also show that the measured data can be used to reliably estimate the range displacements of the aperture over the flight path when a fixed point in the scene is used a reference.;Experiments with autofocus for strip map synthetic aperture radar data;Not health related;Not health related;0
"Z. Wang; J. C. Olivier";2021;We propose three types of Markov models to generate high-resolution data every five minutes. Experimental data was obtained from a monitor station in southwest China, as well as near rivers of varying land-river connectivity in the Hampshire Avon catchment in England. The comparison of the original high-resolution wind speed and the synthetic high-resolution data from three types of models shows that the statistical characteristics including mean value, autocorrelation, maximum value, minimum value, amplitude probability distribution and variance are satisfactorily reproduced. The amplitude probability density distribution of synthetic data aligns with the Weibull distribution to a good extent. We demonstrate that the Kullback-Leibler divergence of synthetic data from the duplex algorithm model is reduced by 16.7% and 28.6% compared to a second-order and a first-order Markov model, which significantly reduces information loss. The generalization of the duplex algorithm model shows errors are small and within acceptable limits. The result shows that the model generalizes well in some areas.;Synthetic High-Resolution Wind Data Generation Based on Markov Model;Not health related;Not health related;0
"K. Xu; Z. Tian; J. Tang; J. Wang; S. Zhang";2017;Motion compensation, referred to as micronavigation, is an unavoidable issue to obtain a high quality image for multiple receiver synthetic aperture sonar (SAS). A feasible micronavigation method based on the inertial navigation system (INS) data is presented in this paper. The INS data is obtained from some high-precision inertial navigation equipment, thus the accuracy of motion errors can be guaranteed. Using the back projection algorithm, some imaging results of experiment data are shown in the end of paper. Meanwhile, some imaging results focused by the conventional chirp scaling algorithm are also given, and the compared results show the effectiveness of the presented method.;An INS data-based micronavigation method for the imaging of multiple receiver synthetic aperture sonar;Not health related;Not health related;0
"D. Chang; G. Zhang; X. Yong; J. Gao; Y. Wang; W. Wang";2022;Deep learning is a data-driven technique that demands network models trained using big datasets. In seismic structure interpretation, it is very difficult, time-consuming, and relatively economic costs to prepare training datasets by directly annotating real seismic data. Seismic convolution method is an efficient way to synthesize seismic data, which can easily and quickly generate a large amount of training datasets. However, there are large differences in feature space between synthetic seismic data and real seismic data. That results in the poor performance of network models trained using synthetic training datasets on real seismic data. In this article, we propose to use Fourier domain adaptation (FDA) to achieve domain transfer. First, the amplitude spectrum of synthetic seismic data is replaced with those of real seismic data to make feature space mapping. Then synthetic seismic data is used for transfer training of network models to improve their performance on real seismic data. The experimental results demonstrate that the FDA performs the domain transfer of synthetic seismic data to real seismic data, which improves the generalization of network models trained based on synthetic seismic data. Meanwhile, the FDA is a promising method for deep learning model transfer training in seismic structure interpretation.;Deep Learning Using Synthetic Seismic Data by Fourier Domain Adaptation in Seismic Structure Interpretation;Not health related;Not health related;0
"R. Mitra; D. Varam; E. Ali; H. Sulieman; F. Kamalov";2022;The primary objective of this paper is to present a set of synthetically generated datasets as a benchmark for evaluating feature selection algorithms (FSAs). The use of synthetic datasets is encouraged because of their utility in controlling data parameters, including the exact number of relevant, redundant, and irrelevant features. This paper proposes four numeric datasets with several sources of inspiration, namely based on geometric objects, trigonometric equations and multi-cut linear combinations. These synthetically generated datasets come with a fixed number of relevant, redundant and irrelevant features, which are then evaluated using feature selection algorithms currently popular within industry and academia. This highlights the function of these datasets as benchmarks for future researchers in the field of feature selection. Accordingly, the datasets will also be made available through GitHub for use as evaluation metrics, whilst the code is made available to be modified according to the application for the researcher. This may include research into the performance of FSAs, the development of new synthetic data, and beyond.;Development of Synthetic Data Benchmarks for Evaluating Feature Selection Algorithms;Not health related;Not health related;0
"G. Byambatsogt; G. Koutaki; L. Choimaa";2019;In this paper, we propose a new framework for generating big sized dataset using synthetic data generation by robotics. In learning-based recognition, for example, using convolutional neural networks (CNNs), it is critical for the performance, to collect high-quality and large amounts of training data. Previously, to increase the training data set, a data augmentation technique based on digital signal processing were applied to the original sound data. However, the data augmentation based on digital signal processing data is a limited method, because it depends on some previous knowledge of the data and cannot perform for all domains. On the other hand, we propose a new dataset collection technique using a robot that automatically plays instruments, by which it becomes possible to add high-quality data to training samples. Experimental results for guitar chord recognition show that the proposed method using CNNs and a guitar robot can outperform the CNN systems with the traditional data augmentation.;Improved Chord Recognition using Synthetic Data Generation by Robot;Not health related;Not health related;0
"N. Reddy Nandyala; R. Kumar Sanodiya";2023;The ability to detect life in challenging underwater environments holds the potential to preserve many aquatic species and coral reefs. Recent object detection research has witnessed a remarkable upsurge in natural images but not in Underwater, due to the imbalanced lighting, inadequate contrast, frequent occlusions, and the mimicry displayed by aquatic life forms. The assessment of object recognition models utilized in various contexts has augmented the need for annotated datasets. Due to the labor-intensive nature of generating these datasets, we have opted to undertake training using synthetic images as an alternative. In this study, we train the cutting-edge YOLO object detection system on a synthetic underwater dataset, with the aim of achieving category-agnostic object detection and then evaluated through practical assessments conducted on real underwater images. In addition, we provide benchmarking results for different YOLO versions in this work, assessing their performance on both real-world and synthetic datasets. Our investigation reveal that YOLOv5 shines in its ability to perform on synthetic data, whereas the latest YOLOv8, excels in real data domains, outpacing other two models tested. These findings have far reaching implications for the design and development of object detection in underwater environments.;Underwater Object Detection Using Synthetic Data;Not health related;Not health related;0
"S. Savastano; R. Guida";2018;A new methodology, based on the International Organization for Standardization (ISO) Guide to the expression of Uncertainty in Measurement (GUM), for the analysis of uncertainties in Synthetic Aperture Radar (SAR) remote sensing data is presented. The principal idea is to assess the sources of uncertainty, developing computational approaches to propagate uncertainties through the whole SAR signal processing chain. The final goal is to construct uncertainties budgets to quantify the total uncertainty of SAR products, permitting their `trace-ability' to international reference standards.;Uncertainty Quantification in Synthetic Aperture Radar Remote Sensing Data Processing;Not health related;Not health related;0
"D. Dadamia; M. Acuña; E. De Luca; A. Oviedo; M. Palomeque; M. Thibeault; P. Ferrazzoli; L. Guerriero";2015;This paper reports model simulations of L band backscattering for dominant crops in Pampas region (Argentina). The objective is to exploit L band signatures that will be provided by SAOCOM SAR. For corn, sunflower, soybeans and wheat, a previously developed theoretical model has been run with input data sets representative of these crops. Results of parametric simulations, covering realistic ranges of crop height and soil moisture, are shown. Validation results, based on measurements in Italy and the US are also presented. The models were further run using ground measurements directly collected in Pampas, where radar experimental campaigns are in progress.;Generating a synthetic data base of polarimetric signatures to exploit SAOCOM observations over Pampas;Not health related;Not health related;0
"B. A. Inan; D. Rondao; N. Aouf";2023;LiDAR point-cloud segmentation is a crucial issue for autonomous cars applications. The standard method for segmenting large-scale point clouds is to project 3D point cloud onto a 2D LiDAR image and apply convolutions to it. In this paper, we also follow this method and we want to detect and classify occurrences of road-objects, namely cars, cyclists, and pedestrians. To achieve this goal, we adapted the SqueezeSeg deep neural network. To address the challenge of obtaining labeled data for training autonomous driving systems, we used the CARLA autonomous driving simulator to generate a synthetic dataset in a simulation environment. The proposed network is initially trained on real-world LiDAR point-cloud data acquired from the KITTI dataset. Then, we created a synthetic dataset using the CARLA autonomous driving simulator in order to obtain more data and determine its impact on the validation accuracy of real-world data. To compare our current work to earlier work, we employ the same method. Our synthetic dataset has additional classes, such as cyclists and pedestrians, and when combined with real-world data, it significantly improves validation accuracy for each class, surpassing previous work. This demonstrates the effectiveness of our approach in detecting and classifying road-objects using LiDAR point-clouds, which is essential for the safe operation of autonomous vehicles.;Enhancing LiDAR Point Cloud Segmentation with Synthetic Data;Not health related;Not health related;0
"M. P. Pepin; J. J. Sacchini";1996;Examines the conditions necessary to preserve damped exponentials in synthetic aperture radar (SAR) phase histories. A small number of 2D damped exponentials can model SAR data when the radar return is due to a few point scatterers. Error in parametric estimation of the exponential parameters results from the method of data collection, spotlight SAR or radar chamber measurement. This error is quantified and compared to the estimation bound (Cramer-Rao). Methods to reduce this error and preserve true point scatterer locations are successfully applied. Also, a fast method of focusing SAR data to the point scatterer locations is developed.;Preserving exponentials in synthetic aperture radar data;Not health related;Not health related;0
"T. A. Øigård; A. Hanssen; R. E. Hansen";2004;The heavy-tailed Multivariate Normal Inverse Gaussian (MNIG) distribution is a recent variance-mean mixture of a multivariate Gaussian with a univariate inverse Gaussian distribution. Due to the complexity of the likelihood function, parameter estimation by direct maximization is exceedingly difficult. To overcome this problem, we propose a fast and accurate multivariate Expectation-Maximization (EM) algorithm for maximum likelihood estimation of the scalar, vector, and matrix parameters of the MNIG distribution. Important fundamental and attractive properties of the MNIG as a modeling tool for multivariate heavy-tailed processes are discussed. The modeling strength of the MNIG, and the feasibility of the proposed EM parameter estimation algorithm, are demonstrated by fitting the MNIG to real world wideband synthetic aperture sonar data.;The Multivariate Normal Inverse Gaussian distribution: EM-estimation and analysis of synthetic aperture sonar data;Not health related;Not health related;0
"M. Grimmer; H. Zhang; R. Ramachandra; K. Raja; C. Busch";2022;The vast progress in synthetic image synthesis enables the generation of facial images in high resolution and photorealism. In biometric applications, the main motivation for using synthetic data is to solve the shortage of publicly-available biometric data while reducing privacy risks when processing such sensitive information. These advantages are exploited in this work by simulating human face ageing with recent face age modification algorithms to generate mated samples, thereby studying the impact of ageing on the performance of an open-source biometric recognition system. Further, a real dataset is used to evaluate the effects of short-term ageing, comparing the biometric performance to the synthetic domain. The main findings indicate that short-term ageing in the range of 1-5 years has only minor effects on the general recognition performance. However, the correct verification of mated faces with long-term age differences beyond 20 years poses still a significant challenge and requires further investigation.;Time flies by: Analyzing the Impact of Face Ageing on the Recognition Performance with Synthetic Data;Not health related;Not health related;0
"J. Adams; E. Murphy; J. Sutor; A. Dodd";2021;A literature review was conducted using journal articles and conference proceedings to examine emerging research practices, and applications of synthetic visual data over the past 5 years. The current research examined articles related to research trends in artificial intelligence training intended to improve computer vision and object detection. Search strings were developed and used to retrieve research articles from the ACM and IEEE databases. The resulting articles were examined for trends, general practices, disciplines where the greatest efforts have been made, advances, and relevant production processes. The research reveals that visual synthetic data encompasses filtering, augmentation, and object domain randomization techniques. Further, all of the research that included an evaluation of synthetic visual data suggest that there are noteworthy performance improvements in accuracy. Additionally, producing realistic synthetic data reduces the current limitations related to labeling, image quality, paucity of relevant data, and privacy issues.;Assessing the Qualities of Synthetic Visual Data Production;Not health related;Not health related;0
"S. Khan; M. Mishra; V. K. Tiwari";2018;Various biometric verification techniques are in use today like iris, fingerprint, signatures, hand vein structure, voice, face etc. The Direct problem states the feature extraction and matching whereas another branch which deals with the generation of synthetic biometric is an inverse biometric problem. This paper presents the effects and uses of such synthetic biometric. In some cases, it can be used to replace the original dataset so as to save time and expenses whereas on the other hand it can be used for spoofing and forgery. Therefore inverse biometric problem has a wide application in forensic investigation, systems with lack of available training sets, generation of anti-spoofing security systems and simulation of controlled and flexible training skills. This paper also suggests a probabilistic approach for an inverse biometric problem with respect to offline signature verification systems.;Effect and Uses of Synthetic Data Generation in Inverse Biometric Problem;Not health related;Not health related;0
"G. F. Araujo; R. Machado; M. I. Pettersson";2023;This article proposes a method to simulate Synthetic Aperture Radar (SAR) targets for specific incidence and azimuth angles. Images synthesized by Electromagnetic Computing (EMC) are used to train a Conditional Generative Adversarial Network (cGAN). Two synthetic image chips of the same class and incidence angle, separated by two degrees in azimuth, are used as input to the cGAN. The cGAN predicts the image of the same class and incidence angle whose azimuth angle corresponds to the bisector of the two input chips. An evaluation using the SAMPLE dataset was performed to verify the quality of the image prediction. Running through a total of 100 training epochs, the cGAN converges, reaching the best Mean Squared Error (MSE) after 77 epochs. The results demonstrate that the proposed method is promising for Automatic Target Recognition (ATR) applications.;A Tailored cGAN SAR Synthetic Data Augmentation Method for ATR Application;Not health related;Not health related;0
"M. Solla; S. Lagüela; M. X. Álvarez; H. Lorenzo; B. Riveiro";2012;This work presents a multidisciplinary approach to detect moisture in ancient masonry bridges, which has proven as a valuable tool to support the preservation of such historic assets. For the evaluation, non-destructive assessment was considered by means of ground-penetrating radar (GPR), photogrammetry and infrared thermography. Because of the inner heterogeneity of masonry structures, the analysis and interpretation of field GPR data resulted complex. Sophisticated finite-difference time-domain (FDTD) modelling was therefore used to assist and to improve in the interpretation of the field data. Simulations were elaborated using a mixed model of parallelization, and more realistic and large scale models were built from the accurate data provided by photogrammetric and thermographie procedures.;A multidisciplinary non-destructive approach to analyze moisture in historic masonry structures: Integration of both field and synthetic GPR data generated from photogrammetric and infrared imaging;Not health related;Not health related;0
"J. Ock; H. No; S. Kim";2023;The transition of paradigm from non-standalone to standalone mode in 5G network creates an opportunity to utilize innovative machine learning and AI-based technology for network traffic analysis. In particular, NWDAF plays a key role in leveraging AI-based models to optimize and enhance the 5G core network functions, including anomaly detection and load balancing. However, it is challenging to ensure the sufficient performance of prediction algorithms under dynamic conditions in NWDAF without guaranteeing the quality and quantity of training data, but only a select group has access to the 5G dataset. To overcome this issue, this paper proposes an approach to generate high-quality synthetic 5G NWDAF data using CTGAN, a specialized generative model that creates synthetic output based on tabular input data. We provide preliminary results of leveraging CTGAN to generate 5G synthetic data and evaluate the synthesizing quality for anomaly detection.;Poster: Exploring Synthetic Data Generation for Anomaly Detection in the 5G NWDAF Architecture;Not health related;Not health related;0
"A. S. Juhé; D. Rinck; A. Maier";2023;In the future, medical imaging devices such as computed tomography (CT) and magnetic resonance (MR) are expected to become increasingly autonomous. Therefore, design criteria and workflow of such devices will change substantially. Moreover, sensor data of the system are required to develop scene understanding algorithms to support and guide the user. In this work, we present an approach to generate synthetic 3D point cloud data from a simulated MR examination experienced on the Microsoft Hololens2. The complete workflow of an MR examination using a virtual, autonomous MR scanner is reproduced in an AR scene. The user is recorded by a system of active stereo vision RGBD-cameras while interacting with AR elements. A registration routine of the AR scene and the RGBD-cameras is described, and accuracy measurements are provided. The real point clouds are fused with virtually generated point clouds from the AR scene.;Generation of Synthetic 3D Data Using Simulated MR Examinations in Augmented Reality;Not health related;Not health related;0
"B. D. Bue; E. Merényi";2010;We describe a proof of concept for class knowledge transfer from a labeled hyperspectral image to an unlabeled image, captured with a different (hyper-/multi-spectral) sensor, when the spatial extents of the images partially overlap. By defining a set of spatio-spectral correspondences between the labeled source image and the unlabeled target image, we create a mapping between the images we can use to propagate labels from the source to the target image. This mapping allows us to classify the target image using the source labels without manually defining training labels in the target image. We evaluate the technique using state of the art synthetic hyperspectral imagery.;Using spatial correspondences for hyperspectral knowledge transfer: Evaluation on synthetic data;Not health related;Not health related;0
"J. P. Göpfert; C. Göpfert; M. Botsch; B. Hammer";2017;Convolutional neural networks have recently shown great success in computer vision. They are able to automatically learn complicated mappings, often reaching human or super-human performance. However, a lack of labeled data can preclude the training of such networks. This is the case in the reconstruction of 3-dimensional human heads from 2-dimensional photographs. Approaching the problem backwards, starting from 3-dimensional heads and using photo-realistic rendering, one can create any number of training data to tackle the problem. This way, fine control over the data allows for new insights into how a convolutional neural network interprets data and how variability in the training and test data affect its performance. We perform a systematic analysis in order to determine how the presence of different types of variability in the training data affects the generalization properties of the network for 3-dimensional head reconstruction.;Effects of variability in synthetic training data on convolutional neural networks for 3D head reconstruction;Not health related;Not health related;0
"E. Ucuzova; E. Kurtulmaz; F. Gökalp Yavuz; H. Karacan; N. E. _ahın";2021;"It aims to develop an artificial intelligence model that can analyze driver behavior and compare the model's performance between real data and synthetic data by testing this model on different synthetic data. In general, a huge amount of data is needed to develop a successful artificial intelligence model; especially, due to reasons such as the difficulty of obtaining the necessary data to model driver behavior, ways of synthesizing new data with the real data at hand were investigated. Accordingly, the synthpop library, which can create an entirely new data set from real data by maintaining the basic statistics and distribution of the data, and doing this with the CART algorithm, was used. The synthetic data set obtained is tested on an artificial intelligence model that performs driver behavior analysis trained with real data; test results of both data were compared and, as a result, promising results were obtained. Accordingly, it has been concluded that data from different fields, especially in areas where it is difficult to obtain data such as vehicle usage data, can be used to increase the performance of existing models by reproducing with the synthpop library.";Synthetic CANBUS Data Generation for Driver Behavior Modeling;Not health related;Not health related;0
"H. Lee; K. Lee; H. Yang; S. -Y. Oh";2020;Object detection is one of the main task for the deep learning applications. Deep learning performance has already exceeded human's detection ability, in the case when there are lots of data for training deep neural networks. In the case of military fields, there are needs to resolve the data shortage problem to employ deep learning system efficiently with benefits. Generating the synthetic data can be a solution, but the domain gap between the synthetic and real data is still an obstacle for training the model. In this paper, we propose a method for decreasing the domain gap by applying style transfer techniques to synthetic data for military vehicle detection. Utilizing FastPhotoStyle to the synthetic data aids efficiently improving the accuracy of object detection when the real data is insufficiency for training. Specifically, we show that stylization which enables artificial data more realistic diminishes the domain gap by evaluating the visualization of their distributions using principal component analysis and Fréchet inception distance score. As a result, the performance has been improved about 8% in the AP@50 metric for stylized synthetic data.;Applying FastPhotoStyle to Synthetic Data for Military Vehicle Detection;Not health related;Not health related;0
"J. Bakker; J. van der Kruk; J. Bikowski; H. Vereecken";2011;A large permittivity contrast between a thin surface layer and the underlying substratum, caused by rapid changing dynamic processes in the subsurface, can result in waveguide dispersion. In some cases, a single-layer waveguide cannot explain the measured electromagnetic waves. Here, we show an analysis of four synthetic dispersive datasets of two-layer leaky waveguides which have large and small permittivity contrasts between the layers as well as an increasing and a decreasing permittivity with depth. The four synthetic datasets are inverted with a multi-layer inversion algorithm for leaky waveguides and with different single-layer inversion algorithms. For the two-layer waveguides with a large increasing permittivity with depth, it is not possible to reliably reconstruct the model parameters of the lowest layer. For small contrasts within the waveguide, the single-layer inversion algorithm resulted in an arithmetic mean value for the permittivity and a total waveguide thickness.;Two-layer inversion of dispersive GPR data due to freezing induced waveguides — A synthetic study;Not health related;Not health related;0
"S. Fukuda; N. Hirosawa";1998;A classification scheme of land covers in SAR images using wavelet-based textural information is presented. The proposed scheme treats multifrequency polarimetric data. Extraction of texture features, polarization selection, feature reduction, and so on, are discussed. The scheme has been successfully applied to the Flevoland site.;Land cover classification from multifrequency polarimetric synthetic aperture radar data using wavelet-based textural information;Not health related;Not health related;0
"M. J. Kastle; J. A. Malas";2010;Discrepancies can result when creating common data sets consisting of comparable synthetic and measured range complex scattered field samples when the phase references of each do not coincide. This can be especially true when using signal processing techniques to produce one dimensional (range profiles) or two dimensional (Synthetic Aperture Radar or SAR images) representations of the target scattered field where range bins and cross-range bins are formed. Range profiles and SAR images can be misaligned or have different bin amplitudes due to target scatterers in synthetic and measured scenarios shifted with respect to one another. Obtaining equivalent data samples requires attention to the measured data calibration process and phase reference location. This paper will address the common phase reference problem by an analysis of experimental data for specific targets and rotation system. Suggestions are provided for possible solutions to current challenges. The data analysis will include synthetic and measured range data comparisons, range calibration, and target position and range alignment processes using Theodolite laser measurements.;Establishing a common phase reference for comparing synthetic data to RF range measurements;Not health related;Not health related;0
"S. S. Saif; K. Aras; A. Giuseppi";2022;Automated Optical Inspection (AOI) is among the most common and effective quality checks employed in production lines. This paper details the design of a Deep Learning solution that was developed for addressing a specific quality control in a Printed Circuit Board Assembly (PCBA) manufacturing process. The developed Deep Neural Network exploits transfer learning and a synthetic data generation process to be trained even if the quantity of the data samples available is low. The overall AOI system was designed to be deployed on low-cost hardware with limited computing capabilities to ease its deployment in industrial settings.;Automated Optical Inspection for Printed Circuit Board Assembly Manufacturing with Transfer Learning and Synthetic Data Generation;Not health related;Not health related;0
"O. Can; Ö. Er; Y. Kunt";2021;It is not always possible to find enough data in Deep Learning applications, where huge amounts of data is needed. One may face difficulties while trying to perform qualified works, where data cannot be collected in rare occuring events and situations involving privacy concerns. To overcome this issue, we create a military vehicle dataset in which great amounts of Synthetic Dataset can be found. Thus, with this dataset, we bring a solution to the problem of detecting military vehicles, which is an area where collecting sufficient data is a problem. In a training with 5, 10 and 50 epochs we raise the mAP score by 0.09, 0.03, and 0.02, respectively.;Use of Synthetic Data on Object Detection Applications;Not health related;Not health related;0
"M. MahdianPari; M. Motagh; V. Akbari";2012;This paper proposes a novel speckle reduction method that combines an advanced statistical distribution with spatial contextual information for SAR data. The method for despeckling is based on a Markov random field (MRF) that integrates a K-distribution for the SAR data statistics and a Gauss-MRF model for the spatial context. These two pieces of information are combined based on weighted summation of pixel-wise and contextual models. This not only preserves edge information in the image, but also improves signal-to-noise ratio (SNR) of the despeckled data. Experiments on real SAR data demonstrate the effectiveness of the algorithm compared with well-known despeckling methods.;Speckle reduction and restoration of synthetic aperture radar data with an adoptive Markov random field model;Not health related;Not health related;0
"A. G. Putrada; M. Abdurohman; D. Perdana; H. H. Nuha";2023;Machine learning has become one of the main pillars of smart lighting. In a machine learning project life-cycle, the next step is model deployment after the intelligence model has good performance. However, this can be a problem because sometimes the deployment environment is not yet perfect, so a deployment simulation is required. Our research aims to create and evaluate synthetic data and simulations for smart lighting control using the nested Markov chain. We have an original smart lighting dataset resulting from making a smart lighting prototype that utilizes passive infrared (PIR) sensors. We construct a Markov chain model and a nested model from this dataset. We generate synthetic data from the two models and then evaluate them using the classification-integrated moving average (CIMA) model we developed in previous research. The test results show that the synthetic dataset resulting from the nested Markov chain model is more approximate to the original dataset than the Markov chain model. The p-value of the movement value and attendance value of the nested Markov chain is greater than the p-value of the Markov chain, namely 0.45 and 0.22, respectively. The performance of the CIMA model on synthetic datasets resulting from the nested Markov chain is also better than the Markov chain.;Synthetic Data with Nested Markov Chain for CIMA-Based Smart Lighting Control Deployment Simulation;Not health related;Not health related;0
"P. Blonda; C. Tarantino; A. D'Addabbo; G. Satalino; G. Pasquariello";2001;In this work, the results obtained in the classification of a multi-source - multi-temporal remote sensed data set by means of a distributed neuro-fuzzy system are compared with the results of a traditional centralized neural classification system, based on a single multilayer perceptron (MLP) neural network module. The distributed system is composed by a set of neural classifiers, whose partial results were combined with both Sugeno and Choquet fuzzy integrals. Two classification experiments were carried out with the distributed system. In the first experiment, each neural module of the distributed system used the same learning rule but was trained with a subset of the input features, i.e., a specific spectral band. In the second experiment, the neural modules of the system were trained with the same complete set of input features available for each training pixel, but consisted of MLP networks characterized by different specific topologies or different neural algorithms. The results show that larger improvements can be obtained by combining more independent classifiers. The Choquet fuzzy integral provided better performance than Sugeno fuzzy integral. The centralized system, based on a single MLP module, provided the best classification performance.;Combination of multiple classifiers by fuzzy integrals: an application to synthetic aperture radar (SAR) data;Not health related;Not health related;0
"D. Medda; E. Aymerich; Y. MirkoAnoffo";2018;Light field images contain information on directional lighting distribution. This means that a light field image is composed of multiple views of the same subject. Synthetic light field data is obtained using 3D rendering software, so that no equipment is. An architecture for quality evaluation of compressed synthetic light field images is proposed in this paper. Performances of two state-of-the-art video encoders (x265 and VP9) are evaluated and the obtained results are discussed.;Analysis of Synthetic Light Field Data Compression Performances;Not health related;Not health related;0
"M. Stackler; R. Pilard; J. Duvernay; M. Matthieu; J. Cochard";2019;Synthetic Aperture Radar (SAR) is a type of radar technology developed in the 1950s. It is used in an extensive list of applications from sub-surface imaging in geology to satellite based earth observation payload for natural disaster monitoring and prevention, deforestation and crops monitoring. As with most applications involving high-sampling speed data converters, future SAR system aims at reducing their size, weight, cost and power consumption while becoming more flexible in order to support multiple end-use with a single hardware platform. One solution discussed in this paper is a new type of architecture supported by the latest generation of advanced data converters: Software Defined Radio (SDR) and Software Defined Microwave (SDM). These architectures aims at reducing the amount of front-end hardware needed, push as much processing and signal treatment as possible within the processing unit, translating in higher requirement for the data converters. This paper will first introduce SDR/SDM architecture before explaining the higher ADC/DAC requirements. In a last section, performance results of Teledyne e2v's latest Analog to Digital Converter (ADC) and a Digital to Analog Converter (DAC) are shown in the X-band (8-12GHz) illustrating the benefit of SDR/SDM architecture for SAR system.;Microwave Capable Data Converters Enabling Softare Defined Synthetic Aperture Radar;Not health related;Not health related;0
"J. W. Hu; I. T. Bowman; A. Nica; A. Goel";2019;Many self-managing relational database management systems (RDBMS) need to programmatically generate synthetic data to train machine learning models. This paper proposes the concept of shadow database and a framework to derive shadow database from production database that matches distribution properties of source data. Moreover, we have designed and implemented an embedded synthetic data generation tool that takes data distribution profile as input and generates a shadow database according to histograms of source data. The distribution profile is passed into the tool either through an export-import mechanism or as a JSON string. The shadow database can scale to be larger or smaller than the original database and serve as a testbed to train learning models. Unlike most other data generation tools, our tool is implemented as SQL procedures that can be embedded in the underlying RDBMS.;Distribution-Driven, Embedded Synthetic Data Generation System and Tool for RDBMS;Not health related;Not health related;0
"G. Schnattinger; C. Baur; B. Huber";2023;State-of-the-art personnel security screening based on microwave imaging technology heavily relies on threat detection models created by using supervised machine learning techniques. This usually calls for excessive efforts to build up huge databases containing labelled sample data of people being screened. In the case of walk-through screening devices, threat detection may also incorporate a posture tracking mechanism of the person being screened. This normally calls for the creation of a dedicated posture database through the tedious and costly task of manually labelling postures in volumetric microwave images. However, it is demonstrated that by employing electromagnetic simulation, a comparable posture tracking algorithm can be created while using synthetic data. Thus, the development time and costs can be reduced while maintaining a high threat detection performance. In addition, synthetic data can improve posture detection accuracy or add support for novel postures.;Generating and Using Synthetic Data for Machine Learning in Personnel Security Screening Scenarios;Not health related;Not health related;0
"N. Karslıo_lu; H. Kaya; A. A. Salah";2019;In this work, we propose a method for automatic emotion recognition from movie clips. This problem has applications in indexing and retrieval of large movie and video collections, summarization of visual content, selection of emotioninvoking materials, and such. Our approach aims to estimate valence and arousal values automatically. We extract audio and visual features, summarize them via functionals, PCA, and Fisher vector encoding approaches. We used feature selection based on canonical correlation analysis. For classification, we used extreme learning machine and support vector machine. We tested our approach on the LIRIS-ACCEDE database with ground truth annotations. The class imbalance problem was solved by generating synthetic data. By fusing the best features at score and feature level, we obtain good results on this problem, especially for the valence prediction.;Movie Emotion Estimation with Multimodal Fusion and Synthetic Data Generation;Not health related;Not health related;0
"C. R. de Macedo; F. Nunziata; M. Migliaccio; D. Velotto";2017;In this study, SAR oil spill observation is investigated for different polarizations, incidence angles and wind conditions. The dataset is composed by a time-series of 42 TerraSAR-X dualpol SAR images acquired between July 2011 and April 2016. The incidence angle, wind speed and polarization dependency for oil spill observation is shown for the first time using a large time-series of the same oil type. Experimental results show that the oil seepage observation is impaired with increasing the incidence angle and strongly depends on wind conditions. However, those effects are more pronounced using the single-polarization channels, while they are mitigated by using dual-polarimetric channels.;Time-series of dual-polarimetric synthetic aperture radar data to observe oil seeps;Not health related;Not health related;0
"A. G. de Montgareuil; Y. Delesse; P. Malbranche";2003;For years, Genec has been intensively using its outdoor testing platform in Provence (South of France) to perform long-term modules energy output measurements. Former works at Genec showed an empirical law between the daily energy output, the energy input in kWh per square meter and the daily average ambient temperature. Recent measurements and intensive use of relational databases precise this empirical law. Determination of module efficiency in all irradiance and temperature conditions allow to express module efficiency as a linear function of average module temperature and to model this linearity. This work is funded in part by the French Agency for Environment and Energy Management (ADEME).;An empirical synthetic law between the modules energy output and the meteorological data;Not health related;Not health related;0
"G. Ariho; J. D. Paden; A. Hoffman; K. A. Christianson; N. Holschuh";2022;Ice dynamics are a maj or factor in sea level rise and future sea-level rise projections [1]. The vertical velocity profile of the ice is one major knowledge gap in both observations and model experiments. We propose to apply multipass differential interferometric synthetic aperture radar (DInSAR) techniques to data from the Multichannel Coherent Radar Depth Sounder (MCoRDS) to measure the vertical displacement of englacial layers. Estimation of englacial layer vertical displacement requires compensating for the spatial baseline between interferometric antenna pairs using radar trajectory information and estimates of the cross-track layer slope from direction of arrival (DOA) analysis, but airborne systems suffer from unknown spatial baseline errors. The current DInSAR algorithm assumes zero error in the array position information when inferring displacement and the direction of arrival for subsurface scatterers, which means that unincorporated baseline errors map into errors in cross-track slope and vertical velocities. Here we demonstrate a maximum likelihood estimator that jointly estimates the vertical velocity, the cross-track internal layer slope, and the unknown baseline error due to GPS and Inertial Navigation System (INS) errors.;Joint Estimation of Ice Sheet Vertical Velocity and Englacial Layer Geometry from Multipass Synthetic Aperture Radar Data;Not health related;Not health related;0
"Y. -D. Liu; J. Yuan; W. -j. Wang";2012;In this paper, the CMOD4 and CMOD_IFR2 models used to derive the wind field from scatterometer data are employed to retrieve the wind vector from SAR data. The quality of the retrieval results of the two models is controlled using the variational method. The results are then compared with those of numerical weather predictions. For moderate wind speeds, the two models provide a highly accurate retrieval of the wind field, with CMOD_IFR2 producing slightly more accurate results than CMOD4.;Retrieval of the Sea-Surface Wind Field from Synthetic Aperture Radar Data;Not health related;Not health related;0
"A. L. le Roux; M. da Silveira";1996;In April 1994 a multi-frequency SAR sensor was flown onboard the Space Shuttle Endeavour. This work involves the classification of ground cover in an area around Pietermaritzburg using data captured on this mission. The ground cover was classified by considering the statistics of the data. The purpose of this work is to gain experience in the use of SAR data so that its applicability to potential South African users can be shown and so that user support can be provided when an airborne SAR system is flown in South Africa.;Classification of multi-frequency synthetic aperture radar data from area around Pietermaritzburg;Not health related;Not health related;0
"B. J. Davis; T. S. Ralston; D. L. Marks; S. A. Boppart; P. S. Carney";2007;Optical coherence tomography (OCT) is an optical ranging technique analogous to radar - detection of back-scattered light produces a signal that is temporally localized at times-of-flight corresponding to the location of scatterers in the object. However the interferometric collection technique used in OCT allows, in principle, the coherent collection of data, i.e. amplitude and phase information can be extracted. Interferometric synthetic aperture microscopy (ISAM) adds phase-stable data collection to OCT instrumentation and employs physics-based processing analogous to that used in synthetic aperture radar (SAR). That is, the complex nature of the coherent data is exploited to give gains in image quality. Specifically, diffraction-limited resolution is achieved throughout the sample, not just within focal volume of the illuminating field. Simulated and experimental verifications of this effect are presented. ISAM's computational focusing obviates the trade-off between lateral resolution and depth-of-focus seen in traditional OCT.;Interferometric Synthetic Aperture Microscopy: Physics-Based Image Reconstruction from Optical Coherence Tomography Data;Not health related;Not health related;0
"X. Pic; M. Dimopoulou; E. G. S. Antonio; M. Antonini";2023;"Over the past years, the ever-growing trend on data storage demand, more specifically for ""cold"" data (i.e. rarely accessed), has motivated research for alternative systems of data storage. Because of its biochemical characteristics, synthetic DNA molecules are now considered as serious candidates for this new kind of storage. This paper introduces a novel arithmetic coder for DNA data storage, and presents some results on a lossy JPEG 2000 based image compression method adapted for DNA data storage that uses this novel coder.The DNA coding algorithms presented here have been designed to efficiently compress images, encode them into a quaternary code, and finally store them into synthetic DNA molecules. This work also aims at making the compression models better fit the problematic that we encounter when storing data into DNA, namely the fact that the DNA writing, storing and reading methods are error prone processes.The main take away of this work is our arithmetic coder and it's integration into a performant image codec.";MQ-Coder Inspired Arithmetic Coder for Synthetic DNA Data Storage;Not health related;Not health related;0
"J. López-Amaya; A. A. López-Caloca; A. Monsiváis-Huertero";2020;Data fusion methodologies have been implemented in agricultural applications with different types of sensors. One of the problems in delineating cultivation areas is the mixture of spectral signatures due to the transitions between the types of cultivation, built areas, and other natural covers. In order to improve discrimination and identification of crop types, structure data fusion techniques were evaluated. This article aims at showing the potential of using satellite data from the European Space Agency, both optical and SAR, in order to improve land cover classification of agricultural land located in Mexico. To achieve this, an analysis of the spectral, spatial and textural data was performed. Specifically, two classification algorithms were used and compared. The first is based on vector support machines and the second one on Random Forests. The methodology was applied for the study of 4 types of crops in 2017 in the municipality of Villa de Arriaga located in the state of San Luis Potosí. As final results, maps were obtained with the areas with a kappa greater than 0.80.;Identification of Agricultural Parcels using Optical and Synthetic Aperture Radar Data;Not health related;Not health related;0
"P. Laguduvan Thyagarajan; P. Berens; H. Nies; I. Ihrke";2022;Coregistration has been a major concern in SAR interferometric and tomographic processing. The coregistration of real airborne SAR data is challenging in comparison to a spaceborne case. This paper focuses on the problems involved in the coregistration of an airborne dataset obtained from the Wachtberg imaging radar in X-band operated by Fraunhofer FHR. An analysis of the incidence angles and the coherence of each dataset will be used as a measure to understand the reason behind misregistration. We explore the possibility of computing displacements in pixels using existing methods from computer vision. These techniques are very popular for optical data. Here, we test their applicability to radar data with promising outcomes.;Coregistration of Airborne Synthetic Aperture Radar Data using Optical Flow;Not health related;Not health related;0
"M. S. Reis; S. J. S. Sant'Anna; E. Pantaleão";2017;Synthetic Aperture Radar (SAR) data is very important to land cover change detection, mainly in areas of tropical forests that are constantly under cloud cover. In this study, six features extracted from two L band SAR full-polarimetric images were evaluated for region based binary change detection in a region within the Brazilian Amazon, in the years of 2006 and 2009. These features were the intensities from polarizations HH, HV and VV and the three components from Freeman-Durden polarimetric decomposition. The best results were obtained by HV intensity and the volume and double-bounce scattering components from Freeman-Durden polarimetric decomposition. Visual analysis of data indicates the volume scattering component as to better represent changes in the land cover. This feature also appears to be less sensitive to noise than the other components from the polarimetric decomposition. However, since full-polarimetric data is still limited in many areas within the Brazilian Amazon, the use of HV intensity is recommended, if only polarized data is available.;Change detection using polarimetric L band synthetic aperture radar data;Not health related;Not health related;0
"S. Seol; B. Kim";2023;While eco-friendly energy is attracting the public’s interest in recent environmental issues, the market of lithium-ion batteries is expanding. Ensuring the safety and reliability of batteries is important in using batteries. Hence, it is essential to monitor the state of health (SOH) of the battery. Model-based SOH estimation has a limitation because battery data exhibit non-linear characteristics according to complex internal changes. To overcome this limitation, the data-based estimation of the SOH of the battery has been researched extensively. Data-based SOH estimation requires a large, high-quality battery dataset. In general, a battery dataset is built based on data obtained from experimental tests. However, this approach is time-dependent. To overcome this problem, data augmentation algorithms are used. TimeGAN is one of the time-series data augmentation algorithms that generate time-series data in the latent space while preserving temporal dynamics and static characteristics. To ensure stable training of data, TimeGAN generates synthetic data after data has been preprocessed using Min-Max scaling. In this study, various preprocessing techniques were applied for TimeGAN to generate more realistic synthetic data which was close to the original battery data. Results based on different preprocessing techniques were analyzed, and the preprocessing technique suitable for generating a battery dataset was proposed. Discriminative and predictive scores were used for evaluation with t-SNE graphs for visual comparison. When Max-Abs scaling and Normalization preprocessing techniques were applied, the discriminative score improved by 53.7 and 44.4%, respectively, compared to the conventional Min-Max scaling.;Study on Improving Reliability of Synthetic Battery Data with TimeGAN Pre-processing;Not health related;Not health related;0
R. C. Robertson;1989;A procedure, referred to as wave-number filtering, is developed that dramatically reduces the undesirable effects of 3-D heterogenetics on some elements of the MT (magnetotelluric) impedance. The wave-number filtering process is evaluated by comparing the filtered and the unfiltered apparent resistivities derived from the MT impedence and the filtered and unfiltered MT impedance phases for a particular 3-D model consisting of a heterogeneous region on the surface of an otherwise homogeneous earth. In all cases, the apparent resistivity derived from unfiltered data leads to substantial error in the inferred conductivity of the basement region, while the apparent resistivity derived from filtered data leads to an accurate interpretation of the basement conductivity. As expected, the effect of wave number filtering on the MT impedance phase is much less dramatic than the effect on the apparent resistivity.<>;The effect of wave number filtering on synthetic three-dimensional magnetotelluric data;Not health related;Not health related;0
A. Sowter;1997;Satellite-based radar systems have been collecting data of the Earth's surface for some time. Prominent in this area are the ERS series of satellites, operated by the European Space Agency (ESA), which have been imaging the world for over five years. Within this period, many potential applications of the data have been found but the full economic benefits of the sensor are still being realised. This paper explores a number of applications, with a particular focus on the mapping and monitoring of land features, where satellite radar systems have a future, both in terms of offering a unique source of information and a back-up where other, more appropriate, sources of data are available. In particular the author discusses agriculture, forestry, oil and gas exploration, urban mapping and development.;Satellite synthetic aperture radar data-capability and prospects;Not health related;Not health related;0
"E. A. Lesiv; K. A. Vytovtov; E. A. Barabanova";2023;This paper discusses the problem of stabilizing a multicopter over a tethered high-altitude platform by using visual analysis systems to determine the position and orientation of the target platform. We propose a neural network architecture to solve the problem of backup local navigation system of tethered unmanned aerial vehicles in GPS-Denied and GLONASS-Denied environments. The input of the system is a video frame from the multicopter's onboard camera, and the output is the estimates of displacement and rotation of the tethered high-altitude platform. This research proposes a technique for synthesizing training examples based on the application of 3D graphics tools. Thus, the neural network is trained on synthetic data generated in a virtual environment.;Neural Network Algorithm of Video Analysis for Backup Local Navigation System of Tethered UAVs with Training on Synthetic Data;Not health related;Not health related;0
N. Patel;2023;Synthetic aperture radar (SAR) imaging data in general have not been openly accessible for consumption to the general public in the past few decades, as mainly governments have led the development of such platforms, due to the commercial industry lacking the need of such data (with few exceptions).;Open Source Data Programs From Low-Earth Orbit Synthetic Aperture Radar Companies: Questions and answers [Industry Profiles and Activities];Not health related;Not health related;0
L. R. Cadete;2000;The understorey is an important forest component in the structure and functioning of a forest. Synthetic aperture radar (SAR) images contain an interesting potential to deserve an investment for the tracking of their analysis possibilities regarding the understorey. Their acknowledged advantages when compared to optical imagery - being independent of solar light, cloud cover and hazy atmospheric conditions, and presenting increased penetration capabilities - make them attractive data sources for remote sensing applications, including the understorey case. A pathfinder study has been recently launched to analyse SAR data sensitivity to the understorey presence in maritime pine stands of central Portugal and to assess the possibilities of extracting biophysical information for this forest stratum. This document pays special attention to the radiometric and geometric corrections of the radar imagery selected.;Forest understorey study with synthetic aperture radar data;Not health related;Not health related;0
"S. Skansi; K. _ekrst; M. Kardum";2020;Summary - In this paper we propose an non-machine learning artificial intelligence (AI) based approach for telecom data analysis, with a special focus on clique detection. Clique detection can be used to identify households, which is a major challenge in telecom data analysis and predictive analytics. Our approach does not use any form of machine learning, but another type of algorithm: satisfiability for propositional logic. This is a neglected approach in modern AI, and we aim to demonstrate that for certain tasks, it may be a good alternative to machine learning-based approaches. We have used a simple DPLL satisfiability solver over an artificially generated telecom dataset (due to GDPR regulations), but our approach can be implemented on any telecom data by following the SAT encoding we have developed, and the DPLL solver can be substituted by a more advanced alternative such as CDCL. This paper extends the method presented in [1] for banking logs to data containing caller information, and proposes a more efficient encoding.;A Different Approach for Clique and Household Analysis in Synthetic Telecom Data Using Propositional Logic;Not health related;Not health related;0
"E. Rignot; R. Chellappa";1991;;Model Based Segmenatation of Synthetic Aperture Radar Data;Not health related;Not health related;0
"C. I. Kourogiorgas; A. D. Panagopoulos; J. D. Kanellopoulos; S. N. Livieratos; G. E. Chatzarakis";2013;In this paper, we examine the impact of the system and geometrical characteristics of the satellite link on the rain attenuation dynamics. More particularly, we study the variation of the rain attenuation dynamic parameter of the Maseng-Bakken model for various links configurations. Due to the increase of the operating frequency of future satellite communication systems, the dynamics of rain attenuation are very critical for the design of such systems. Applying the synthetic storm technique at the 1-year rain rate measurements at the NTUA Campus of Athens, the rain attenuation dynamics are investigated assuming that rain attenuation follows the Maseng-Bakken model as a stochastic process. Summing up, the dependence and the variability of the rain attenuation dynamic parameter with the elevation angle, frequency, wind speed and polarization is shown.;Investigation of rain fade dynamic properties using simulated rain attenuation data with synthetic storm technique;Not health related;Not health related;0
"B. Yan; S. Wang; W. Hu; X. Liu; W. Wang; Y. Liu; J. Wang; S. Qiu";2023;The application of digitalization in railway infrastructure is steadily increasing, and it accelerates the process of automating the asset management and inspection process of railway infrastructure. In this study, a point cloud segmentation method for railway infrastructure is proposed based on virtual model synthetic data and improved dynamic graph convolutional network (DGCNN). First, virtual data of railway infrastructure is created and inserted into real data for blending and augmentation, which is used to train point cloud segmentation neural networks. Acceptable segmentation results are achieved by using improved dynamic graph convolutional neural networks to train the augmented data and using the real data for point cloud segmentation. It is shown that the dataset augmented with virtual data achieves a 3.38% improvement in the accuracy of the final point cloud segmentation over that without augmentation, and the segmentation accuracy using the improved DGCNN network improves by 2.97% over that of the DGCNN network without improvement. This work could effectively improve the effect of 3D point cloud segmentation model to achieve accurate and efficient digitization of railway infrastructure.;Semantic Segmentation of Railway Infrastructure Based on Virtual Model Synthetic Data;Not health related;Not health related;0
K. Cheng;2020;Synthetic datasets are necessary for performance evaluation and function test in most database applications. In this paper, we propose a regular expression-based data generation language (DGL) for flexible test data generation. We extend the standard regular expressions to include references to external resources, sequential numbers, probability distributions, type/format inference, dictionary sampling. In order to implement the proposed scheme efficiently, result caching and database caching techniques are developed and evaluated by experiments.;A Regular Expression-based DGL for Meaningful Synthetic Data Generation;Not health related;Not health related;0
"D. D. Jersak; M. J. Byrd; A. J. Blanchard";1995;Conventional narrow angle SAR requires frequency diversity to achieve range resolution. Sufficient bandwidth is not always available, especially when using CW radars. Holographic SAR (HSAR) imaging techniques can be applied to a wide variety of different measurement configurations, including monochromatic as well as frequency diverse. An introduction to HSAR theory is presented. This is followed with several case studies, including the specific application to monochromatic swept-angle bistatic data.;Application of holographic synthetic aperture radar techiques to monochromatic swept-angle bistatic data;Not health related;Not health related;0
"S. Lyra; J. Jin; S. Leonhardt; M. Lüken";2023;Neonatal sepsis is one of the most serious complications in neonatal intensive care units. Due to the often immature immune system, sepsis-related comorbidities are the major contributors to increased neonatal mortality. The rapid progression of the disease makes early treatment critical for patient survival. However, early diagnosis of sepsis remains difficult due to its non-specific symptoms. In recent years, Machine Learning-based techniques have been used in various medical applications to predict diseases using clinical data. In this work, we optimized and evaluated four prediction models with different architectural concepts. Two public datasets containing clinical data from adults and neonates were used for training. The adult data were collected to pre-train the models. Since neonatal data with sepsis diagnosis are very limited, we propose an augmentation method to generate synthetic clinical data. For the final evaluation, the real data of neonatal patients were defined as a test set. An AUROC of 0.91 and an AUPRC of 0.38 were obtained. These results are promising for early prediction of neonatal sepsis using artificial data for augmentation.Clinical relevance— This work demonstrates the potential of Machine Learning-based prediction models for the detection of sepsis to improve the early diagnosis of life-threatening conditions in neonatal intensive care units.;Early Prediction of Neonatal Sepsis From Synthetic Clinical Data Using Machine Learning;health related;health related;1
"N. Dalsania; Z. Patel; B. Chaudhury; S. Purohit";2020;Electromagnetic forces, thermal, and radiation loads experienced by the in-vessel components/vacuum vessel at the time of the tokamak plasma current quench (CQ) greatly affect the health of the overall plasma device. Thus the mitigation is of paramount importance, which requires a proper identification of the disruption precursors. Using new machine learning (ML) and artificial intelligence (AI) approaches, it is possible to identify disruption precursors, however, such approaches require training the ML/AI models. This training of models requires a huge amount of experimental data, which sometimes may not be sufficient due to variation in the current quench profile across different tokamak. This necessitates the need for accurate synthetic current quench data generation for different types of the CQ profiles observed experimentally. We propose a novel approach for synthetic CQ data generation, wherein the set of experimental current quench data is clustered into groups with similar patterns and each cluster, rather than the entire set, is employed as a base for generating more realistic and accurate synthetic data consisting of all important current quench shape aspects. The synthetically generated data is evaluated and categorized as disruption, soft landing, or step landing with multiple slopes. The disruption data set is further analyzed qualitatively for the CQ nature - exponential, linear, or Gaussian. The quantification of the minimum number of experimental data sets and it's time resolution for generating meaningful synthetic data are also explored. The typical current quench parameters namely average CQ rate $(QR_{90-10})$, instantaneous CQ $((dI_{p}/dt)$, CQ time ($\tau$) have been estimated for the synthetic data, and subsequent comparison with the experimental parameter across different tokamak CQ has been found to be in a fair agreement.;A Novel Approach for Plasma Current Quench Studies Via Synthetic Data Generation;Not health related;Not health related;0
"N. Wu; Q. Liang; T. S. Durrani";2016;This paper studies co-prime sampling for two-dimensional synthetic aperture radar (SAR) imaging and proposes a new approach based on co-prime up-sampling and compressive sensing to improve the resolution of SAR images. In order to decrease the redundancy in SAR phase history, we extend the co-prime down sampling structure to the fast-time domain and introduce a random matrix to compress the data in the slow-time domain. Since the SAR image is very sparse, directly applying compressive sensing algorithm can not recover clear picture. As a result, co-prime up-sampling with gradient projection for sparse reconstruction (GPSR) algorithm is proposed in this work. Simulation results show that even after data reduction, the new approach could still acquire high resolution images. The compression ratio could be 10:1 overall.;Data compression for synthetic aperture radar and resolution improvement;Not health related;Not health related;0
"J. F. Paris; B. L. Wood";1991;;Simultaneous Image Data Acquired By A Multispectral Scanner And A Polarimetric, Multifrequency Synthetic Aperture Radar Over A Wetland Site In Colusa County, California: A Comparative Analysis;Not health related;Not health related;0
"J. Liu; J. Yu; X. Wang; W. Nie; L. Zhang; Z. Pu";2023;The training of deep learning models is inseparable from sufficient data as the driving force. However, in the power transmission model training task, it is very difficult and expensive to obtain sufficient training samples, so data augmentation has become a common means of increasing training samples. In recent years, various deep generative models have demonstrated strong sample generation capabilities in different modalities, especially generative adversarial networks (GANs) have made important progress in synthesizing compelling image and audio samples. However, the traditional GAN model faces difficulties in the large amount of data and computing resources required for training in the field of transmission lines, and there may be problems such as lack of diversity and detail in generated images, and unstable training. In order to solve these problems, this paper proposes to use a diffusion model-based method to generate transmission line images, expand the data set and improve model performance. The final transmission image generated by the model is complete and clear, and the overall model performance is improved by 1.5%.;Transmission line inspection models improved by synthetic data from diffusion models;Not health related;Not health related;0
"K. Wang; J. Chen; A. Kiaghadi; C. Dawson";2020;In this study, we developed a new SAR/InSAR-based land-cover classification algorithm for estimating surface roughness, a key parameter that is critical for accurate compound flood modeling. We successfully classified the Greater Houston area and New Orleans into nine classes with distinct surface roughness using L-band ALOS PALSAR-1 data. The ALOS-derived surface roughness estimates show an excellent agreement with those derived from NOAA's Coastal Change Analysis Program (C-CAP, 2010) land-cover classification data acquired around the same time. The new algorithm allows us to fill the temporal gaps in the existing surface roughness database. The radar-derived surface roughness estimates will be fed into the the Advanced Circulation (AD-CIRC) modeling framework to understand the compound flood risk associated with rapid land-cover changes due to urban expansion over the last decade.;A New Algorithm for Estimating Surface Roughness Using Interferometric Synthetic Aperture Radar (InSAR) Data;Not health related;Not health related;0
"V. R. Urs; V. Maiya; J. Channegowda; C. Lingaraj";2022;There’s been an increase in attempts to control the surge in pollution levels due to extensive exploitation of conventional fossil fuels. These efforts have fueled research for alternative green solutions. Lithium-ion batteries are immensely beneficial for energy storage system. They are extremely advantageous in the automobile industry, particularly as a source to power Electric Vehicles (EVs). Lithium-ion batteries are also vital for powering consumer electronics. The State of Charge (SOC) measurement is used to calculate the remaining usage time of batteries, is one of the most pertinent metric. The goal of current research has been to develop accurate State of Charge (SOC) prediction algorithms. All existing methods require significant amount of superior-quality curated dataset. However, battery researchers have minimal access to commercial battery datasets and therefore must rely on open-access public datasets that lack the required heterogeneity to generate generalised SOC algorithms. To resolve this issue of lack of data, we introduce a Sample Convolution and Interaction Networks (SCINet) to produce resilient synthetic battery data. The code implementation can be found on: https://github.com/vinayakrajurs/Sample-Convolution-Interaction-Syntheic-Data;Synthetic Data Generation using Resilient Sample Convolution and Interactive Learning Approach;Not health related;Not health related;0
"P. Singh; A. Necholi; W. Moreno";2023;Engineering education research often requires large amounts of data that can be time-consuming and costly to collect. In response to these challenges, synthetic data generation has emerged as a pragmatic solution across various industries, allowing researchers to sidestep the complexities associated with data collection and preprocessing, thereby streamlining the focus on actual model implementation. Despite the widespread adoption of synthetic data in diverse domains, its integration into engineering education research remains relatively underexplored.This paper aims to fill this gap by proposing a novel method for synthetic education data generation through the application of Bayesian approaches. The proposed approach involves leveraging a domain knowledge base to construct a Bayesian network and its corresponding Conditional Probability Tables (CPTs). The subsequent use of Gibbs sampling facilitates the generation of synthetic data, enabling a direct comparison between the characteristics of the original and synthetically generated datasets.;Synthetic Data Generation for Engineering Education: A Bayesian Approach;Not health related;Not health related;0
"R. Gitzel; A. Kotriwala; T. Fechner; M. Schiefer";2021;Machine Learning algorithms excel at tasks such as classification but require a large amount of balanced training data. In this paper we use synthetic image data to train a Convolutional Neural Network to recognize industrial non-pipeline leaks. Using the open source raytracer Blender, a set of images is rendered. We discuss the key steps for obtaining the data, explain our network architecture, and discuss the performance on real-world data. The results using purely synthetic training data are quite encouraging with an F1 score of 0.95 on real test data.;Using Synthetic Data to Train a Visual Condition Monitoring System for Leak Detection;Not health related;Not health related;0
J. Engelbrecht;2009;Information on the distribution of surface soil moisture is important for a number of applications. Due to the high temporal and spatial variability, and consequently the cost of monitoring by field observations, a means of remote monitoring of soil moisture content using remote sensing data is needed. The aim of this study was to test soil moisture retrieval algorithms based on synthetic aperture radar data (SAR). This includes the use of Envisat ASAR and ALOS PALSAR data, which was provided by the European Space Agency. Both linear regression and multiple-polarization models were applied for soil moisture quantification. The results could not be validated due to a lack of distributed field-based measurements but were compared to rainfall figures over the same period. Though inconclusive, the results suggest that the techniques show promise in their ability to quantify surface soil moisture.;Synthetic aperture radar data employed for soil moisture estimation in the Piketberg region, South Africa;Not health related;Not health related;0
"M. Barnell; C. Raymond; A. Salmin; D. Brown; D. Isereau";2021;New model quantization techniques have been researched to inform future information/data processing approaches. This hardware evaluation and the associated data exploitation methods developed support systems that require high performance computing (HPC) and machine learning (ML) models where significant throughput is desired. Specifically, these applications include, but are not limited to, low cost compute that supports the detection and classification of objects in a scene, where it is not feasible to spend valuable resources on HPC capabilities. Additional applications include data exploitation upstream, near sensors, where size, weight and power (SWAP) are constrained. This research included the analyses of representative data, synthetic aperture radar (SAR) imagery called the Moving and Stationary Target Acquisition and Recognition (MSTAR) data. The NVIDIA Tesla, Xavier, and Titan compute architectures were used and analyzed as part of this research. These graphics processing units (GPU) represent architectures that span various operating powers (~10 to over a few 100 Watts). Additionally, the energy utilization per frame was determined and analyzed, e.g., the energy use of the Tesla went from 104.4 to 50.68 micro-Joules/frame when quantization was reduced to an 8-bit integer. The Tesla architecture also improved processing throughput from 448 frames per second (FPS) to 1085 FPS when quantized to 8-bit integers. An important part of this new research showed the compute systems retained SAR detection/classification performance of over 97% mean average precision (mAP) on the MSTAR imagery data after quantization – thereby retaining its capability to detect and classify objects.;Model Quantization and Synthetic Aperture Data Analyses Increasing Throughput and Energy Efficiency;Not health related;Not health related;0
"A. Topal; M. F. Amasyali";2021;Synthetic data generation is one of the methods used in machine learning to increase the performance of algorithms on datasets. However, these methods do not ensure success on each dataset. In this study, it has been investigated that which type of synthetic data generation algorithms are useful in which datasets by examining the effects of SMOTE, Borderline-SMOTE and Random data generation algorithms on 33 datasets. For this, each dataset has been fully balanced as a result of synthetic data generation. In order to evaluate the results, datasets are divided into three groups as balanced, partially balanced-unbalanced and unbalanced in accordance with the unbalance ratio. The datasets formed as a result of the data generation of the algorithms and the original datasets have been trained with an ANN models and their performance has been evaluated on the test set. Experimental results have shown that adding synthetic data to the datasets with the abovementioned algorithms generally increases the success in balanced and partially balanced-unbalanced datasets, but generally does not work in unbalanced datasets. Borderline-SMOTE, which produces border samples in balanced datasets, and SMOTE in partially balanced-unbalanced datasets have been more successful.;When does Synthetic Data Generation Work?;Not health related;Not health related;0
D. Y. Lai;1999;Interaction between surface waves and horizontally varying surface currents produce variations in sea surface roughness which can be observed by a synthetic aperture radar (SAR). An inversion model is developed to extract surface currents from SAR images. This data inversion is based on iterative fitting of observed SAR data to forward models of wave-current interaction, radar scattering, and SAR imaging. The inversion model is applied to SAR measurements of solitary internal waves in the New York Bight during the SAR Internal Wave Signature Experiment (SARSEX) (Gasparovic et al., 1988). The surface currents of internal waves thus extracted are compared with in-situ measurements. A spaceborne SAR can provide all-weather, day-night imagery of the ocean with a swath of /spl sim/50 km and a spatial resolution of <20 m. The ability to extract accurate surface currents from SAR images provides a cost-effective means for obtaining surface current measurements over a wide area. This technique is especially useful when areas are imaged frequently by the sensor, such as about once a day for NASA's Lightweight SAR (LightSAR) currently in the planning stage.;Extraction of surface currents of solitary internal waves from synthetic aperture radar data;Not health related;Not health related;0
"S. E. Seyed Roghani; E. Koyuncu";2020;With every new synthetic RGB-D data set, the scenes are becoming more and more photo-realistic. However, the depth-estimator models trained on these sophisticated data sets still fail in generalization to real scenes. Recent attempts to overcome this challenge mainly include transferring style of the RGB images from synthetic to real or vice versa. Such approaches make it possible to train the depth-estimator without the availability of real image-depth pairs. On the other hand, this study considers a case in which a limited number of real RGB-D samples are also available. The real samples' availability allows for fine-tuning as an approach to fill the gap between the style of the real and synthetic samples. Results show that no matter how many real samples are available, to fit the real samples to all the parameters of the pre-trained model (considered for this study) is always better than only fine-tuning the parameters of the encoder of the model. Also, it is in the availability of less than 12.5% of the real data required to achieve the best performance of the model that complete fine-tuning provides results better than training from scratch. This study, also details the process of building a synthetic data for flaying robots in the Unity game Engine.;Fine-tuning Monocular Depth-Estimator Artificial Neural Networks Trained on Synthetic RGB-D Data Sets for Real Scenes;Not health related;Not health related;0
"A. R. Asadi; P. -L. Loh";2023;We study the Gibbs exponential mechanism and its use in generating private synthetic data. We first present bounds on the expected utility of the Gibbs and generalized Gibbs mechanisms based on their privacy parameters. Next, we provide two notions of privacy and functionals of utility with respect to which the Gibbs mechanism is provably optimal among all mechanisms. These rely on a valuable property of Gibbs distributions relating them to relative entropy, called the Gibbs variational principle, and its extension to Rényi divergences and generalized moments. Finally, we study how to use the Gibbs mechanism to generate synthetic data privately. Combining known results in empirical process theory with the privacy-utility tradeoff results of this paper, we derive bounds on the utility of the Gibbs mechanism as a function of the size of the synthetic database.;On the Gibbs Exponential Mechanism and Private Synthetic Data Generation;Not health related;Not health related;0
A. M. K. Szeto;1995;A method for obtaining the Fourier transform of a synthetic aperture radar reflectivity map is described where the reflectivity map itself does not need to be produced from the radar data. The procedure essentially rearranges a sequence of numerical integrations in such a manner that allows one or more numerical integration to be replaced by a closed-form expression involving Fresnel integrals.<>;On the direct determination of reflectivity spectrum from synthetic aperture radar data;Not health related;Not health related;0
"M. Lakshminarayanan; N. S. John; J. Channegowda; A. Raj; F. Naaz";2021;Artificial Intelligence (AI) has revolutionized technology in the recent past. AI in the energy sector has proven to be of great importance at forecasting energy loads in power systems and predicting state of charge in batteries. Although ample improvements have been made in the energy sector, access to experimental battery datasets has hampered progress. This has resulted in restricted improvement of machine learning models. This paper aims to provide guidelines to produce high fidelity battery parameter data using Generative Adversarial Networks (GANs). This approach was used to produce synthetic data using the NASA Prognostic Data Repository and smartphone battery dataset. Discriminative and predictive scores of 0.0422, 0.0703 and 0.0638, 0.1427 were obtained for the smartphone charge dataset and NASA Prognostic dataset respectively.;Devising High Fidelity Synthetic Data using Generative Adversarial Networks for Energy Storage Systems;Not health related;Not health related;0
"J. E. S. Fransson; J. Wallerman; A. Gustavsson; L. M. H. Ulander";2013;Synthetic Aperture Radar (SAR) backscatter data from the Swedish airborne CARABAS-II and LORA systems were used to estimate stem volume at stand level. The study was performed in hemi-boreal forests at the Remningstorp test site, located in southern Sweden. In total, ten 80 m _ 80 m stands, where all trees were measured in situ, with stem volumes in the range of 70-530 m3 ha-1 (on average 347 m3 ha-1) were analyzed. SAR data from CARABAS-II and LORA were acquired from two different years, with nine unique flight headings that were repeated for each system and year. Regression analysis was used to estimate stem volume and the accuracy was assessed in terms of Root Mean Square Error (RMSE). As a first step, stem volume was estimated for each flight heading separately. The accuracy assessment was then performed by weighting the separate estimates for each system and year inversely proportionally to the variance about the regression function. The best results for CARABAS-II and LORA showed a relative RMSE of 7% and 24% of the mean stem volume, respectively. In a previous study, stem volume was estimated using LiDAR data and the same forest stands, resulting in an RMSE of about 12%. In conclusion, the estimation accuracy of stem volume using combined low-frequency CARABAS SAR images was found to be superior to that from using LiDAR data for the stands investigated.;Estimation of stem volume in hemi-boreal forests using airborne low-frequency Synthetic Aperture Radar and lidar data;Not health related;Not health related;0
"H. Wu; J. Tang";2021;For the problem of the low computational efficiency in the existing CSA based on MSR for the multiple-receiver SAS because it is required to perform image reconstruction for each receiver, an efficiency CSA based on MSR for the multiple-receiver SAS is proposed in this paper. Firstly, to make that the range cell migration, azimuth modulation, and range modulation of different receiver can be represented by that of the reference receiver, the signal of each receiver is shifted to the same shortest range. Secondly, to obtain the imaging result by performing one imaging reconstruction, the multiple receivers signal shifted is transformed into the monostatic signal by azimuth reconstruction. Thirdly, the imaging reconstruction based on MSR's 2D spectrum is derived by utilizing the chirp scaling principle. Finally, the effectiveness of the proposed algorithm is validated by the simulation experiments.;An Efficiency Imaging Algorithm for the Multiple receivers Data in Synthetic Aperture Sonar;Not health related;Not health related;0
"M. Rajendran; C. T. Tan; I. Atmosukarto; A. B. Ng; A. Grant; E. Cameracci; S. See";2023;The construction and operation of Metaverse virtual environments, e.g., 3D reconstruction and activity detection is an important supporting technology of computer vision. Recently, synthetic data has seen a surge in adoption for model training in computer vision. Prior research generally show a positive correlation between the volume of synthetic training data and inference accuracy. This paper focuses on the domain of activity detection, and explores how to improve the performance of such algorithms using synthetic data. In particular, we present an overview of the state-of-the-art in using domain randomization approaches for synthetic data generation. This paper presents initial inference accuracies of a model trained on initial attempts at domain randomized synthetic data (7.2%), compared to a model trained on real-world data (9.2%). The synthetic data, although performed worse, indicated promising trajectories for future work, approximately 2% away from the real-world result.;Exploring Domain Randomization’s Effect on Synthetic Data for Activity Detection*;Not health related;Not health related;0
"X. Liu; B. Yuan; C. Liu; G. Song; M. Han; J. Lv";2023;In this paper, the influence of low resistivity cover layers, exploration resolution and other factors on high-density electrical method is illustrated by several groups of typical model synthetic data inversion examples and two engineering examples. In practical application, it is necessary to combine other geological data to improve the identification of abnormal bodies. Generally speaking, the more obvious the electrical difference between the target body and surrounding rock, the better the premise of high-density electrical method.;Inversion Research of Synthetic Data of Typical Models and Application in Engineering Geophysics of High-Density Electrical Method;Not health related;Not health related;0
R. de Porras Bernácer;2017;"The present paper describes the design strategies conceived to implement a module able to accomplish different tasks in the baseband environment of a synthetic aperture radar (SAR). This module, called the FGM (Formatting and Gyro stabilizing Module), is part of the QUASAR project for UAVs [1], currently being developed at INTA's radar laboratory [2]. The main features of the FGM can be resumed as followed: Receive, format, compress, filter, decimate and send raw data from the Acquisition Module to the radar's storage unit and the real time processing unit using serial line protocols; read back stored data or processed images and send them to a radio link module; synchronous generation of clock and attenuation data for the D/A Converters of the system; receive position and attitude data related to the movement of the airplane in order to adjust the transmission parameters of the radar and control the antenna's pointing direction (gyro stabilization); communicate with system's host through a PCIe bus and, finally, monitor the temperature gradients of the custom PCBs of the system.";Designing a Synthetic Aperture Radar’s Data Formatting and Antenna Gyro Stabilizing Module;Not health related;Not health related;0
"T. N. Varga; D. Gruen; S. Seitz; N. MacCrann; E. Sheldon; W. G. Hartley; A. Amon; A. Choi; A. Palmese; Y. Zhang; M. R. Becker; J. McCullough; E. Rozo; E. S. Rykoff; C. To; S. Grandis; G. M. Bernstein; S. Dodelson; K. Eckert; S. Everett; R. A. Gruendl; I. Harrison; K. Herner; R. P. Rollins; I. Sevilla-Noarbe; M. A. Troxel; B. Yanny; J. Zuntz; H. T. Diehl; M. Jarvis; M. Aguena; S. Allam; J. Annis; E. Bertin; S. Bhargava; D. Brooks; A. Carnero Rosell; M. Carrasco Kind; J. Carretero; M. Costanzi; L. N. da Costa; M. E. S. Pereira; J. De Vicente; S. Desai; J. P. Dietrich; I. Ferrero; B. Flaugher; J. García-Bellido; E. Gaztanaga; D. W. Gerdes; J. Gschwend; G. Gutierrez; S. R. Hinton; K. Honscheid; T. Jeltema; K. Kuehn; N. Kuropatkin; M. A. G. Maia; M. March; P. Melchior; F. Menanteau; R. Miquel; R. Morgan; J. Myles; F. Paz-Chinchón; A. A. Plazas; A. K. Romer; E. Sanchez; V. Scarpine; M. Schubnell; S. Serrano; M. Smith; M. Soares-Santos; E. Suchyta; M. E. C. Swanson; G. Tarle; D. Thomas; J. Weller; DES Collaboration";2021;"We develop a novel data-driven method for generating synthetic optical observations of galaxy clusters. In cluster weak lensing, the interplay between analysis choices and systematic effects related to source galaxy selection, shape measurement, and photometric redshift estimation can be best characterized in end-to-end tests going from mock observations to recovered cluster masses. To create such test scenarios, we measure and model the photometric properties of galaxy clusters and their sky environments from the Dark Energy Survey Year 3 (DES Y3) data in two bins of cluster richness $\lambda \in [30; 45)$, $\lambda \in [45; 60)$ and three bins in cluster redshift ($z\in [0.3; 0.35)$, $z\in [0.45; 0.5)$ and $z\in [0.6; 0.65)$. Using deep-field imaging data, we extrapolate galaxy populations beyond the limiting magnitude of DES Y3 and calculate the properties of cluster member galaxies via statistical background subtraction. We construct mock galaxy clusters as random draws from a distribution function, and render mock clusters and line-of-sight catalogues into synthetic images in the same format as actual survey observations. Synthetic galaxy clusters are generated from real observational data, and thus are independent from the assumptions inherent to cosmological simulations. The recipe can be straightforwardly modified to incorporate extra information, and correct for survey incompleteness. New realizations of synthetic clusters can be created at minimal cost, which will allow future analyses to generate the large number of images needed to characterize systematic uncertainties in cluster mass measurements.";Synthetic galaxy clusters and observations based on Dark Energy Survey Year 3 Data;Not health related;Not health related;0
"M. Simpson; N. K. N. Aznan; J. Brennan; P. Watson; P. James; J. Jonczyk";2022;Passenger behaviour on public transport has become a source of great interest in the wake of the COVID-19 pandemic. Operators are interested in employing new methods to monitor vehicle utilisation and passenger behaviour. One way to do this is through the use of Machine Learning, using the CCTV footage that is already being captured from the vehicles. However, one of the limitations of Machine Learning is that it requires large amounts of annotated training data, which is not always available. In this poster, we present a technique that uses 3D models to generate synthetic training images/data and discuss the effect that training with the synthetic data had on the Machine Learning models when applied to real-world CCTV footage.;Generating Synthetic Images & Data to Improve Object Detection in CCTV Footage from Public Transport;Not health related;Not health related;0
"D. Sufiyan; Y. H. Pheh; L. Soe Thura Win; S. K. Hla Win; U. -X. Tan; S. Foong";2023;To successfully adhere to flight plans, aerial vehicles must keep track of their location in 3D space, which is usually reliant on external references such as GNSS which are susceptible to interference. To develop self-reliant onboard positional localization, a workflow using 360-degree panoramic images in an image-based localization system using a Deep Convolutional Neural Network is proposed. 360-degree panoramic images have the advantage that they take into account visual information from all angles. Model performance is also enhanced by generating synthetic data from a 3D model of the region of interest created via photogrammetry techniques. The performances of different training configurations are compared, and the configuration with mixed real and synthetic data exhibits the highest performance, an approximately 10 to 15 percent improvement over using solely real data. Additional image augmentations also further reduce the localization error by 8 to 15 percent.;Panoramic Image-Based Aerial Localization using Synthetic Data via Photogrammetric Reconstruction;Not health related;Not health related;0
C. Tan;2019;Having access to high-quality test data is an important requirement to ensure effective cross-organizational integration testing. The common practice for addressing this need is to generate synthetic data. However, existing approaches cannot generate representative datasets that can evolve to allow the simulation of the dynamics of the systems under test. In this PhD project, and in collaboration with an industrial partner, we investigate the use of machine learning techniques for developing novel solutions that can generate synthetic, dynamic and representative test data.;A Model-Based Approach to Generate Dynamic Synthetic Test Data;Not health related;Not health related;0
"J. Tesarik; J. Vrba";2020;Microwave imaging (MWI) method could provide a great opportunity for early stroke diagnosis in the future and thus reduce the health consequences caused by stroke. Based on different dielectric properties of healthy and stroke tissue MWI systems can help to differentiate the type of stroke in prehospital care. The main purpose of this contribution was to validate the newly designed multilevel 24-port MWI system on synthetic numerical data. Inside the 3D human head phantom, the different stroke phantom types (HEM - haemorrhagic or ISCH - ischemic) with different diameters were placed. Using the reconstruction algorithm based on Born Approximation and TSVD the stroke phantoms could be followed and distinguished. The numerical analysis of MWI system showed promising results where positions, diameters and types of stroke phantoms were successfully reconstructed. The system proved some limitations as a disability to detect objects with a size lower than half of the used wavelength which can be eliminated by increasing the operating frequency range in the future.;Validation of Multilevel 24-port Microwave Imaging System for Brain Stroke Monitoring on Synthetic Numerical Data;Not health related;Not health related;0
"R. Gaetano; D. Amitrano; G. Masi; G. Poggi; G. Ruello; L. Verdoliva; G. Scarpa";2014;Reliable segmentation of SAR images requires some forms of user supervision: we resort here to the interactive version of the Tree-Structured Markov Random Field (TS-MRF) segmentation suite. The TS-MRF model, and the associated segmentation tool, provide a flexible and spatially adaptive description of the data. In the interactive version, the user can drive the process based on the inspection of the current result, deciding step-by-step which direction to take, and switching from one segmentation modality to another. Experiments with the segmentation and classification of multitemporal SAR images prove the potential of the interactive approach and of the TS-MRF tool.;Interactive segmentation of high resolution synthetic aperture radar data by tree-structured MRF;Not health related;Not health related;0
"S. -H. Yun; S. Owen; F. Webb; H. Hua; P. Milillo; E. Fielding; M. Simons; P. Agram; C. Liang; A. Moore; P. Sacco; E. Gurrola; G. Manipon; P. Rosen; P. Lundgren; A. Coletta";2016;The April 25, 2015 M7.8 Gorkha earthquake caused more than 8,000 fatalities and widespread building damage in central Nepal. Four days after the earthquake, the Italian Space Agency's (ASI's) COSMO-SkyMed Synthetic Aperture Radar (SAR) satellite acquired data over Kathmandu area. Nine days after the earthquake, the Japan Aerospace Exploration Agency's (JAXA's) ALOS-2 SAR satellite covered larger area. Using these radar observations, we rapidly produced damage proxy maps derived from temporal changes in Interferometric SAR (InSAR) coherence. These maps were qualitatively validated through comparison with independent damage analyses by National Geospatial-Intelligence Agency (NGA) and the UNITAR's (United Nations Institute for Training and Research's) Operational Satellite Applications Programme (UNOSAT), and based on our own visual inspection of DigitalGlobe's WorldView optical pre- vs. post-event imagery. Our maps were quickly released to responding agencies and the public, and used for damage assessment, determining inspection/imaging priorities, and reconnaissance fieldwork.;Recent rapid disaster response products derived from COSMO-Skymed synthetic aperture radar data;Not health related;Not health related;0
"M. Methini; J. Priyadharshini";2022;"In order to monitor the vehicle transportation and notify the individual about the information of the obstacles based on the analysis performed in the MPEG4 video file. The parameters to predict the severity of the impact has also been analyzed. An approach to generate synthetic annotated information about the obstacle has been formulated. The information obtained could then be used to train deep neural networks in computer vision tasks. This procedural modeling approach enables physically accurate image synthesis obtained from hand modeled virtual worlds. A sample MPEG4 Video file of vehicle transportation is provided as initial input to the workspace for experimental purpose. Image analysis has been done and based on specific characteristic feature; the actual object present is identified. The same approach would be used in automotive applications in near future.";Procedural Modelling for Synthetic Data Generation in Automotive Applications;Not health related;Not health related;0
"L. Jovanovic; A. Petrovic; T. Zivkovic; M. Antonijevic; N. Bacanin; M. Zivkovic";2023;Proper artificial intelligence (AI) and machine learning (ML) model implementation and training are highly reliant on quality input data. However, the high costs of clinical studies and increasing privacy concerns limit the availability of medical data to researchers. This work explores the potential of applying generative adversarial networks (GAN) for generating synthetic medical data based on publicly available real-world datasets. Synthetic data is generated based on real-world observations and used to train several contemporary ML and AI models. These models are subjected to comparison with models trained on real data. Both trained models are evaluated under identical conditions on real-world testing data. These well-known, high-quality datasets are explored in this study covering diabetes and heart disease. While models trained on synthetic data show a slight decrease in objective performance, they still demonstrate viably accurate outcomes. This suggests that the introduced approach presents a potential stepping stone towards improvising data availability for researchers while respecting patients’ best interests and patient privacy.;Exploring the Potential of Generative Adversarial Networks for Synthetic Medical Data Generation;health related;health related;1
"A. D. Beall; A. J. Lewis";1998;Floating marshes (flotant) are widely distributed in coastal Louisiana and have been studied for many years. Enhanced backscatter can be a useful tool in detecting and mapping flotant on synthetic aperture radar (SAR) data. Ormsby et al. (1985) noted an enhanced return from flooded forests on L-band SAR imagery and from flooded marsh grasses on X-band SAR imagery. The L-band wavelength is capable of penetrating the forest canopy allowing the water surface and tree trunks to act as a corner reflector providing an enhanced return signal. In flooded marsh grasses, the L-band wavelength is reflected. However, the X-band wavelength shows an enhanced return from the water surface and grass stem acting as a corner reflector. X-band SAR is the optimum wavelength for detecting the type of flooded vegetation in coastal marshes. The acquisition of X-band SAR and the completed mapping of flotant in the Barataria Basin of coastal Louisiana allows the critical evaluation of this radar data set for the detection and mapping of flotant throughout the coastal zone of Louisiana.;Detecting and mapping flotant using synthetic aperture radar data;Not health related;Not health related;0
"R. Anand; A. K. Thittai";2020;Compressed Sensing (CS) has been applied by a few researchers to improve the frame rate of synthetic aperture (SA) ultrasound imaging. However, there appear to be no reports on reducing the number of receive elements by exploiting CS approach. In our previous work, we have proposed a strategic undersampling scheme based on Gaussian distribution for focused ultrasound imaging. In this work, we propose and evaluate three sampling schemes for SA to acquire RF data from a reduced number of receive elements. The effect of sampling schemes on CS recovery was studied using simulation and experimental data. In spite of using only 50% of the receive elements, it was found that the ultrasound images using the Gaussian sampling scheme had comparable resolution and contrast with respect to the reference image obtained using all the receive elements. Thus, the findings suggest a possibility to reduce the receive channel count of SA ultrasound system without practically sacrificing the image quality.;Compressed Sensing for Data Reduction in Synthetic Aperture Ultrasound Imaging: A Feasibility Study;Not health related;Not health related;0
M. Marghany;2015;This study is aimed at retrieving wave energy from large scale synthetic aperture radar (SAR) during different monsoon periods. In doing so, nonlinear velocity bunching model algorithm is used to retrieve the information of ocean wave spectra parameters such as significant wave height, directions, and energy on offshore, midshore, and onshore. Therefore, the maximum peak of the wave energy spectra density of 1.4 m2 s has occurred during northeast monsoon period with maximum significant wave height of 4. Clearly, the mid-shore and onshore has the highest peak of 0.8 and 1.37 m2 s, respectively as compared to offshore. Further, the maximum wave energy based on significant wave height is ml 00 kW/m. In conclusions, a nonlinear algorithm of velocity bunching can be used to retrieve the significant wave height from synthetic aperture radar (SAR). In addition, SAR can be used to map the distribution of ocean wave spectra energy.;Retrieving wave spectra energy from synthetic aperture radar of ENVISAT satellite data;Not health related;Not health related;0
"M. Landajuela; R. Anirudh; J. Loscazo; R. Blake";2022;Current state-of-the-art techniques for non-invasive imaging of cardiac electrical phenomena require voltage recordings from dozens of different torso locations and anatomical models built from expensive medical diagnostic imaging procedures. This study aimed to assess if recent machine learning advances could alternatively reconstruct electroanatomical maps at clinically relevant resolutions using only the standard 12-lead electrocardiogram (ECG) as input. To that end, a computational study was conducted to generate a dataset of over 16000 detailed cardiac simulations, which was then used to train neural network (NN) architectures designed to exploit both spatial and temporal correlations in the ECG signal. Analysis over a validation set showed average errors in activation map reconstruction below 1.7 msec over 75 intracardiac locations. Furthermore, phenotypical patterns of activation and the morphology of the activation potential were correctly reconstructed. The approach offers opportunities to stratify patients non-invasively, both retrospectively and prospectively, using metrics otherwise only available through invasive clinical procedures.;Intracardiac Electrical Imaging Using the 12-Lead ECG: A Machine Learning Approach Using Synthetic Data;health related;Not health related;1
"E. C. Reinisch; J. Theiler; A. Ziemann";2020;We apply anomalous change detection (ACD) to synthetic aperture radar (SAR) data to detect forest thinning at the Valles Caldera in New Mexico. By applying ACD across dimensions other than temporal, we establish baselines for change detection. Application of ACD across different polarizations highlights anomalous relationships associated with different types of scattering mechanisms. We also introduce a metric for distinguishing between anomalies consistently present in data over time and more subtle changes which may be obscured by these anomalies. This is especially useful for analyzing SAR backscatter intensity, which can be dominated by the presence of topographic features that are not of interest.;Identifying forest thinning using anomalous change detection on synthetic aperture radar data;Not health related;Not health related;0
"L. Yan; C. Long; X. Weiqi";2019;In order to compare and analyze data from various sources and to enhance data processing efficiency, this paper provides an approach to manage and compare data with multi storage types. To start, the data storages types are categorized into 2 types, which are simple listed text file and complex formation file. Through three retrieving route-database, text file and configuration file-data can be listed into a comparing form. By selecting, order adjusting and figure setting, the data in the listed form can be rearranged. The comparison setting information can be saved as configuration file for fast reloading and browsing. After being preprocessed, the data can be shown into curves for visualized comparison, and the result can be output as file or bitmap. This method has been applied for a certain rocket's data analysis and has met the demand of multi-typed information management.;Synthetic Comparison and Management for Data with Multi Storage Types;Not health related;Not health related;0
"C. Fahnemann; N. Rother; H. Blume";2022;We present an open-source, modular simulator for Synthetic Aperture Radar (SAR) scenarios using Frequency Modulated Continuous Wave (FMCW) sensor systems. It provides a tangible way of understanding the influence of various system parameters on the overall process of SAR image formation. The computationally efficient phase-based FMCW data generator is backed by verification against a real laboratory sensor setup and produces realistic raw sensor data which can be exported for the verification of SAR processor systems.;Interactive synthetic aperture radar simulator generating and visualizing realistic FMCW data;Not health related;Not health related;0
"T. Rogers; P. Gerstoft";2023;We perform inversions of low-altitude refractivity from phased-array observations of the electromagnetic (EM) field using empiric sampling. Populations of samples are used to represent the probability of observation for a given environmental state, with all states equally likely. A single phased-array observation is assumed. The posterior probability of an environmental state is based on the number of its members within the neighborhood of the observation relative to the total overall states that occur within the neighborhood. The results show the dependence of the posterior probability densities on both the environmental state itself and the state of sensing as signal-to-noise ratio.;Empiric Bayesian Inversion of Evaporation Ducts From Synthetic Phased-Array Data;Not health related;Not health related;0
"I. D. Cameron; D. Miller; I. H. Woodhouse";2007;We introduce an iterative maximum aposteriori probability (MAP) method for combining meteorological model output with synthetic aperture radar for offshore wind field estimation. The MAP approach is demonstrated for 40 ENVISAT ASAR scenes collected for 2004-2006 over the UK Irish Sea. The CMOD4 and CMOD5 geophysical model functions are compared and retrievals using MAP and a simpler directon based windspeed algorithm are validated against insitu mast observations. In particular the CMOD5 MAP algorithm shows promising results showing a RMSE of 2.09ms-1.;Offshore wind mapping using synthetic aperture radar and meteorological model data;Not health related;Not health related;0
"A. Sha; K. M. Mithra; S. J. Abit Sai; M. Samhitha; P. S. Harshitha; R. V R";2023;This research study proposes an innovative approach to tackle the challenges of limited data availability and class imbalance in stroke lesion classification. We adopt a two-step methodology that combines unsupervised clustering with the Expectation-Maximization (EM) algorithm to obtain class labels and a Conditional Generative Adversarial Network (cGAN) to generate synthetic stroke lesion data. The EM algorithm aids in clustering the stroke lesion dataset into informative classes without the need for manual annotations, providing valuable class labels for subsequent data synthesis. Further, this study introduces a cGAN, incorporating these class labels, to generate diverse and realistic synthetic stroke lesion samples. The augmented dataset, comprising both real and synthetic data, is utilized to train a deep learning classifier for stroke lesion classification. The main objective of this research work is to enhance the classifier's performance by leveraging the benefits of both unsupervised clustering and cGAN-based data synthesis. Preliminary experiments demonstrate promising results, suggesting that the proposed methodology has the potential to improve stroke lesion classification accuracy and generalization. The proposed approach contributes to overcome the data scarcity challenges, making efforts towards achieving more reliable and effective stroke diagnosis and patient care.;Enhancing Stroke Lesion Classification with Conditional GAN: Leveraging Unsupervised Clustering and Class-Conditioned Synthetic Data;Not health related;Not health related;0
"R. E. Carande; M. Marra; D. Cronin; P. Nagy";1998;Two-antenna interferometric SAR instruments acquire SAR data in such a manner that the signals may be combined and processed to extract the elevation of each pixel. In addition to providing a high resolution topographic map of the area, this allows for geometric rectification and automatic map projection of the SAR image. The interferometric coherence may be used to assist in land-use classification which can be further exploited for assisting in automated feature detection and extraction. This paper describes and demonstrates algorithms suitable for automatic generation of map products from interferometric SAR data. The software automatically extracts map elements including land-use polygons, transportation networks, and other features such as buildings, power and communication distribution networks. In addition, estimation of bald earth topography is possible by removing the elevation of features which lie on the surface such as buildings and trees. We present brief algorithmic descriptions and initial results of software designed to rapidly and automatically produce map elements and output map products from input IFSAR data.;Automated mapping and feature extraction using high resolution interferometric synthetic aperture radar data;Not health related;Not health related;0
"R. Zulkashev; M. Polyak";2023;In this paper an approach to building a dataset for the diarization problem is considered. A pipeline is proposed for fine-tuning a pretrained diarization model on synthetic data in order to increase diarization accuracy for Russian language. The suggested algorithm for processing audio data allows to add random portions of silence between adjacent audio segments, make several segments partially overlap and add specific type of noise to the audio track. Diarization error rate metric is calculated on the generated audio files by utilizaing Python’s pyannote diarization model. The results show that the artificially generated synthetic audio data is adequate and closely resembles real audio tracks. The proposed method involves speech extraction, concatenation, normalization, audio track formation, and noise overlaying. Our experiments have shown that generating longer and more diverse conversations leads to better quality data.;Synthetic Audio Data Generation Algorithm for the Diarization Problem;Not health related;Not health related;0
"P. Li; C. Yaras; T. Sarwar; P. -C. Ku; Q. Qu";2023;A deep learning prototype for a reconstructive spectrometer was developed using an experimentally determined spectrometer response and synthetic data generated from the response matrix. Benchmarking with synthetic and experimental data was performed.;Accelerating Deep Learning in Reconstructive Spectroscopy Using Synthetic Data;Not health related;Not health related;0
"C. Yin; D. Ran; X. Jia";2016;An innovative real data aided frequency hopping inverse synthetic aperture radar (FH-ISAR) simulator is presented in this paper. Real data obtained by conventional ISAR is used as the input dataset, and a series of virtual FH-ISAR dataset, which is simulated, or reproduced based on the input dataset, are obtained given parameters as pulse numbers, frequency hopping bandwidth, frequency hopping code, etc. This simulator can also complete imaging procession and series of clear images of non-cooperative targets which has the inherent precise complexities of real maneuvering target's movement, can be obtained. It provides a necessary and cool toolkit for FH-ISAR investigations.;Real Data Aided Imaging Simulation for Frequency Hopping Inverse Synthetic Aperture Radar;Not health related;Not health related;0
"J. Schmidt; P. Basili; B. Sang; R. Förstner";2022;This study aims to demonstrate the capability of Machine Learning algorithms to retrieve enhanced (on top of atmospheric background) methane concentrations from space-borne hyperspectral images. As most ML algorithms rely on existing data, they face both suitable data availability issues and methane retrieval biases caused by residual noise and/or geometric distortions. These limitations frequently cause insufficiently trained algorithms in terms of generalization and can lead to a failure in the training process. Hence, we developed an image simulator, the Remote Sensing Image Simulation Environment (RISE), which produces user-defined simulated ground-truth reference images for different scenes and sensors. Consequently, RISE can be used to create training datasets for ML algorithms across a wide range of applications.. With RISE, realistic methane plume datasets occurring from point source leakages can be created and processed, retrieving intermediate products such as top-of-atmosphere (TOA) radiances, level 1 (L1) images as well as enhanced methane concentration maps. Enhanced methane concentration maps paired with L1 spectra were used to train various ML regression models. Preliminary results prove the capability of ML algorithms to quantify enhanced methane concentrations.;Deep Learning Methane Retrievals Based On Synthetic Data;Not health related;Not health related;0
"M. Becquaert; E. Cristofani; M. Vandewal";2013;Compressive Sensing (CS) has proven its effectiveness for many applications by reducing the sampling rate and decrementing the acquistion time. This study evaluates if, how and under which conditions, CS can be applied on Frequency Modulated ContinuousWave (FMCW) Synthetic Aperture Radar (SAR) data for scenes containing only a limited number of scatterers. Two different approaches for the reconstruction are proposed and are evaluated on real data: (1) the CS reconstruction of the subsampled FMCW signal itself and (2) the CS reconstruction of the scattering coefficients of the scene. The limits of performance of CS are shown through simulated SAR data. The influence of the sparsity and the choice of a sampling scheme are evaluated in function of the number of samples needed for the exact reconstruction of the scene.;On the applicability of compressive sensing on FMCW synthetic aperture radar data for sparse scene recovery;Not health related;Not health related;0
N. S. Arini;2002;Classification is one of the most important image analysis tasks as it provides image data with labels that transform it into information about the real world. In this research, classification of clutter in single and multichannel very high-resolution airborne SAR imagery is achieved by extending several classification methodologies to overcome limitations which exist when applied to SAR data. This is achieved by first pre-processing the data using segmentation techniques, then classifying the resulting regions using a number of features calculated over the regions. By attempting to classify regions as opposed to single pixels we can increase the dimensionality of the feature space even for single channel data, and overcome the problems of speckle and texture inherent in SAR imagery. Both supervised and unsupervised classification methodologies have been adapted to feature-based region classification. The methodology has been extended to multichannel data, specifically polarimetric data. Examples of the application of the proposed method are given for several very high-resolution data sets from the QinetiQ airborne SAR. It is concluded that the proposed approach provides a successful and flexible, sensor independent solution to the problem of classification of very high-resolution SAR data for many different applications.;Post-segmentation feature-based classification of synthetic aperture radar data;Not health related;Not health related;0
"Y. -K. Lee; J. -S. Won";2011;Main goal of this study is to characterize the seasonal variations of radar backscattering according to halophyte species using multi-temporal TerraSAR-X satellite data over Ganghwa tidal flat, then suggest optimum season to define halophyte species. We used SAR backscattering signals from cities to confirm the result of radiometric calibration. Seasonally averaged backscattering coefficients of classes were analyzed by tidal condition. The student t-test was carried out to quantify the separability among different halophyte species (P. australis and S. japonica) and exposed tidal flat based on the averaged backscattering coefficient. Based on t-test, images acquired on late summer in flood condition are suggested to examine the maximized expansion of P. australis and S. japonica. We also investigated the relationship between radar signals and soil adjusted vegetation index (SAVI) to minimize the effect of background tidal flat conditions.;Using X-band synthetic aperture radar data to monitor salt marsh;Not health related;Not health related;0
"T. Kubota; N. Yoshida; S. Urita; T. Iguchi; S. Seto; J. Awaka; H. Hanado; S. Kida; R. Oki";2013;The JAXA-NASA Joint Algorithm Team has developed the Level 2 (L2) algorithm for Dual-frequency Precipitation Radar (DPR) onboard the Global Precipitation Measurement (GPM) core observatory, which provides estimated precipitation rate, radar reflectivity factor, and precipitation information. The synthetic DPR Level 1 (L1) data is necessary as a test bed of the DPR L2 algorithms. In this study, synthetic DPR L1 data estimated from the TRMM/PR data are produced in 7 orbits during 15th March 2007 and 32 orbits during 31st Mar. to 1st Apr. 2011. The at-launch codes of L2 Ku, L2 Ka, L2 DPR algorithms (Version 4.20130328) were applied to the L1B synthetic data. The Ku/Ka/DPR products are compared with the PR products. Precipitation rates at 2km altitudes of Ku-L2 estimates are similar to those of the PR 2A25 product. Correlation coefficients are 0.93 and 0.81 over ocean and land, respectively. On the other hand, underestimation in Ka-L2 estimates and overestimation in DPR-L2 estimates are found in precipitation rate more than 4mm/hr over ocean.;Development of synthetic GPM/DPR data from TRMM/PR and evaluation of GPM/DPR level-2 “at-launch” algorithms using them;Not health related;Not health related;0
"J. Adams; J. Sutor; A. Dodd; E. Murphy";2021;A method to produce synthetic visual data was developed for this study. The resulting synthetic imagery was used to train a YOLOv3 classification model to identify various species of sea turtles. The Blender library and Python were used to render and augment the images. Nine datasets were generated, each with different ratios of synthetic and authentic images. YOLOv3 was then trained and the performance resulting from each of the datasets was evaluated to measure which dataset produced the most confident model. The researchers optimized the classification model and demonstrated significant accuracy improvements, especially with models trained without authentic data. A dataset consisting of fully synthetic dataset performed best in testing, with an average confidence level of 9.68 percent greater than the fully authentic dataset.;Evaluating the Performance of Synthetic Visual Data for Real-Time Object Detection;Not health related;Not health related;0
"Y. Burad; K. Burad";2021;Modern deep learning applications require huge data to deliver required accuracy. In most cases accumulating the required amount of data of sufficient quality is often very difficult and costly. We propose a GAN based solution to this problem. CycleGAN is a method of unpaired image -to -image translation which transforms the input image to target domain while preserving all the essential features of the input image. We can leverage CycleGAN to generate synthetic data of required quality by choosing the appropriate target domain.;Leveraging Unpaired Image to Image Translation for Generating High Quality Synthetic Data;Not health related;Not health related;0
"M. R. Nowak; Y. Choe";2018;We introduce a novel method to generate biologically grounded synthetic cerebrovasculature models in a datadriven fashion. First, the centerlines of vascular filaments embedded in an acquired imaging volume are obtained by a segmentation algorithm. That imaging volume is reconstructed from a graph encoding of the centerline (i.e., generating the model's ground truth) and the segmentation algorithm is applied to the resultant volume. As the location and characteristics of the vasculature embedded in this volume are known, the accuracy of the segmentation algorithm can be assessed. Moreover, because the synthetic volume was reconstructed directly from biological data, an assessment is made on embedded filaments that are representative of the topological and geometrical characteristics of the dataset. We believe that such models will provide the means necessary for the enhanced evaluation of vascular segmentation algorithms.;Data-Driven Synthetic Cerebrovascular Models For Validation Of Segmentation Algorithms;Not health related;Not health related;0
"M. Arifeen; A. Petrovski";2022;Tabular synthetic data generating models based on Generative Adversarial Network (GAN) show significant contributions to enhancing the performance of deep learning models by providing a sufficient amount of training data. However, the existing GAN-based models cannot preserve the feature correlations in synthetic data during the data synthesis process. Therefore, the synthetic data become unrealistic and creates a problem for certain applications like correlation-based feature weighting. In this short theoretical paper, we showed a promising approach based on the topology of datasets to preserve correlation in synthetic data. We formulated our hypothesis for preserving correlation in synthetic data and used persistent homology to show that the topological spaces of the original and synthetic data have dissimilarity in topological features, especially in 0th and 1st Homology groups. Finally, we concluded that minimizing the difference in topological features can make the synthetic data space locally homeomorphic to the original data space, and the synthetic data may preserve the feature correlation under homeomorphism conditions.;Topology for Preserving Feature Correlation in Tabular Synthetic Data;Not health related;Not health related;0
"E. Kobayashi; A. Kosuge; M. Hamada; T. Kuroda";2023;An occlusion-resilient mmWave imaging radar-based object recognition system for advanced driver-assistance systems (ADAS) of construction machinery application is developed. As ADAS for construction sites, millimeter wave application is required in poor visibility environments such as nighttime, bad weather, and muddy conditions where object recognition by RGB cameras and LiDAR is difficult. A remaining technical challenge for ADAS is occlusion. Two techniques are proposed to improve the accuracy in occlusion scenes. First is a technique which generates simulated training data for occlusion environment to improve accuracy while reducing the cost for the training data preparation. The second is a parallel inference DNN architecture which enables object recognition with high accuracy in both normal and occlusion scenes by running two DNNs optimized respectively for normal and occlusion scenes in parallel. The object recognition accuracy of mAP50 in occlusion scenes improves by 15 points compared to the conventional technique. The decrease in recognition accuracy in non-occlusion scenes is only 4 points.;An Occlusion-Resilient mmWave Imaging Radar-Based Object Recognition System Using Synthetic Training Data Generation Technique;Not health related;Not health related;0
"A. Donald; R. Duggal; J. Welter; D. Springmann; T. Schoemehl";2008;Efficient lightweight microwave power modules produce high RF output power in a compact package. Microwave power modules (MPM) are the first choice for rugged applications where reliability, efficiency and high power to weight ratio equipment is needed to perform in advanced applications such as high data rate communication and mobile synthetic aperture radars. L-3 Communications Electron Devices has developed a high performance low noise microwave power module (MPM) with CW output power of 200 watts in Ku band.;A compact efficient 200W Ku-band MPM for data links and synthetic aperture radar;Not health related;Not health related;0
"A. Budillon; V. Pascazio; D. Pisa; G. Schirinzi";2005;In this paper a new approach to synthetic aperture radar (SAR) data processing, is presented. The method properly takes into account the spatial truncation of the data in the azimuth direction, due to the finite recording frame. It allows an enlargement of the well focused area, assuring lower reconstruction error, respect to conventional processing techniques. The good performance of the method is demonstrated through reconstructions from simulated data, putting emphasis on the well focused signals.;Synthetic aperture radar imaging from truncated data;Not health related;Not health related;0
"Y. Wu; Y. Yuan; Q. Wang";2022;"Crowd understanding has widespread applications, including video surveillance, crowd monitoring. Unlike existing coarse-grained crowd understanding methods(e.g., counting people in images), crowd instance segmentation can provide more precise results (pixel-wise segmentation for each person in images). However, crowd instance segmentation demands a considerable amount of pixel-wise labeled data, which is very time-consuming and challenging to annotate accurate human instance masks in the crowd scene. In this paper, we propose a data generator and labeler to automatically generate synthetic crowd instance segmentation data. Then based on it, we build a large-scale synthetic crowd instance segmentation dataset called ""GCIS Dataset"". Besides, we demonstrate two approaches that utilize the synthetic GCIS dataset to advance the performance of crowd instance segmentation: 1)supervised crowd instance segmentation: pretrain crowd instance segmentation models on GCIS dataset, then finetune on other real data. It can remarkably boost the model’s real-world performance; 2) crowd instance segmentation via domain adaption: transfer the synthetic GCIS dataset to photo-realistic images, then train the model together with transformed data and real data, which shows better performance when tested on real-world data. Extensive experiments show the validity of the synthetic GCIS dataset for crowd instance segmentation. The dataset and source code will be released online.";Learning From Synthetic Data for Crowd Instance Segmentation in the Wild;Not health related;Not health related;0
"C. -b. Yin; D. Ran";2016;Innovative frequency hopping inverse synthetic aperture radar (FH-ISAR) imaging simulator was presented in this paper aided with the real collected inverse synthetic aperture radar (ISAR) data with conventional constant carrier frequency. These real data obtained by conventional ISAR was used as the input dataset to start the FH-ISAR Imaging simulation, and a series of virtual FH-ISAR dataset, which was simulated, or reproduced based on the input real dataset, were obtained given the parameters such as pulse numbers, frequency hopping bandwidth, frequency hopping code, etc. This simulator can also complete the imaging processing and series of clear images of non-cooperative targets which has the inherent precise complexities of real maneuvering target's movement, can be obtained. It provides a necessary and cool toolkit for FH-ISAR investigations.;Frequency hopping inverse synthetic aperture radar imaging simulation aided with real radar data;Not health related;Not health related;0
"J. J. Yackel; D. G. Barber";1998;The study of atmosphere-sea ice-ocean processes using synthetic aperture radar (SAR) endeavors to improve estimates of energy flows and climate state variables from seasonally dynamic Arctic sea ice. This paper works toward this goal by characterizing the seasonal coevolution of the thermodynamic, electrical, brine volume, mechanical and microwave scattering characteristics of a snow covered landfast first-year (FY) sea ice volume for the purpose of investigating the theoretical framework which links them. The feasibility for using the time series evolution of the microwave scattering coefficient (/spl sigma//sup 0/) from SAR as a proxy indicator of the thermodynamic nature of FY sea ice over this seasonal transition is then assessed. Results indicate that SAR may hold utility in explaining microwave scattering over smooth FY sea ice types. FY sea ice surface temperature was found to explain between 69 and 84% of the seasonal variation in microwave backscatter from ERS-1 SAR during 3 consecutive winter to summer transitions (1993, 1994, 1995) at field locations near Resolute Bay, NWT. A discussion of the seasonal coevolution amongst the aforementioned components highlights the 'temperature-brine' relationship as instrumental in the development of a 'thermodynamic-scattering' link for landfast FY sea ice.;Measuring the thermodynamic state of sea ice using synthetic aperture radar (SAR) time series data;Not health related;Not health related;0
"H. -M. Jeon; L. H. Pham; D. N. -N. Tran; H. -H. Nguyen; J. W. Jeon";2022;In this paper, we propose generating training and testing images for autonomous driving using the CARLA simulator. Several experiments have been performed to prove the effectiveness of training on synthetic data, including those on mixed datasets including both data generated from CARLA and real-world data. The result of such experiments showed that synthetic data generated using the CARLA simulator was effective as training and testing data for on-road object detection.;3D Synthetic Image Training and Testing Data for Autonomous Driving;Not health related;Not health related;0
"P. Kulbhushan; J. Chahar; I. Kar";2023;The purpose of this research is to demonstrate that using the generated synthetic dataset will produce better results than the original teeth x-rays and thus be more efficient for carrying out hierarchical multiclass classification, this will also allow dental caries classification to be automated in accordance with the GV Black Standards. Five distinct models that were trained and tested using both the new and original datasets are compared. As the first step in each of these tasks, simple object detection will be used to identify each tooth, and then hierarchical classification will be used to achieve the desired results for classifying the cavities so that appropriate treatment can be determined based on the class of caries.;GV Black Inspired Hierarchical Multiclass Classification using Panoramic Radiographic Synthetic Data;Not health related;Not health related;0
"P. Areerob; C. Khongprasongsiri; V. Prikboonchan; D. Thampithakpong";2023;Nowadays, autonomous vehicle technology plays a more important role in our lives. Many people pay attention to driverless cars, but, in fact, every industry needs technology that increases safety. But the problem is that the data must be diverse. We proposed a data synthesis in which the number of the two sources needed to be a reasonable ratio. In this paper, the ratio between synthetic and real data is observed experimentally with clearly and noise injection data in the Yolact and Detectron2 model. In the benchmark test, the optimal ratio is 3.5 and 2.5 for Yolact and Detectron2, respectively. Moreover, provided that training data with mixing data method provide better accuracy than one data type training method.;How many appropriate added synthetic data to improve semantic segmentation?;Not health related;Not health related;0
"M. L. Pugh; A. M. Waxman; M. J. Duggin; J. M. Hassett";2004;Although the processing of electro-optical imagery from Earth observation satellites has been effectively used for classification of many types of land cover, forest classification has been generally limited to broad categories such as deciduous or coniferous. Recent studies suggest that the combination of imagery from satellites with different spectral, spatial, and temporal information may improve classification performance. This paper discusses the results of new fusion research aimed at extracting additional information from the combination of multisensor imagery to improve forest classification performance. For this investigation multiseason LANDSAT and RADARSAT imagery was combined using a new biologically-based opponent-color image fusion and data mining technique, in conjunction with visual texture enhancement, and the Fuzzy ARTMAP neural classifier [A. M. Waxman et al. (2002)]. This approach is shown to quickly learn individual forest classes from a small number of training examples and enable added-value assessment of different sensor modalities.;Neural image fusion of remotely sensed electro-optical and synthetic aperture radar data for forest classification;Not health related;Not health related;0
"R. Richbourg; T. Stone";1997;"Very high resolution digital elevation models (DEM) are becoming more commonplace as source data to support the construction of terrain representations for use by simulations. Often however, the simulation systems are not capable of processing the large amounts of data found in the original elevation models. This is particularly true in the case of virtual simulations that, include a visualization component. Such systems must often dramatically reduce the data to meet processing limitations of graphics components responsible for rendering real-time, ""out of the window"" views of the environment for presentation to the simulation users who are to be immersed in a virtual world. Typically, the data reduction process relies on transformation of the dense, regular lattice of DEM elevation points into a network of irregularly sized, shaped, and oriented polygons. Point selection strategies are critical to this transformation. They define a small set of the original data points, according to a pre-specified polygon budget, that will represent the entire area described by the source DEM and thus determine the fidelity of the resulting polygonal representation. This paper describes a novel and developing point selection strategy and provides a brief comparison with a more traditional approach.";Developing a point selection strategy for elevation data modeling in synthetic environments;Not health related;Not health related;0
"J. Dümmel; X. Gao";2021;Traditional object detection methods require large amounts of training data. Eliminating the need to generate this data saves time and manual or computational effort. In the field of object re-identification (Re-ID) the similarity of objects is defined by comparing query and target images. Given a query image from an object, these algorithms are able to retrieve the object in a target image, even if the class of the object is not known to the network. In industrial environments, CAD data usually exists for mechanical components. We present a system that uses this CAD data to generate query images to retrieve mechanical components in real images. We train our system exclusively with synthetic training data and evaluate the reality gap when testing real target images. In the evaluation we show, that our system is also able to retrieve unknown classes it has not been trained with. Finally, we identify improvements of our system and address potential research directions of object Re-ID in the future.;Object Re-Identification with Synthetic Training Data in Industrial Environments;Not health related;Not health related;0
"F. Ranja; E. B. Nababan; A. Candra";2023;One of the particular concerns of the Bank for ATM transaction services is the availability of cash at ATMs. Prediction of the availability of ATM money is required by the Bank to manage funds optimally. This study aims to analyze and predict cash availability at ATM machines using Time-GAN and Extreme Gradient Boosting (XGBoost). Time-Series data is highly dependent on the size and consistency of the dataset used in the training. The features available in the dataset are limited and have constraints such as missing dimensions or missing values. Therefore, synthetic data generation technique is used as an effective way to increase the amount of data and handle imbalanced data. Synthetic data generation has been shown to increase the generalizability of models with Time-Series data. The generated data will be divided into Training data, Validation data, and Testing data, resulting in a Load Model that will be analyzed using the XGBoost method. The ultimate goal of this research is to provide a summary of the evaluation and performance that results in better ATM availability for future research. Model performance is evaluated with the Mean Absolute Error (MAE) metric 2.57 value, Mean Squared Error (MSE) 1.64 value, and R-squared 5.02 value.;Synthetic Data Generation Using Time-Generative Adversarial Network (Time-GAN) to Predict Cash ATM;Not health related;Not health related;0
"R. Myung; S. Choi; W. Choi; H. Yu; D. Lee; E. Lee";2016;The advent of IoT caused vigorous development of multiple services in diverse industry areas. Especially, smart home services have high accessibility than other services since they are based on everyday home appliances, so they gained special attention of application developers. Those services generate a large volume of data: it not only helps service providers to provide services more efficiently, but also helps them to guarantee Service Level Agreement (SLA) of those services. In this paper, we apply time dependency and usage continuity of daily appliances, and correlation between different services to model behavior patterns of service users and generate reliable synthetic data based on their real log data.;Elaborate Synthetic Data Generation for Internet of Things Services at Smart Home Environment;Not health related;Not health related;0
"K. Srinivas; S. S. Gedam; A. B. Inamdar";2018;In the analysis of Inherent Optical Properties (IOPs which are a - total absorption coefficient and bb - the total backscattering coefficient) of water bodies, the parameters generally retrieved using semi-analytical model for inversion of hyperspectral remotely sensed reflectance from shallow waters are mainly based on five variables related to absorption due to phytoplankton, gelbstoff and detritus (combined), backscattering due to particulate matter, bottom reflectance and geometric depth. Optimization techniques, both local and global are used in inversion for retrieval of optimized values of aforementioned parameters. In semi-analytical models, two empirically fitted constants, g0 and g1 are often used to relate the subsurface remote sensing reflectance (rrs) to IOP. These two constants vary with particle phase function for a given remote sensing setting and hence need to be predetermined. In the present study, a local optimization technique for inversion is proposed, considering g0 and g1 as variables and its effect on improvement of accuracy of IOP retrieval is reported. A synthetic dataset with 3000 hyper spectra in 400-720 nanometre range with 5nm bandwidth and corresponding to sandy bottom type with three levels of depth with 20 concentrations of Chlorophyll - a is created to study the effect. We suggest that, processing of Hyperspectral Imagery from shallow waters can include these additional variables for better accuracies for retrieval of IOPs.;Effect of inclusion of new variables in inversion of semi – analytical model for synthetic hyperspectral data of shallow waters;Not health related;Not health related;0
"T. Bollian; M. Younis; G. Krieger";2019;Radar remote sensing instruments have played an increasingly important role in understanding Earth and its dynamic processes. Their operational frequencies are predetermined based on the physical properties that are to be retrieved. Simultaneously, the use of their allocated frequency bands in P- to C-band by other services has increased. A reliable mitigation of Radio Frequency Interference (RFI) is therefore critical for future Earth observation missions.New space-borne multi-channel Synthetic Aperture Radar (SAR) instruments are capable of Digital Beamforming (DBF). Each receive channel is recorded individually to form the digital antenna pattern after data acquisition. This opens up new opportunities for spatial RFI filtering. Unfortunately, this approach requires significant processing power on board the satellite as it is unfeasible to downlink the increased data volume. This paper presents a new method based on auxiliary beams that relaxes the requirement for the on-board processing power and only affects the downlinked data volume minimally. The information measured with the auxiliary beam can be used for an on-ground RFI correction.;Digital Beamforming Based RFI Extraction for an On-Ground Correction of Synthetic Aperture Radar Data;Not health related;Not health related;0
"K. S. Mayer; L. C. M. Dos Santos; R. P. Pinto; M. P. A. Dal Maso; C. E. Rothenberg; D. S. Arantes; D. A. A. Mello";2023;We propose a DNN-based QoT estimation technique that operates on a network-wide scale. The DNN training data is composed of connection paths, frequency slots, and OSNR transponder telemetry, collected from both synthetic and physical connections. Simulation results indicate effective and low-complexity QoT estimation.;DNN-based QoT Estimation Using Topological Inputs and Training with Synthetic-Physical Data;Not health related;Not health related;0
"P. J. Mayuga; S. F. Tiongson; J. J. Abao; J. M. Mendoza; G. A. Pernez; R. Ramirez";2022;The use of satellite-based remote sensing data to investigate ground surface conditions has recently been growing. Specifically, synthetic aperture radar (SAR) data have successfully been utilized to detect ground surface deformations and damaged area mapping. This study uses Sentinel-1 SAR data to map the 2018 Naga landslide area in Cebu Island in the Philippines. The intensity and coherence difference analyses were conducted to detect surface changes resulting from the landslide. The amplitude- and coherence-based results coincide with the actual landslide area and successfully map ground surface mass movement using pre- and post-failure SAR images. This paper presents the unique advantages and potential of Sentinel-1 SAR data for landslide area mapping in large-scale open-pit mining sites for rapid disaster impact assessment.;Landslide Area Mapping Using Synthetic Aperture Radar (SAR) Data: the Case of The 2018 Naga Landslide;Not health related;Not health related;0
"Y. Jiang; C. Gao; L. Ji; Y. Wu";2019;In order to solve the problem of sparse training samples in logo recognition task, a multi-type context-based logo data synthesis algorithm is proposed. The algorithm comprehensively utilizes the local and full context of the logo object and the scene image to guide the synthesis of the logo image. The experimental results on the FlickrLogos-32 show that the proposed algorithm can greatly improve the performance of the logo recognition algorithm without relying on additional manual annotation, verify the validity of the synthesis algorithm, and further prove that multi-type context can improve the performance of the object recognition algorithm.;Context-Based Synthetic Data for Logo Recognition;Not health related;Not health related;0
G. Jimenez;2009;In order to gain efficiencies throughout the fleet training cycle and conserve fuel, the Navy is in the process of increasing its use of synthetic training technologies. Maximum training value and increased readiness result from the most realistic training possible, which must include a robust and realistic physical environment. Commander, Naval Meteorology and Oceanography Command (CNMOC) will be providing meteorological and oceanographic data and expertise to enhance Fleet Synthetic Training (FST) events. This paper describes the issues associated with inserting operational data into modeling and simulation training events.;Oceanographic data issues for support of Navy Fleet Synthetic Training;Not health related;Not health related;0
C. Wackerman;1993;"During SAXON-FPN spotlight mode Synthetic Aperture Radar (SAR) data was collected of ocean surface waves under different environmental conditions and imaging geometries. From this data, estimates can be derived for the values of three ocean surface parameters of importance to SAR imaging: (1) the range of radial velocities within a SAR resolution cell; (2) the magnitude of the real transfer function; and (3) the phase of the real transfer function. The real transfer function relates surface wave height modulations to modulations in radar cross section. In this paper the author describes a procedure to estimate these parameters from the spotlight data and show results from the SAXON-FPN data. For certain imaging geometries the parameter estimates are consistent with previous in situ measurements.<>";Estimation of synthetic aperture radar modulation transfer function parameters with spotlight mode data;Not health related;Not health related;0
"T. Asaka; T. Nonaka; T. Sugimura; K. Iwashita";2022;Synthetic aperture radar (SAR) imaging is commonly used to monitor civil engineering structures and analyze various phenomena in mountainous areas. Interferometric SAR (InSAR) results need to be corrected using global navigation satellite system (GNSS) based control station data to estimate the displacement of reinforced slopes. In this study, the suitability of the interpolation method for GNSS Earth Observation Network System (GEONET) data was evaluated using field survey results. Interpolation was performed using the Green's function for six different spline methods, and the accuracy was determined by comparison to geographical coordinates from virtual reference station real-time kinematic global navigation satellite system (VRS RTK-GNSS) surveying at three test sites in Chiba Prefecture, Japan. We conclude that Green's function for spline is a useful method of interpolating GEONET elevation data, and that the most accurate spline method was the continuous curvature spline in tension for a spherical surface.;Evaluation of Interpolation Methods for Geonet Data Used in Interferometric Synthetic Aperture Radar;Not health related;Not health related;0
"Y. Zhang; X. Shan; W. Gong; G. Zhang";2021;The challenge of ruling out potential rupture nodal planes with opposite dip orientations during interferometric synthetic aperture radar (InSAR)-based kinematic inversions has been widely reported. Typically, slip on two or more different fault planes can match the surface deformation measurements equally well. The ambiguous choice of the nodal plane for the InSAR-based models was thought to be caused by InSAR's 1-D measurement and polar orbiting direction, leading to its poor sensitivity to north–south crustal motion. Through synthetic experiments and simulations, this paper quantitatively demonstrates the main reason of the ambiguous InSAR-based models, which confuse researchers in the small-to-moderate thrust earthquake cases investigation. We propose the inherent 1-D measurement is not the principle cause of the fault plane ambiguity, since models derived from the same InSAR data predict similar, but not identical, 3-D deformation patterns. They key to differentiating between these different models is to be able to resolve the small asymmetry in the surface deformation pattern, which may be smaller in amplitude than the typical noise levels in InSAR measurements. We investigate the fault geometry resolvability when using InSAR data with different noise levels through ‘R’ value. We find that the resolvability does not only rely on the InSAR noise, but also on the fault geometry itself (i.e. depth, dips angle and strike). Our result shows that it is impossible to uniquely determine the dip orientation of thrust earthquakes with Mw < 6.0 and depth > 5.0 km with InSAR data at a noise level that is typical for mountain belts. This inference is independent from the specific data set (i.e. interferogram or time-series) and allows one to assess if one can expect to be able to resolve the correct fault plane at all.;The ambiguous fault geometry derived from InSAR measurements of buried thrust earthquakes: a synthetic data based study;Not health related;Not health related;0
"E. Rignot; R. Chellappa; R. Kwok";1991;;Classification of Multifrequency Multilook Synthetic Aperture Radar Data;Not health related;Not health related;0
"T. Streckert; D. Fromme; M. Kaupenjohann; J. Thiem";2023;Surgical instrument segmentation is a key step in robot assisted minimally invasive surgery and can enable other applications such as augmented reality. However, accurate instrument segmentation is still challenging due to the complex environment. With the growth of deep learning, the results have improved significantly. However, due to small data sets, the generalization of the trained networks needs to be improved, as the results drop significantly when applied to new, unseen data. To increase the generalization of a CNN for instrument segmentation, a synthetic data set is created for further training. The data set is created with images of surgical instruments in front of a green screen and background images of surgical procedures that do not contain any instruments. In addition to the background images from real procedures, a GAN is trained to generate more background images. The instruments are merged with the background using three different blending functions. A total of 15,000 new images are generated. Two models are trained, model 1 with the EndoVis17 data set and model 2 with the EndoVis17 and the synthetic data set. The models are based on the SegNet architecture with pretrained parameters and differ only in the training data used. Both models achieve a mIoU above 90% on the EndoVis17 test data. Additionally, both models are tested on the EndoVis15 data set, which provides a new environment compared to the training data. model 1 achieves a mIoU of 56.75% and model 2 of 76.15%. The additional training with the synthetic data set has improved the generalization of the CNN by about 20 percentage points on the new unseen data.;Using Synthetic Data to Increase the Generalization of a CNN for Surgical Instrument Segmentation;health related;Not health related;1
"Z. Zhang; W. Xu; P. Huang; W. Tan; Y. Qi";2021;The squinted sliding spotlight synthetic aperture radar (SAR) based on the fixed pulse repetition frequency (PRF) and continuously varying PRF will lead to the problems of blind ranges and azimuth non-uniform sampling. Fortunately, the block varying PRF can avoid the above two problems in the whole acquisition interval simultaneously, but it will cause the azimuth sampling intervals of the data blocks corresponding to different PRFs to be different. In addition, the squint angle will introduce a new Doppler spectrum aliasing phenomenon. This makes the conventional two-step preprocessing algorithm infeasible. To resolve these problems, a preprocessing approach of squinted sliding spotlight mode based on block varying PRF is proposed. In this method, data blocks corresponding to different PRFs are first subjected to the range frequency-dependent de-twist and de-ramping to resolve Doppler spectrum aliasing caused by the squint angle and azimuth beam steering. Afterward, different numbers of azimuth zero-padding are introduced into the obtained raw data to make the azimuth sampling intervals of the output data blocks corresponding to different PRFs consistent. Furthermore, the signal after azimuth combination should be resampled to make azimuth sampling interval at different range frequency equal. Finally, the feasibility of proposed preprocessing method is verified by simulation experiments on point targets.;Azimuth Preprocessing of Squinted Sliding Spotlight Synthetic Aperture Radar Data with Block Varying PRF;Not health related;Not health related;0
"M. Sugimoto; N. Shiroto; K. Ouchi";2011;A new technique for estimating wave height of range-traveling ocean waves by using the polarization ratio of synthetic aperture radar (SAR) data is proposed. Unlike the previous methods based on scattering models such as composite surface model [1], the new method does not require any scattering models that strongly affect the estimation accuracy. First, the dependence of the polarization ratio on local incidence angles and its application to estimating wave height are described. Next, the estimation method is applied to the data acquired by Phased Array L-band SAR (PAL SAR) on board of Advanced Land Observing Satellite (ALOS). Then, the results are analyzed and discussed.;Estimation of ocean wave height using polarization ratio of synthetic aperture radar data;Not health related;Not health related;0
"A. Poibrenski; J. Sprenger; C. Müller";2018;In order to make highly/fully automated driving safe, synthetic training and validation data will be required, because critical road situations are too divers and too rare. A few studies on using synthetic data have been published, reporting a general increase in accuracy. In this paper, we propose a novel method to gain more in-depth insights in the quality, performance, and influence of synthetic data during training phase in a bounded setting. We demonstrate this method for the example of pedestrian detection in a frame-by-frame semantic segmentation class.;Toward a Methodology for Training with Synthetic Data on the Example of Pedestrian Detection in a Frame-by-Frame Semantic Segmentation Task;Not health related;Not health related;0
"A. Swami; T. V";2023;Recommender systems are a great academic and industrial success in e-commerce and other areas. However, access to real-world historical interaction data for evaluating the recommender systems remains a challenge due to privacy, cost, and multiple issues. The state of the art techniques for generating synthetic data that is closest in relation to real world data, needs atleast a sample of real world data, which is also a problem especially when it comes to bundled data. We propose a novel way of generating multi-label synthetic data specifically for Bundle Recommender system problems, that does not need a sample of real world data but still is closest in relation to real world data. The proposed approach is hybrid as it uses a combination of process based and data based synthetic data generation methods. We demonstrate the feasibility of this approach by generating bundled data for Cosmetics product purchases. Also, synthetic data generated by the state-of-the-art method and our hybrid approach were compared, and it was found that the two sets of data are comparable in terms of statistical characteristics. The generated synthetic data can be used for evaluating bundle recommender systems for the industry.;Multi-Label Tabular Synthetic Data Generation for Bundle Recommendation Problem;Not health related;Not health related;0
"L. Wijesinghe; D. Ryu; A. Western; J. Aryal; Y. Zhang";2023;This study evaluate the performance of a Wheat Canopy Scattering Model (WCSM) at L-band, which was initially developed to simulate the backscatter of C-band Synthetic Aperture Radar (SAR), using the L-band UAVSAR data and ground-based measurements of soil moisture, soil surface roughness and crop parameters collected from wheat fields during the SMAPVEX12 field campaign. Results show that WCSM is capable of estimating HH-pol backscatter with an error less than 2.48 dB. On the other hand, relatively large RMSE of 4.38 dB and 4.34 dB were observed for VV and VH backscatter coefficients, respectively. Furthermore, it was observed that model tends to overestimate VV backscatter. It is also observed that co-pol total backscatter from a wheat canopy is sensitive to incidence angle followed by root mean square (RMS) height and soil moisture while cross-pol backscatter showing high sensitivity to wheat crop biophysical parameters.;Simulating the Backscattering of L-Band Synthetic Aperture Radar from a Wheat Field using Smapvex12 Data;Not health related;Not health related;0
"Y. Fan; J. Wan; Y. Yuan; Q. Wang";2023;Due to the domain gap between the public large-scale datasets and actual scenes, the crowd counting models trained on the common datasets have a significant performance degradation when applying in practical applications. To address the above issue, one of the solution is to label additional data from the novel scenes, which is time-consuming and impractical for multiple scenes. Another solution is to utilize domain adaptation approaches to adapt a well-trained model to novel scenes. However, most of these approaches focus on appearance adaptation while the background and the crowd distribution is not adapted. In this paper, we propose a weakly-supervised method with real-synthetic hybrid data which only requires a small portion of unlabelled real images and auto-generated synthetic labelled images for training. First, the hybrid data is generated based on background from the real scene and random distributed synthetic persons. Second, an initialized counter is trained based on the hybrid data and the crowd distribution is predicted based on the predictions on real images. Then, a better crowd counter is trained based on new hybrid data generated from updated crowd distribution. The process is iterated until convergence. Extensive experiments demonstrate the effectiveness of the proposed method.;Weakly-Supervised Scene-Specific Crowd Counting Using Real-Synthetic Hybrid Data;Not health related;Not health related;0
"G. Zahnd; M. Larsson; H. Gao; A. Sérusclat; D. Vray; J. D'Hooge";2012;Ultrasound imaging represents a well designed modality to estimate the motion of biological tissues in vivo, from which relevant clinical information can be assessed. However, the lack of ground truth constitutes a challenging issue when it comes to evaluate the accuracy of computerized methods. Indeed, quantification of the reliability of experimental results often involves manual or visual human operations, which may introduce subjectivity and variability. Nonetheless, numerical simulation of the imaged tissues allow a comparison with a known reference. For this purpose, we propose in this work a realistic kinematic multi-layer model of the common carotid artery. A set of 10 models was generated by randomly positioning scatterers, on which intensity, specular reflection, and bidimensional motion over the duration of one cardiac cycle were applied. Two computerized methods, namely a block-matching method and a segmentation method, were also applied on our model using identical parameter settings as those used for in vivo clinical data, in the objective to assess their accuracy. The tracking errors were 42 ± 40 _m and 12 ± 10 _m in the longitudinal and radial directions, respectively. The segmentation errors were 28±18 _m for the lumen diameter, and 15±10 _m for the intima-media thickness. We conclude from these results that our model can constitute a reliable method to quantify the accuracy of computerized algorithms.;A novel method to generate synthetic ultrasound data of the carotid artery based on in vivo observation as a tool to validate algorithm accuracy;Not health related;Not health related;0
"T. P. Tun; I. Pisica";2023;Generating synthetic electricity consumption data is crucial for developing efficient energy systems in smart cities. In this paper, we propose the use of Tabular Generative Adversarial Networks (Tabular GAN) for generating synthetic data for residential electricity consumption. Tabular GANs have been used in various domains and have shown promising results in generating high-quality synthetic data. The performance of our proposed method was evaluated by comparing the probability density, mean, standard deviation, and variances of the synthetic data with the original data. The results showed that the Tabular GAN method generated synthetic data that closely match the statistical characteristics of the original data and the simulation outcome indicated that the synthetic data generated by Tabular GAN could effectively simulate the patterns and behaviors observed in the original data. Overall, the proposed method demonstrates the effectiveness of using Tabular GANs for generating synthetic electricity consumption data.;Synthetic Electricity Consumption Data Generation Using Tabular Generative Adversarial Networks;Not health related;Not health related;0
"X. Yongxin; P. Hailiang; C. Zongzhi";1995;Synthetic aperture radar (SAR) is portrayed as a multiple access channel. An information theory approach is applied to the SAR imaging system, and the information content about a target that can be extracted from its radar image is evaluated by the average mutual information measure. A conditional (transition) probability density function (PDF) of the SAR imaging system is derived by analyzing the system and a closed form of the information content is found. It is shown that the information content obtained by the SAR imaging system from an independent sample of echoes will decrease and the total information content obtained by the SAR imaging system will increase with an increase in the number of looks. Because the total average mutual information is also used to define a measure of radiometric resolution for radar images, it is shown that the radiometric resolution of a radar image of terrain will be improved by spatial averaging. In addition, the imaging process and the data compression process for SAR are each treated as an independent generalized comunication channel. The effects of data compression upon radiometric resolution for SAR are studied and some conclusions are obtained.;An information theory approach to the data compression and imaging system for synthetic aperture radar (SAR);Not health related;Not health related;0
"N. Nandakumar; J. Eberhardt";2023;The introduction of Industrial Artificial Intelligence led us to adopt a variety of innovative technologies that add value to the product’s life cycle. One such breakthrough is computer vision-based quality inspection of manufactured products. The main challenge with implementing computer vision in industry is that of a shortage of dataset for training the neural networks. The main objective of this study is to provide an overview of the various methods available for synthetic data generation that can be utilized by industries. We emphasize on flexible production systems that produce unique products every time. The performance of an Object Detection model trained on real and synthetic datasets is used to validate one of the approaches.;Overview of Synthetic Data Generation for Computer Vision in Industry;Not health related;Not health related;0
"G. Kumpatla; H. Veresi; B. S. P; S. Abhishek; A. T";2023;Medical analysis is a key component of contemporary healthcare since it helps doctors diagnose patients accurately to plan and track their treatments. Accurate identification of brain tumors is essential for doctors to treat patients swiftly and efficiently. This research investigates a novel approach to overcome the constraints imposed by scarce and sensitive medical data, generating synthetic images of brain tumors using Generative Adversarial Networks (GANs). The proposed method has the potential to enhance activities involving medical processing, which will aid in making diagnoses and formulating treatment plans. The preliminary findings demonstrate its potential relevance to a wider variety of medical processing tasks, demonstrating that this augmentation method yields significant benefits.;Revolutionizing Brain Tumour Prediction: A Pioneering GAN-based Framework for Synthetic Data Generation;health related;Not health related;1
"S. D. Fisher; M. A. Richards; J. H. McClellan";2007;"This paper will present the findings associated with data eclipsing effects in the inverse Polar Format Algorithm (IPFA) for turntable spotlight inverse synthetic aperture radar (ISAR) imaging systems. Typical turntable ISAR systems illuminate a scene of interest with an electromagnetic source in order to obtain samples of the reflectivity function over a given set of aspect angles. These samples form a bandlimited portion of the spatial frequency (kx,ky) response of the scene, from which a focused image may be generated by using a simple 2D inverse fast Fourier transform (IFFT). However, the collected samples lie on a polar raster in (kx,ky) space, which cause defocusing effects in the final image unless interpolation to a rectangular grid of points is first accomplished. The IPFA implements a frequency selective waveform in order to sample the reflectivity function of the scene of interest so that the (kx,ky) samples lie on a rectangular grid of points. This is accomplished by ""designing"" a rectangular grid of (kx,ky) sample points based upon the desired image resolution, pixel spacing, and scene size. Once this is accomplished, a corresponding set of frequency/aspect angle (f,_) pairs can be generated for data collection. However, the (f,_) pairs must first be sorted in order of increasing aspect angle in order to get the correct frequency transmission schedule. This sorting results in a staggered transmission schedule that can result in pulse overlapping (eclipsing) if certain conditions are not met regarding the pulse length and turntable rotation rate. This paper will detail the requirements under which these conditions must be met and will provide simulation results showing the effect of pulse overlap in image formation.";Data Collection Eclipsing Effects in the Inverse Polar Format Algorithm (IPFA) for Turntable Spotlight Inverse Synthetic Aperture Radar (ISAR) Imaging Systems;Not health related;Not health related;0
"P. _rtem; E. _rtem; N. Erdo_mu_";2019;"The following topics are dealt with: biometrics (access control); learning (artificial intelligence); feature extraction; fingerprint identification; face recognition; image classification; image matching; medical image processing; cryptography; neural nets.";Impact of variations in synthetic training data on fingerprint classification;Not health related;Not health related;0
"X. Zheng; B. Wang; D. Kalathil; L. Xie";2022;A two-stage machine learning-based approach for creating synthetic phasor measurement unit (PMU) data is proposed in this article. This approach leverages generative adversarial networks (GAN) in data generation and incorporates neural ordinary differential equation (Neural ODE) to guarantee underlying physical meaning. We utilize this approach to synthetically create massive eventful PMU data, which would otherwise be difficult to obtain from the real world due to the critical energy infrastructure information (CEII) protection. To illustrate the utility of such synthetic data for subsequent data-driven methods, we specifically demonstrate the application of using synthetic PMU data for event classification by scaling up the real data set. The addition of the synthetic PMU data to a small set of real PMU data is shown to have improved the event classification accuracy by 2 to 5 percent.;Generative Adversarial Networks-Based Synthetic PMU Data Creation for Improved Event Classification;Not health related;Not health related;0
"Y. Liu; H. Xu; Z. Tang";2022;As the world develops, medical data privacy is becoming an important issue. Differential privacy can better ensure accuracy and reduce the reflection of personal privacy, while generative adversarial networks can generate data sets. Obviously, the combination of differential privacy and generative adversarial network (GAN) can synthesize sensitive medical data to form new data sets, which not only protects personal privacy but also does not affect medical research. We summarize the current difficulties in medical data, compare the common medical data types with typical DP methods of different GAN, analyze the advantages and disadvantages of each method, and discuss future development. At the same time, we also introduce GAN applied in the image data field. Because this approach is very mature, we only introduce some commonly used GANs, compare their improvements, and introduce their applications in data generation. This may help in the future development of medical data privacy algorithms.;Synthetic data generation for medical by generative adversarial network;health related;health related;1
"A. W. . -C. Heng; Hock Lim; Soo Chin Liew; B. T. G. Tan";1997;An efficient two-dimensional synthetic aperture radar (SAR) processing algorithm based on the exact transfer function is presented. The approximations made are similar to those in the chirp-scaling algorithm but it works with range-compressed data and can easily handle systems with range-varying Doppler centroids. The range-dependent migration is treated as a small perturbation from that of a given range position. This small deviation is corrected by resampling whilst the main migration factor (large in cases of large Doppler centroids) is corrected by a two-dimensional phase multiply. The azimuth compression filter is range-dependent and implemented as in the range-Doppler algorithm. This algorithm can be easily incorporated into SAR processors using the classical range-Doppler algorithm. The authors tested the algorithm on the Seasat Goldstone scene, ERS-1 and RADARSAT data. The interferometric offset tests show that the algorithm is phase-preserving.;Precision two-dimensional focusing of spaceborne synthetic aperture radar data with range-varying Doppler centroid;Not health related;Not health related;0
"C. Beleznai; M. Zeilinger; J. Huemer; W. Pointner; S. Wimmer; P. Zips";2023;Vision-based perception is a key enabling technology when attempting to convert human work processes into automated robotic workflows in diverse production and transport scenarios. Automation of such workflows, however, faces several challenges due to the diversity governing these scenarios: various objects to be handled, differing viewing conditions, partial visibility and occlusions. In this paper we describe the concept of an occlusion-robust pallet recognition methodology trained fully in the synthetic domain and well coping with varying object appearance. A key factor in our representation learning scheme is to entirely focus on geometric traits, captured by the surface normals of dense stereo depth data. Furthermore, we adopt a local key-point detection scheme with regressed attributes allowing for a bottom-up voting step for object candidates. The proposed geometric focus combined with local key-point based reasoning yields an appearance-independent (color, texture, material, illumination) and occlusion-robust detection scheme. A quantitative evaluation of recognition accuracy for two network architectures is performed using a manually fine-annotated multi-warehouse dataset. Given the standardized pallet dimensions, spatially accurate pose estimation and tracking, and robotic path planning are carried out and demonstrated in two automated forklift demonstrators. These demonstrators exhibit the ability to consistently perform automated pick-up and drop-off of pallets carrying arbitrary items, under a wide variation of settings.;Automated pallet handling via occlusion-robust recognition learned from synthetic data*;Not health related;Not health related;0
"N. Bhogapurapu; A. Bhattacharya; Y. S. Rao";2021;India launched the Chandrayaan-2 satellite on 22 July 2019, with several sensors onboard to map the Lunar surface and subsurface. Among the several sensors, the Dual Frequency Synthetic Aperture Radar (DFSAR) operates in L- and S-band frequencies in full and compact polarimetric modes. This paper analyzes full- and compact polarimetric SAR data acquired by L- and S-bands to explore backscattering characteristics of lunar surface and subsurface. The data utilized in this study are collected near the south polar region of the moon. We applied a few polarimetric decomposition techniques and scattering signatures to analyze the data. First, we analyzed the model-free four-component decomposition method and the model-based Yamaguchi four-component decomposition powers over various lunar surface features for the full-pol data. Similarly, for the compact-pol data, the model-free three-component decomposition and the m – _ are employed. Second, we carried out a comparative analysis of the decomposed powers at different sensor frequencies. In this preliminary study, we observed that the recently developed model-free decomposition techniques provide meaningful insights in interpreting physical scattering mechanisms in unknown/under-explored terrains of the lunar surface.;Chandrayaan-2 Dual Frequency Synthetic Aperture Radar (DFSAR) Full and Compact Polarimetric Data Analysis for the Lunar Surface;Not health related;Not health related;0
"T. Bakırman; G. Bilgin; F. B. _anlı; E. Uslu; M. Üstüner";2014;In this study, synthetic aperture radar (SAR) and multispectral data are fused with different methods in order to observe the effect of fusion methods on the accuracy of different classification techniques. At the same time, different polarizations of SAR data are included in fusion process and results are examined. The fusion methods that are used in this study are Brovey Color Normalized, Hue Saturation Value (HSV), Gram - Schmidt (GS) Spectral Sharpening and Principal Components (PC) Spectral Sharpening. Fused images are classified using k-nearest neighbor, support vector machine and radial based function neural network. The study area is chosen on Menemen Plain, which contains agricultural lands, and it is located in _zmir. Multispectral RapidEye satellite image and TerraSAR-X radar data are used for the analysis. Achieved results were presented in the tables. The highest accuracy is achieved by K-NN classification of TerraSAR-X and VH fusion with GS method as 95.74%.;Fusion and classification of synthetic aparture radar and multispectral sattellite data;Not health related;Not health related;0
"R. S. Zawadzki; S. Parvaneh";2023;In cardiology, we frequently develop machine learning models to predict events such as heart failure. Oftentimes, these events occur at low incidence in the available data, especially for under-represented subpopulations, which limits classifier performance due to class imbalance. To mitigate these issues, we investigate the use of synthetic data generation, or algorithms trained to mimic realistic patient data. In particular, we use synthetic data to augment training data for Catboost in classifying chronic heart failure using the University of California, Irvine myocardial infarction complications dataset (n = 1,700). Our primary metrics of interest are the mean and the variability of AUC and F1-Score across five-fold cross-validation. Overall, we find modest gains in performance over the baseline classifier with no augmented data. Nevertheless, the more sophisticated generators, both with and without hyperparameter tuning, did not confer better performance than simpler methods. Furthermore, all methods were subject to large variability in classification metrics across folds. While synthetic data generation is a promising tool for class imbalance, more investigation is needed to find optimal sample sizes and settings for the stability of results.;Synthetic Data Generation in Small Datasets to Improve Classification Performance for Chronic Heart Failure Prediction;health related;Not health related;1
"A. Tanaka; K. Uehara; T. Tamura; Y. Saito";2012;We propose the use of SAR (Synthetic Aperture Radar) backscatter intensity data as a tool for monitoring coast. ALOS (Advanced Land Observing Satellite) PALSAR (Phased Array type L-band SAR) data are analyzed to investigate the temporal changes in shape and area of mouthbars in the Mekong River distributaries. River mouthbars with strong backscatter, which is surrounded by the water, are successfully extracted using a histogram thresholding algorithm. Estimated areas of river mouthbars gradually increase on an annual time scale. Seasonal variations of areas were also recognized, and they appear to be related to the mean monthly sea level, discharge, and rainfall. Moreover, it is highly likely that tidal height at the time of SAR data acquisition clearly reflect the area of river mouthbar by examining over half-day intervals for ascending and descending images. This study shows that if appropriately calibrated, the SAR data is useful to quantifying long-term changes in river mouthbar.;Area change detection in river mouthbars at the Mekong River delta using Synthetic Aperture Radar (SAR) data;Not health related;Not health related;0
"S. Wirsing; B. Finley";2018;Mobile network usage models (along with system models) are useful tools for estimating the performance of future and current mobile communication systems. However current mobile network usage models lack either the granularity or flexibility required for assessing advanced concepts (such as cognitive radio and dynamic spectrum access) at academic research levels. Therefore, in this work, we propose a network usage model that is both granular (modeling at the individual device level) and flexible (allowing arbitrary combinations of empirically derived user types and various time scales). We derive the user types through time-series clustering of network usage patterns of around 400 diverse US-based smartphone users. Additionally, the model accounts for the bursty nature and self-similarity of such usage and we show these characteristics are preserved over several time scales.;Synthetic Smartphone Data Usage Modeling Through User Clustering;Not health related;Not health related;0
"C. Chaabani; R. Abdelfattah";2018;In this paper, we are dealing with image clustering in regard to the flooding extent delineation using Synthetic Aperture RADAR (SAR) and Interferometric SAR (InSAR) data. Even though we focus on flood mapping, it is not necessarily correct to consider the data division into two clusters (flooded and not flooded regions). In the context of unsupervised classification, the selection of optimal clusters number is a crucial task that affects the understanding of the clustering result. Therefore, the main objective of this work is to specify the required number of clusters needed in order to get an accurate flood map using the improved FCM approach that takes into account the InSAR coherence information. Indeed, we are solving this cluster analysis problem using fuzzy internal validity criteria namely Partition Coefficient and Partition Entropy indices. Lastly, we present experimental results concerning the Mallegue river flooding event that happened in 2005 in the North-West of Tunisia.;Evaluation of the optimal number of clusters for unsupervised flood mapping using Interferometric Synthetic Aperture RADAR data;Not health related;Not health related;0
"M. Zia; S. Frazier; H. Nazaripouya";2023;In power system applications, accessing real data is one of the main challenges. Load modeling is an example that can be hampered by a lack of real data. This paper proposes synthetic data generation as a solution to deal with insufficient datasets. To this end, Timeseries Generative Adversarial Networks (TimeGANs) are proposed to create synthetic electricity data for an agricultural load. Center pivot irrigation system data is targeted, which does not exhibit the same trends as residential or commercial utility data, and information is presented in the lower dimensions. Obtained results show that TimeGANs can leverage the lower dimensional information to produce synthetic data with the same characteristics as the actual data. The proposed method can be utilized for modeling different electric loads.;Synthetic Agricultural Load Data Generation Using TimeGANs;Not health related;Not health related;0
"Y. Mudhafar; D. Talbi; Z. Gal";2022;Extension of real data by synthetic data becomes more important aspect of the virtualization technics today. In this paper we demonstrate how synthetic data generated from real data can be used in the supervised classification process of three different recurrent neural networks: Long-Short Term Memory (LSTM), Bidirectional LSTM (BiLSTM) and Gated Recurrent Unit (GRU). Other aspect is presented concerning the influence of the noise to the classification of real and synthetic data series. The paper demonstrates that LSTM network has better classification performance than GRU, even the last one has higher accuracy during the training. Synthetic data can eternalize just part of the features of the original real data and extraction efficiency of these characteristics depend on the applied neural network.;Neural Network Based Comparison of Real and Synthetic Data Series in TeraHertz Domain;Not health related;Not health related;0
"J. Sadowsky; D. M. Weintraub; I. E. Feldberg; S. D. Diamond";1994;An inexpensive workstation that creates an immersive environment for several people to see and manipulate computer generated 3D graphic objects would have many innovative and important applications, such as planning and brainstorming (3D viewgraphs), education (3D chalk-board), and data visualization. We have been developing a PC-based system with a multimedia architecture. Each participant uses a head-mounted display with a video camera at each eye, and mixes the live video with computer generated graphics to create a heads-up display of the common scene with added virtual objects. We present the architecture, design problems and solutions, and application experiences.<>;A synthetic environment system for planning, education and data visualization;Not health related;Not health related;0
"M. A. Tolman; D. G. Long";2011;This paper compares the peak SNR and the point target impulse response function of stripmap SAR data processed with the well-known Omega-k algorithm and ideal matched filtering. The impulse response function resulting from Omega-k is distorted and stretched in azimuth compared to a matched filter. The distortion depends on the amount of frequency shift required in the Stolt mapping step. In at least some cases, the range resolution is improved compared to matched filtering at the expanse of additional azimuth interference.;New results on the Omega-k algorithm for processing synthetic aperture radar data;Not health related;Not health related;0
"M. Caruso; L. Gastaldi; S. Pastorelli; A. Cereatti; E. Digo";2022;Several biomedical contexts such as diagnosis, rehabilitation, and ergonomics require an accurate estimate of human upper limbs kinematics. Wearable inertial measurement units (IMU s) represent a suitable solution because of their unobtrusiveness, portability, and low-cost. However, the time-integration of the gyroscope angular velocity leads to an unbounded orientation drift affecting both angular and linear displacements over long observation interval. In this work, a Denavit-Hartenberg model of the upper limb was defined in accordance with the guidelines of the International Society of Biomechanics and exploited to design an optimization kinematics process. This procedure estimated the joint angles by minimizing the difference between the modelled and IMU-driven orientation of upper arm and forearm. In addition, reasonable constraints were added to limit the drift influence on the final joint kinematics accuracy. The validity of the procedure was tested on synthetic and experimental data acquired with a robotic arm over 20 minutes. Average rms errors amounted to 2.8 deg and 1.1 for synthetic and robot data, respectively. Clinical Relevance - The proposed method has the potential to improve robustness and accuracy of multi-joint kinematics estimation in the general contexts of home-based tele-rehabilitation interventions. In this respect adoption of multi-segmental kinematic model along with physiological joint constraints could contribute to address current limitations associated to unsupervised analysis in terms of monitoring and outcome assessment.;An ISB-consistent Denavit-Hartenberg model of the human upper limb for joint kinematics optimization: validation on synthetic and robot data during a typical rehabilitation gesture;Not health related;Not health related;0
"D. Nikolova; I. Vladimirov; A. Manolova";2023;In this paper, an experimental study of state-of-the-art techniques in Human Activity Recognition (HAR) is presented. Different Deep Learning algorithms, including CNNs and RNNs, are examined and compared. The experimental part is done using two real-life datasets Kinetics-400 and UCF-101 and one synthetic - SURREACT. All of them are used both for training and testing. The models - SlowFast, X3D and MViT are evaluated using accuracy top-1, and the results are identifying the best-performing combinations of dataset and model. One important question this study is trying to answer is whether a synthetic dataset can replace a real-world one. Finally, limitations and future directions are discussed, along with potential real-world applications.;An Experimental Analysis of Deep Learning Models for Human Activity Recognition with Synthetic Data;Not health related;health related;0
"Zhang Tao; Tan Lulu; Yang Ruliang";2009;An unsupervised segmentation method of polarimetric synthetic aperture radar (POLSAR) data is proposed in this paper. Firstly, this method performs initial classification of the POLSAR data based on polarimetric target decomposition. Then a model for the conditional distribution of the polarimetric coherency matrix is combined with a Markov random field (MRF) representation for the distribution of the region label to obtain the posterior distribution. Finally, iterated conditional modes (ICM) method is used to obtain the final segmentation results. A new MRF model with variable parameter is used in this method. Experiment results indicate that the proposed method achieves improvement over standard MRF method.;Unsupervised segmentation of polarimetric synthetic aperture radar data by Markov random field;Not health related;Not health related;0
"Y. Liang; T. Zhang; R. Wen; Z. Zhang";2011;The linear PN junction temperature sensor was used to collect the oil pool temperature of the hydraulic synthetic test-bed, and a temperature collecting system was designed with multi-sensor data fusion. The temperature detection circuit was given. And the temperature measurements were estimated by the least square weighted fusion estimation algorithm online which can improve the estimation precision of the oil temperature. In such way the algorithm provided a solid foundation for improving the temperature control precision. The simulation results indicate that the data fusion algorithm effectively deals with the congeneric multi-sensor data fusion.;Application of Multi-sensor Data Fusion on Hydraulic Synthetic Test-bed;Not health related;Not health related;0
"B. D. Jersak; M. J. Byrd; B. D. Krenek; A. J. Blanchard";1995;When combining different frequency bands together, phase discontinuities at the junctions are unavoidable. This paper examines the effect of these phase discontinuities on holographic synthetic aperture radar (HSAR) images. To study the problem, an actual measured data set consisting of two frequency bands (10-18 and 18-26 GHz) is used. A system model is introduced which relates physical displacement of the target to the resulting phase shift, which varies with frequency as well as transmitter/receiver location. The phase shifts corresponding to various amounts of displacement are added to the upper frequency band, and the resulting data is used to generate difference images. These difference images are either the coherent or incoherent difference between an image generated using the original data and one generated using the phase shifted data. It is concluded that the system model is valid, and that small displacements seen in controlled laboratory environments are insignificant for the target and frequencies studied.;Effects of phase discontinuities in banded data when generating holographic synthetic aperture radar images;Not health related;Not health related;0
"L. C. Adi; T. M. Cheng";2022;Real-time object detection is a smart tool in assisting in-field human workers. In order to achieve a better performance out of object detection deep learning models, tremendously larger quantities of data are needed to train the models’ network. However, obtaining data is expensive in terms of cost, extremely time-consuming, and prone to errors in the annotated part which reduces the detection performance. Hence, in recent years, many research has been focusing on developing alternatives to supplement real data with synthetic data. One of the most popular techniques to generate them is called domain randomization. In this research, an extensive analysis of the impact of various domain randomization strategies was conducted to discover the impact on object detection among the strategies to generate synthetic data from the example industrial objects. The strategies of object orientation and rendering randomization were comprehensively inspected. Insights on the usage of better synthetic data generation were given to improve object detection performance in selected aspects. The method is especially efficient when extremely limited real data were available, and the delicate employment of synthetic data has strengthened the object detection performance by up to 37% of improvement.;Analysis of Impact of Synthetic Image Data with Multiple Randomization Strategies on Object Detection Performance;Not health related;Not health related;0
"M. Widjaja; A. Darmawan; S. Mulyono";2012;This paper presents the development of a fuzzy model for classification of paddy growth stages based on synthetic MODIS data. Classification of growth stages is an important process in prediction of crop production using a remote-sensing technology. The proposed approach takes advantages of the nature of a fuzzy system which is able to capture gradual changes/movements by fitting its membership functions. A novel approach to shaping fuzzy input membership functions based on box-plot parameters is also presented. The developed fuzzy model was build and tested on 3935 sets of synthetic MODIS data. The results show that the proposed method was able to classify the growth stages satisfactorily and was robust to handle noises in the data.;Fuzzy classifier of paddy growth stages based on synthetic MODIS data;Not health related;Not health related;0
"S. Sarkar; L. Thorpe; E. Benetos; M. Sandler";2023;In this work, we tackle the challenging problem of separating mono-phonic instrument mixtures found in chamber music from monaural recordings. This task differs from the Music Demixing Challenge where the task is to separate vocals, drums, and bass stems from mastered stereo tracks. In our task, we separate the instruments in a permutation invariant fashion such that our model is capable of separating any two monophonic instruments, including mixtures of the same instrument. This task is particularly difficult due to label ambiguity and high spectral overlap. In this paper, we present a pre-training strategy and data augmentation pipeline using the multi-mic renders from the synthetic chamber ensemble dataset EnsembleSet and evaluate its impact using real-world chamber ensemble recordings from the URMP dataset. Our data augmentation pipeline, using synthetic data, has resulted in up to a remarkable +5.14 dB cross-dataset performance improvement for time-domain separation models when tested on real data. Our fine-tuning strategy in conjunction with our data augmentation pipeline results in up to +10.62 dB performance improvement w.r.t. our baseline for chamber ensemble separation. We report a strong negative correlation between pitch overlap and separation performance with an average of 5 dB performance drop for examples with pitch overlaps. We also show that pre-training our model with string, wind, and brass ensembles helps with separation of vocal harmony mixtures from Bach Chorales and Barbershop Quartet datasets with up to +17.92 dB SI-SDR improvement for 2 source vocal harmony mixtures.;Leveraging Synthetic Data for Improving Chamber Ensemble Separation;Not health related;Not health related;0
"K. Mattingly; S. De";2022;Small-sat systems are providing unprecedented access to remote sensing data and at a cost far lower than traditional spacecraft. This makes them appealing for a range of environmental and humanitarian monitoring applications previously thought unfeasible. In this paper we are demonstrating Capella Space's VHR X-band SAR, high-revisit satellite constellation applied to rice agriculture, glaciology, and a particular focus on urban mapping.;Capella Space X-Band Synthetic Aperture Radar (Sar) Data Applied to Environmental and Humanitarian Use Cases in Earth Observation with A Focus on Urban Footprint Mapping;Not health related;health related;0
"B. W. Sheffield; F. E. Bobe; B. Marchand; M. S. Emigh";2023;Synthetic Aperture Sonar (SAS) imaging has become a crucial technology for underwater exploration because of its unique ability to maintain resolution at increasing ranges, a characteristic absent in conventional sonar techniques. However, the effective application of deep learning to SAS data processing is often limited due to the scarcity of labeled data. To address this challenge, this paper proposes MoCo-SAS that leverages self-supervised learning (SSL) for SAS data processing, classification, and pattern recognition. The experimental results demonstrate that MoCo-SAS significantly outperforms traditional supervised learning methods, as evidenced by significant improvements observed in terms of the F1-score. These findings highlight the potential of SSL in advancing the state-of-the-art in SAS data processing, offering promising avenues for enhanced underwater object detection and classification.;Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition;Not health related;Not health related;0
"K. Mattingly; S. De";2023;"Satellites are often used to observe, understand, and track movement at the earth’s surface. SAR is an especially powerful tool for such applications due to its persistent viewing capability; it operates day or night and penetrates clouds, smoke, and other particulate cover that can occlude the ground from other instruments, like optical. There are many known techniques specific to determining motion in SAR data. However, this paper will apply an optical flow open source package, like Particle Image Velocimetry (PIV), to determine motion in SAR data provided by Capella Space. Movement will be analyzed across target velocities by implementing this technique on subapertures from a single collection and across a time-series collection over a given Area of Interest (AOI). Sample use cases in this paper include large ice blocks moving through a glacial mélange and moving ships on open water.";Computer Vision Techniques Applied to Capella Space VHR X-Band Synthetic Aperture Radar (SAR) Satellite Data to Track Movement: Use Cases Of Glacial Ice and Ships;Not health related;Not health related;0
N. Thanh Thien;2022;In this paper, the author describes the solution for the IEEE BigData Cup 2022 Challenge: Vehicle class and orientation detection in the real-world using synthetic images from driving simulators. This challenge’s task is to enhance the performance of the vehicle detection models, which are trained solely on photo-realistic images, in real-world scenarios. By applying image-to-image translation and domain adaptation technique, the proposed solution can improve the detection results on real-world images using models trained on synthetic data. Furthermore, the proposed method ranked 2nd in the leaderboard with final score of 0.4456.;Vehicle orientation detection based on synthetic data with image-to-image translation and domain adaptation;Not health related;Not health related;0
"J. Liu; C. Wang; L. Xu";2022;Target Detection is one of the most important tasks in Computer Vision, which has broad application prospects in many scenes. For the past few years, great progress has been made in this field with the rapid development of deep learning technology. However, it’s still a big challenge at complex scenarios such as dim environment for traditional single-modal visible images. To address this problem, researchers introduce thermal images as additional modal in consideration of that thermal cameras are less susceptible to interference and explore how to fuse the two modalities information effectively. Nevertheless, the lack of large labeled and high-quality visible-thermal datasets hampers the usage of convolutional neural networks for detection. Therefore, we propose to use image-to-image translation model combined with a differentiable data augmentation method to generate fake thermal images from labeled visible images and use multi-modal target detection model to prove the validity of the method. Our experiment results show that our method can provide us a large labeled dataset of synthetic visible-thermal image pairs with better generalization and the introduction of thermal modality can obtain a better performance than single modality.;Research on Synthetic Data Generation and Multi-modal Target Detection;Not health related;Not health related;0
"L. A. Zherdeva; E. Y. Minaev; N. A. Firsov";2023;To detect surface damage to buildings, it is necessary to involve workers who are at risk of industrial injuries when inspecting hard-to-reach areas of industrial premises. Attraction of special means, such as aerial platforms, safety systems, etc. increase the financial costs with this approach. The use of unmanned aerial vehicles, coupled with neural network algorithms, can simplify this procedure. Due to the inaccessibility, the problem of obtaining training data for neural networks arises, which can be solved by synthesizing them in a virtual environment.;Building surface damage recognition based on synthetic data;Not health related;Not health related;0
"J. Campanyà; X. Ogaya; A. G. Jones; V. Rath; J. Vozar; N. Meqbel";2016;"As a consequence of measuring time variations of the electric and the magnetic field, which are related to current flow and charge distribution, magnetotelluric (MT) data in 2-D and 3-D environments are not only sensitive to the geoelectrical structures below the measuring points but also to any lateral anomalies surrounding the acquisition site. This behaviour complicates the characterization of the electrical resistivity distribution of the subsurface, particularly in complex areas. In this manuscript we assess the main advantages of complementing the standard MT impedance tensor (Z) data with interstation horizontal magnetic tensor (H) and geomagnetic transfer function (T) data in constraining the subsurface in a 3-D environment beneath a MT profile. Our analysis was performed using synthetic responses with added normally distributed and scattered random noise. The sensitivity of each type of data to different resistivity anomalies was evaluated, showing that the degree to which each site and each period is affected by the same anomaly depends on the type of data. A dimensionality analysis, using Z, H and T data, identified the presence of the 3-D anomalies close to the profile, suggesting a 3-D approach for recovering the electrical resistivity values of the subsurface. Finally, the capacity for recovering the geoelectrical structures of the subsurface was evaluated by performing joint inversion using different data combinations, quantifying the differences between the true synthetic model and the models from inversion process. Four main improvements were observed when performing joint inversion of Z, H and T data: (1) superior precision and accuracy at characterizing the electrical resistivity values of the anomalies below and outside the profile; (2) the potential to recover high electrical resistivity anomalies that are poorly recovered using Z data alone; (3) improvement in the characterization of the bottom and lateral boundaries of the anomalies with low electrical resistivity; and (4) superior imaging of the horizontal continuity of structures with low electrical resistivity. These advantages offer new opportunities for the MT method by making the results from a MT profile in a 3-D environment more convincing, supporting the possibility of high-resolution studies in 3-D areas without expending a large amount of economical and computational resources, and also offering better resolution of targets with high electrical resistivity.";The advantages of complementing MT profiles in 3-D environments with geomagnetic transfer function and interstation horizontal magnetic transfer function data: results from a synthetic case study;Not health related;Not health related;0
"Yumin Lee; D. C. Cox";1998;A new technique for initializing an adaptive equalizer is proposed and analyzed. This technique, called synthetic initialization, synthesizes additional training symbols from a training sequence. The training sequence is thus effectively lengthened without any additional transmission overhead. Synthetic initialization can be used with any adaptive algorithm, and is easy to implement. Simulation results show that this new technique is very effective in improving the performance of adaptive decision feedback equalizers for indoor wireless communication systems such as the wireless local area network.;Synthetic initialization of adaptive equalizers for indoor wireless data communications;Not health related;Not health related;0
"G. Chen; L. Qi; C. Liang; X. Yang; S. Zheng; Y. Luo";2012;It is difficult to evaluate the maintainability in design phase obviously. The system level and indicators selecting procedure are given in this paper, and the experience & data based synthetic evaluation method of system maintainability is structured to find out the key component and improving direction, providing a scientific and effective solution for maintainability evaluation. Subsequently, the method is applied in an amplifier system in its design phase. It helps designers to evaluate system's maintainability more economically, and then direct maintainability design process scientifically, which plays more practical roles in the design phase.;Experience & data based synthetic evaluation method of system maintainability;Not health related;Not health related;0
"A. N. Rabchevsky; L. N. Yasnitsky";2022;This review article describes synthetic data, its applications, and examples of improving neural network algorithms with synthetic data. Using these examples, we show the important role of synthetic data in the improvement of neural network algorithms and the development of artificial intelligence.;The Role of Synthetic Data in Improving Neural Network Algorithms;Not health related;Not health related;0
"L. Lei; S. Liu; L. Fu; X. Meng; J. Wu";2014;"Reverse-time migration (RTM) is used to handle complex velocity models including steeply dipping interfaces and dramatic variations in transverse velocity, which promises better imaging results compared with traditional migration methods such as Kirchhoff migration algorithm. RTM has been increasingly used in oil and gas seismic exploration. Based on the similarity of kinematics and dynamics between electromagnetic wave and elastic wave, we apply pre-stack RTM method to process Ground Penetrating Radar (GPR) data in this paper. Finite-difference time domain (FDTD) numerical method is used to simulate the electromagnetic wave propagation including forward and backward extrapolation, the cross-correlation imaging condition is used to obtain the final image. Evaluation is in the context of a complex geological structure model; the Gaussian random roughness surface is introduced to represent the underground layer interface; common offset radar data and multi-shot wide-angle reflection and refraction (WARR) radar data are synthesized. The migration results from electromagnetic wave pre-stack RTM show excellent coincidence with the true model.";Examples of pre-stack reverse-time migration applied to ground penetrate radar synthetic data;Not health related;Not health related;0
"A. S. Wahd; D. Kim; S. -I. Lee";2022;We propose a bottom-up approach for the instance segmentation of cables (commonly referred in the literature as deformable linear objects). While the state of the art instance segmentation techniques propose a bounding box and perform foreground segmentation within each proposed bounding box, we adopt a bottom-up approach as cables can span a considerable part of the image or even the entire image, and therefore, cannot be well localized in a bounding box. In this paper, we show that several operations in the top-down instance segmentation approaches are only applicable for certain classes (i.e., compact objects) such as cars but they are a poor approximation for objects with highly overlapping bounding boxes such as cables. In particular, the non-maximum suppression and RoIPool/RoIAlign operations limit the generalizability of proposal-based instance segmentation methods to such datasets. Furthermore, we introduce a synthetic data generation technique that can also be applied to other popular public datasets such as COCO, Pascal VOC, and Cityscapes.;Cable Instance Segmentation with Synthetic Data Generation;Not health related;Not health related;0
"R. Dengel; M. Pajusalu";2023;Accessible, high-quality datasets for machine learning activities remain limited to this date. Especially for space applications, this is relevant, but even more difficult is it for exploration missions, where the data to be obtained is not known beforehand. To reduce this lack of data, this work proposes a pipeline for automatic realistic data generation for fly-by scenarios such as planned for ESA’s Comet Interceptor mission. The pipeline consists of two components. First, creating digital models of the nucleus and dust cloud using Blender and second, adding camera artefacts with Python. The camera artefacts considered here are radiation effects, image fractures, pixel errors, thermal noise, and dark current. The goal of the pipeline is to generate realistic data with camera artefacts to supplement the limited available real image footage. Additionally, the synthetic generation of data with random combinations of factors will allow to cover many possible scenarios, for example, irregular body shapes like was observed during ESA’s Rosetta mission. The generated data will be used to train efficient neural networks to be deployed on-board spacecrafts’ future processing systems which enable increased data reduction capabilities in space. The pipeline is also designed to create data to verify the system’s behaviour in certain scenarios.;A Synthetic Image Data Generation Pipeline for Spacecraft Fly-by Scenarios;Not health related;Not health related;0
"J. S. Won; W. M. Moon";1990;;Inversion Of Synthetic Aperture Radar Data For Surface Scattering;Not health related;Not health related;0
"G. Li; Z. Zhang; Y. Zhang; D. Mo; N. Wang; R. Wang; K. Zhang";2017;Synthetic aperture ladar (SAL) is a new remote sensing sensor and has the potential for much finer resolution than synthetic aperture radar (SAR) for its much shorter wavelength. For SAL system, frequency modulated continuous wave (FMCW) is usually used to achieve large time-bandwidth product (TBP). In this paper, we proposed the method for focusing the FMCW-SAL data. Firstly, we derived the signal model for FMCW-SAL without the start/stop approximation. And based on the signal model, imaging method is proposed. Then, the motion compensation for SAL is discussed. At last, we showed the imaging results for both the ground-based SAL and the airborne SAL. The imaging results verified the effectiveness of the proposed focusing method.;Focusing of the FMCW synthetic aperture ladar data;Not health related;Not health related;0
"T. Motohka; M. Shimada; M. Watanabe; N. Kawano; T. Shiraishi; R. B. Thapa";2013;In the paper, we present the performance analyses of the L-band SAR data (PALSAR and Pi-SAR-L2) for tropical deforestation and degradation monitoring in Riau province, Indonesia. For deforestation monitoring, accuracy assessment and demonstration of wide area mapping using the time series PALSAR gamma naught mosaics were shown. We used a simple thresholding for the temporal gamma naught changes in HV polarization, which allowed us to map annual deforestation full-automatically with over 90% accuracy and 100 m resolution. For degradation, we found that texture information in backscatter intensity images has a potential for wall-to-wall mapping the changes of canopy structure due to fallen trees.;Monitoring changes in tropical forests using L-band synthetic aperture radar data;Not health related;Not health related;0
"E. Stark; T. Eltoft; B. Braathen";1995;By including all available in-situ information from an area simultaneously imaged by the Landsat and Spot satellites, a vegetation map was constructed, and used to estimate the intensity distributions of five different vegetation classes in as many as ten spectral channels. The estimated class distributions were then used to generate synthetic multispectral data with the statistical properties of the real data. These synthetic data were used as input to some multispectral classifiers, including the classical Bayes classifier, a neural network classifier without contextual information, a neural network classifier with contextual information included as a functional link node, and a neural network classifier with contextual information as the intensity values of the neighbouring pixels. The results show that the neural network classifiers have a much less percentage of misclassifications than the Bayes classifier. By including contextual information of the pixels to be classified, the performance was found be significantly improved.;Performance of vegetation classification methods using synthetic multispectral satellite data;Not health related;Not health related;0
"J. Aristotle De Leon; M. Louie Enriquez; R. Concepcion; I. Valenzuela; R. Rhay Vicerra; H. Co; A. Bandala; E. Dadios";2022;The inversion process of Vertical Electrical Sounding (VES) is an important step in 1-D subsurface surface surveys to determine the true resistivities and heights of different layers of soil or rocks underground which is beneficial in geological and hydrological applications like locating potential areas for aquifers. Machine learning based algorithms is currently a trend in the inversion of vertical electrical sounding (VES) data to address the issues of the conventional methods. However, most models trained are being limited to one electrode half spacing configuration, and not being able to explain the underlying relationships of the model. Hence, the present study seeks to address these by obtaining VES inversion models for four-layer earth models through genetic programming and a synthetic dataset. The synthetic dataset covering different electrode half spacing configurations and VES curve types was generated and used to train the genetic programming model through GPTIPS software. By testing the best models on the synthetic dataset, it offered good metrics on the true resistivities of each layer, but performed poorly on estimating the layers’ heights. Regardless, the models obtained can be symbolically expressed and be interpreted which has not been done in other machine learning inversion models for VES. While this study’s implementation of genetic programming is not yet satisfactory, obtaining the symbolic expressions can allow future works to systematically improve the worst performing models.;Vertical Electrical Sounding Inversion Models Trained from Dataset using Synthetic Data and Genetic Programming;Not health related;Not health related;0
"A. L. Mitchell; A. H. -M. Ng; J. H. Yu; L. Ge";2011;This study investigates the use of multi-frequency (X- and L-band) Interferometric Synthetic Aperture Radar (InSAR) data for Digital Elevation Model (DEM) generation, coastline detection and land cover mapping over Heard, McDonald and Macquarie Islands in Australian Antarctic Territory. Conventional interferometric processing was applied to generate DEMs using recently acquired SAR data. The variable capacity to extract height information from X- and L-band SAR data was investigated. Elevation data was also compared with previously available DEMs generated using NASA JPL TOPSAR and RADARSAT-1 data. Land cover and surface dynamics and the capacity for coastline detection were also investigated using the multi-temporal, multi-frequency SAR intensity data. InSAR is invaluable as a source of elevation data and for extraction of land cover features and surface dynamics, especially in remote and cloud-affected regions such as the Antarctic.;Terrain characterisation of Heard, McDonald and Macquarie Islands using multi-frequency Interferometric Synthetic Aperture Radar (InSAR) data;Not health related;Not health related;0
"Lu Guangyue; Shao Chao";2001;The characteristics of the inverse synthetic aperture radar (ISAR) imaging data are fully analyzed from the view of complex image compression and the corresponding compressing method, Compression of AMplitude and Phase (CAMP) scheme for the phase history data, is proposed and verified by the real ISAR data.;Novel scheme for inverse synthetic aperture radar (ISAR) imaging data compression;Not health related;Not health related;0
"F. Nhita; Adiwijaya; I. Kurniawan";2023;Nowadays, the imbalanced data problem is still a crucial issue in classifying, especially in the medical data domain. For this reason, in this study, we propose a hybrid sampling method that combines adaptive synthetic sampling as an oversampling method and Tomek links as an undersampling method (ADASYN-Tomek) to improve the performance of the classification model. ADASYN has the advantage of synthesizing according to the hard-to-learn conditions of minor class samples. On the other hand, the Tomek links method can overcome class overlapping problems and remove noise. We conducted experiments on the pima indians diabetes data set obtained from the KEEL repository. The experimental results show that ADASYN-Tomek is better than the ADASYN or Tomek single method with a balanced accuracy score and F1score are 0.797 and 0.822, respectively. Furthermore, to improve the performance of the classification model, we performed tuning of the n-neighbors parameter in the oversampling process. We found an increase in the balanced accuracy score and F1-score to 0.835 and 0.856, respectively. The result indicates that the hybrid sampling method has the potential to be further developed to improve the classification model performance in various domain data sets.;Improvement of Imbalanced Data Handling: A Hybrid Sampling Approach by using Adaptive Synthetic Sampling and Tomek links;Not health related;Not health related;0
"M. A. H. Akhand; P. C. Shill; M. M. Hafizur Rahman; K. Murase";2012;The goal of an ensemble construction with several classifiers is to achieve better generalization than that of a single classifier. And proper diversity among classifiers is considered as the condition for an ensemble construction. This paper investigates synthetic pattern for diversity among classifiers. It alters input feature values of some patterns with the values of other patterns to get synthetic patterns. The pattern generation from using exiting patterns seems emphasize both accuracy and diversity among individual classifiers. Ensemble based on the synthetic patterns is evaluated for both neural networks and decision trees on a set of benchmark problems and was found to show good generalization ability.;Ensembles of diverse classifiers using synthetic training data;Not health related;Not health related;0
"A. S. Nair; J. Indu";2020;Water surface extent offers crucial information for reservoir operation management and studies on impact of hydrometeorological changes on reservoir capacity. However, obtaining the water extent using ground observation is not feasible at remote locations. The satellite observations in visible, infrared, and microwave spectrum have high potential in the delineation of water bodies. But each of these products have their own advantages and disadvantages. Therefore, in this study, we evaluate the performance of Landsat-8 and Sentinel-1 Synthetic Aperture Radar (SAR) data to extract reservoir water extent in India. This data is also a crucial component for studies on the future Surface Water and Ocean Topography (SWOT) mission which will be launched next year.;Reservoir Water Surface Area Detection using Satellite observations for synthetic SWOT data simulation;Not health related;Not health related;0
"K. . -S. Kuo; R. C. Weger; R. M. Welch";1997;The POLarization and Directionality of the Earth's Reflectances (POLDER) instrument onboard the Japanese ADEOS satellite offers unique possibilities for the retrieval of aerosol parameters with its polarization and multi-angular capability. In this study the authors examine a technique that simultaneously retrieve multiple aerosol parameters, namely asymmetry factor, single-scattering albedo, surface albedo, and optical thickness, using simulated POLDER reflectances. It is found that, over dark or bright surfaces, simultaneous retrieval of multiple parameters is indeed possible, but not over surfaces with intermediate reflectivity. Among the four parameters, the single-scattering albedo is retrieved with the best accuracy and is the least vulnerable when the reflectance value is subjected to a 0.1% white noise.;Aerosol retrieval using synthetic POLDER multi-angular data;Not health related;Not health related;0
"I. Fedosov; A. Shestakov";2023;Thermocouples account for more than 60% of all temperature sensors in commercial applications. At the same time, operation of thermocouples under the influence of harsh environments results in thermoelectric properties shift of thermocouple wire and growth a thermocouple error. Error detection and compensation are key issues for metrological self-check methods based on tracking drift of a measurement function. Development and tuning of these methods require large datasets with corrupted measurements during operation thermocouple. However, limitations of industrial datasets cause poor sensitivity to error and bounded scalability of current self-check methods. This paper outlines an original approach to development, tuning and validation of a simulation model for deteriorated thermocouples. The model generates synthetic data for thermocouple with drift of the measurement function independent of operation conditions. The model relies on distortion of the low-order coefficients from a nominal measurement function. The coefficient distortion method implements a uniform random distribution for each coefficient with an overall limit for the maximum thermocouple error. A validation scheme consists of three steps: forming an experimental dataset with the established maximum error, estimating model possibility to represent the experimental dataset, evaluating homogeneity of the coefficients for the simulated and experimental datasets via a statistical criterion. The proposed approach was tested for the K-type thermocouple with a maximum error of up to 3 above compared to the standard allowed error for temperature range of 100…800°C. The simulation model represents the experimental dataset with an error less than 0.25 of the standard error allowed for more than 95% of experimental points. Statistical evaluation of coefficients samples confirms H0-hypothesis about the identity of the general sample for each low-order coefficient with significance level $a=0.01$ and above.;Synthetic Data Generation for Thermocouples. An Original Approach to Simulation Modelling of a Measurement Function;Not health related;Not health related;0
"N. Fiscante; F. Biondi; P. Addabbo; C. Clemente; G. Giunta; D. Orlando";2022;Electric power transmission is the bulk movement of electrical energy from a generating site, such as a power plant, to an electrical substation. The interconnected lines which facilitate this movement are known as a transmission network. This is distinct from the local wiring between high-voltage substations and end-users, which is typically referred to as electric power distribution. Efficient long-distance transmission of electric power requires high voltages to reduce the losses produced by heavy current. In this case when electrons pass through the conductor, they cause coherent vibrations sometimes generating the acoustic signals. The problem of carrying out persistent, precise and spatially distributed monitoring of electric transmission line health is to use a large number of sensors physically distributed everywhere and somehow transmit the results to a control station. The use of satellite synthetic aperture radar data could be a viable solution of persistent and wide-area high-voltage vibration monitoring. In this paper we conduct a comprehensive survey on how to monitor both movements in terms of vibrational displacement, due to weathering, and those due to electricity transmission. The results demonstrate the technical feasibility of an operational use of satellite synthetic aperture radar for these purposes in the very short term.;High-Voltage Electric Power Transmission Monitoring by Micro-Motion Estimation using Synthetic Aperture Radar Data;Not health related;Not health related;0
F. Shang;2020;In this paper, first, the rationality of shifted angle value based orientation estimating methods is discussed. Such methods are theoretically hard to achieve high accuracy orientation angles. And, in the case of the actual orientation angle is big, the fluctuation of the estimation results in a local window will be too high to make a final decision. Second, a rough classification method for building area based on the fluctuation level of the shifted angle is suggested.;Discussion on Building Orientation Estimation Using Polarimetric Synthetic Aperture Radar Data;Not health related;Not health related;0
"Y. Wang; W. Lu; Y. Li";2023;Sound speed estimation in medical pulse-echo ultrasound imaging is a critical area of research with profound implications for disease diagnosis. However, traditional methods, such as tomographic reconstruction and layered media modeling, either involve substantial computational complexity or fail to account for lateral sound speed variations. This study introduces a new, efficient method for sound of speed (SoS) estimation, rooted in local event slope analysis of common middle point (CMP) gathers within synthetic aperture ultrasound channel data. This method facilitates direct estimation of tissue sound speed distribution, thus providing valuable quantitative parameters for clinical evaluations. Distinctly, our method yields a higher resolution velocity spectrum compared to conventional stacking velocity spectra, and effectively manages lateral velocity variations, catering to clinical scenarios with lateral sound speed heterogeneities. Ultimately, our proposed technique holds significant potential for enhancing ultrasound imaging quality and diagnostic precision.;Ultrasound Speed of Sound Reconstruction Based on Local Event Slopes of Synthetic Aperture Data;Not health related;Not health related;0
"M. Tao; F. Zhou; Z. Zhang";2015;Radio frequency interference is a major issue for synthetic aperture radar (SAR) imaging. Especially with the presence of wide band interference (WBI), the signal to interference and noise ratio of the measurements are greatly degraded, and thus makes it difficult to obtain high quality SAR image. In this paper, we analyzed the WBI signatures in a real measured data, and addressed the WBI mitigation problem by using the Eigensubspace filtering on the instantaneous spectra. Experimental results of the real measured data show that the proposed scheme is effective for suppressing the interference signal and for obtaining high-quality image.;Correction of wide-band interference signatures in real measured synthetic aperture radar data;Not health related;Not health related;0
"Z. Meng; Y. Li; Mengdao Xing; Zheng Bao";2015;Synthetic aperture radar (SAR) raw data simulator is an important tool for parameter-optimizing and algorithm-testing, particularly for those complicated configurations in which real raw data is difficult to obtain. As a new and special imaging mode, bistatic forward-looking SAR with constant acceleration (BFCA-SAR) can perform two-dimensional imaging for targets in the straight-ahead position over mono-static SAR. But there exist more complicated square roots and high-order terms in range history owing to high velocities and accelerations from both platforms. In addition, space variances in phase terms of two-dimensional frequency spectrum (2-D FS) make it difficult to gain echo data accurately. In this paper, a fast scene raw data simulator for BFCA-SAR based on quantitative analysis and effective correction of phase space variance is proposed. With high precision, our method can generate raw data more efficiently than traditional algorithms.;Fast raw data simulator of extended scenes for bistatic forward-looking synthetic aperture radar with constant acceleration;Not health related;Not health related;0
"R. Natsuaki; P. Prats-Iraola";2021;We propose a novel radio frequency interference (RFI) detection method for multi-receiver SAR systems. Some RFIs are difficult to be distinguished from the scattered radar echoes and thus, the focused SAR image contains some colored noise derived from RFI. Some SAR platforms apply a multi-receiver SAR system. In such a system, RFI signal is kept identical between the receivers while the ideal signals are non-identical because they are the summation of thousands of backscattered echoes. We developed an interferometric analysis method for separating RFI polluted bands from the SAR raw data based on the idea above.;Radio Frequency Interference Detection for Multi-Receiver Synthetic Aperture Radar Based on Interferometric Analysis of Raw Data;Not health related;Not health related;0
"N. Joshi; J. Channegowda; C. Lingaraj";2023;The era of battery powered automotive mobility is picking up pace. There have been several electrification initiatives undertaken by countries across the globe to surmount rising pollution levels. Lithium-ion Electric Vehicle battery packs continue to be a leading source of energy for most of the on-road Electric Vehicles. A key scientific goal among battery researchers is precise inference of State-of-Charge metrics of such storage elements. State-of-Charge helps estimate the range of a vehicle. High-quality battery parameter data is vital to strengthen the current momentum of transportation electrification. As competition among battery pack and Electric Vehicle manufacturers increases over time, confidentiality clauses have prevented access of high-quality battery measurement data to researchers. Lack of data has hindered improvements in developing meticulous State-of-Charge algorithms. In this paper we employ a sparse self-attention approach to produce synthetic battery parameter data. We release the code of our methodology in the results section of the paper to enable reproduction of our key findings.;Extended Battery Parameter Synthetic Data Generation Using Sparse Attention Mechanism;Not health related;Not health related;0
"S. A. Matzner; L. M. Zurk";2007;Most feature recognition and classification algorithms for synthetic aperture radar (SAR) are done in the image domain, and thus do not explicitly exploit the scattering physics underlying the data. Feature extraction in the phase domain is based on electromagnetic scattering models which describe the sensor response to scene features. This idea of processing the SAR data using a filter matched to the phase history signature of a feature was introduced by Franceschetti (1997). Here, we extend the original concept for processing the raw signal to processing the spotlight mode SAR signal. We use a sensor model based on an experimental airborne SAR developed by MIT Lincoln Laboratory, the Lincoln Multimission ISR Testbed (LiMIT). To demonstrate the concept, line feature filters are applied to a simulation of a building from the LiMIT dataset.;Frequency domain feature extraction from synthetic aperture radar data;Not health related;Not health related;0
"U. Nithirochananont; S. Chivapreecha; C. Peanvijarnpong; K. Dejhan";2010;Located in Ladkrabang, Bangkok, The Earth Observation Center (EOC) of the Geo-Informatics and Space Technology Development Agency (GISTDA) is a ground station for receiving, managing, processing and archiving the Earth observation satellites data. The GISTDA EOC has two SAR data processing systems (SDPS) to support data processing from two SAR satellite constellations: RADARSAT and ALOS. This paper provides the overview of both SAR data processing systems including descriptions of the systems, data processing algorithms, architectures and functionality. The paper finally verifies the image quality characteristics of the example SAR data products generated by the two SDPS by comparing with the satellite operating agency's specifications.;GISTDA EOC synthetic aperture radar data processing system;Not health related;Not health related;0
"H. Daud; R. Razali; V. Sagayan; M. Talib";2012;The aim of this study is to investigate the significant level of parameters for synthetics data generated from Computer Simulation Technology (CST) software that replicates sea bed logging environment using ANOVA analysis. Sea bed logging (SBL) is a technique of finding resistive layers under sea bed by transmitting low frequency EM waves through sea water and sediment. Low frequency is used to obtain greater wavelength for longer wave penetration. In this work frequency used are 0.5Hz, 0.25Hz, 0.125Hz and 0.0625Hz and sediment thickness are varied from 1000m to 3000m incremented every 250m. The acquired synthetics data have been processed using cubic spline interpolation technique with step sizes 4,8,12 and 16 and mean square errors (MSE) are calculated between the original and interpolated data. ANOVA analyses are applied to these MSE to investigate whether the parameters such as sediment thickness, frequency and spline step sizes are significant or not. It was found all the three parameters of interests are significant to the MSE calculated.;Sea bed logging: ANOVA analysis for EM synthetics data from CST software;Not health related;Not health related;0
"Z. Geng; W. Li; X. Yu; D. -Y. Zhu; G. Zhang";2023;In open-fields, a properly designed neural network for SAR automatic target recognition (ATR) should be able to produce super-class labels for targets it has never seen before, i.e. out-of-library (OOL) targets. Moreover, the network’s generalization ability should be tested with SAR imagery collected by multiple types of SAR systems. Unfortunately, most of the existing SAR ATR algorithms have only been tested by the Moving Stationary Target Acquisition and Recognition (MSTAR) dataset due to the lack of other publicly available data, which leads to potentially biased results. By jointly exploiting the Synthetic and Measured Paired Labeled Experiment (SAMPLE) dataset recently public-released by the U.S. Air Force Research Laboratory (AFRL) and the proprietary NUAA mini-SAR dataset, the problem of super-class labeling for OOL targets is thoroughly investigated for the first time. Simulation results show that super-class prediction for OOL targets with 100% synthesized SAR imagery as training data is technically feasible. Moreover, we demonstrate that fusing the information from the SAR images acquired at different elevation angles would boost the performance of the ATR algorithms for non-ideal SAR image samples.;Out-of-Library SAR Target Recognition with Deep Learning from Synthetic Data and Multiview Information Fusion;Not health related;Not health related;0
"A. S. Mandan; N. M. Hazzaa; O. Yildiz";2023;Early diagnosis requires cardiovascular disease forecasting. Past patient interests can improve machine learning predictions. This study uses a Generative Adversarial Network (GAN) to augment data to address the issue of insufficient datasets. Our objective is to use the GAN and feature selection (FS) to tackle the issue of class imbalance. The study assesses the efficacy of three different machine learning (ML) algorithms in predicting coronary artery disease (CAD). The UCI repository provided the Z-Alizadeh sani dataset for our analysis. A GAN was used to balance an imbalanced dataset. We used GAN to augment data and synthesise data to improve the models. Support Vector Machine (SVM) as feature selection tool was used to extract more effective attributes. We trained and tested CAD prediction using machine learning algorithms using 5-fold cross validation. We compared the machine-learning algorithm’s outcomes with and without GAN-based data enhancement and feature selection to evaluate its predicted accuracy. According to results GAN and feature selection improved accuracy, sensitivity, and specificity for all algorithms, suggesting that generated data and selected attributes can identify positive and negative cases.;GAN-generated Synthetic Data and SVM-based Feature Selection for Improved Cardiovascular Disease Prediction;health related;health related;0
A. G. Crosby;2007;"Previous work has shown that estimates of the admittance between topography and free-air gravity anomalies are often biased by spectral leakage, even after the application of multiple prolate spheroidal wavefunction data-tapers. Despite this, a number of authors who have used the free-air admittance method to estimate the weighted-average effective elastic thickness of the lithosphere (Te) and to identify topography supported by mantle convection have not tested their methods using synthetic data with a known relationship between topography and gravity. In this paper, I perform a range of such tests using both synthetic surface data and synthetic line-of-sight (LOS) accelerations of satellites orbiting around an extra-terrestrial planet. It is found that spectral leakage can cause the estimated admittance and coherence to be significantly in error—but only if the box in which they are estimated is too small. The definition of ‘small’ depends on the redness of the gravity spectrum. There is minimal error in the whole-box weighted-average estimate of Te if the admittance between surface gravity and topography is estimated within a box at least 3000-km-wide. When the synthetic (uniform) Te is less than 20 km and the coherence is high, the errors in Te are mostly ±5 km for all box sizes greater than 1000 km. On the other hand, when the true Te is greater than 20 km and the box size is 1000 km, the best-fitting Te is likely to be at least 5–10 km less than the true Te. However, even when the coherence is high, it is not possible to use elastic plate admittance models to distinguish between real and spurious small fractions of internal loading when the boxes are smaller than 2000 km in width. Noise in the gravity introduces error and uncertainty, but no additional bias, into the estimates of the admittance. It does, however, bias estimates of Te calculated using the coherence between Bouguer gravity and topography. The admittance at wavelengths between 1000 and 4000 km, which is of interest when studying mantle convection, is only well resolved when the box size is larger than 2000 km, or the synthetic Te is less than 15 km and the true long-wavelength admittance is greater than 30 mGal km_1. When the box size is 1000 km, it is not possible to distinguish between real and spurious convective support of long-wavelength topography. At the altitude of most orbiting satellites (200–400 km), gravity is much redder than it is at ground-level; therefore, it is necessary to use much larger boxes when using LOS accelerations to estimate the admittance than it is when using surface measurements. However, gradual changes in the altitude of the satellite, or even occasional sudden changes, make almost no difference to the estimate of the admittance. Finally, three short investigations are undertaken concerning the effect on the admittance of the application of irregular windows to the data, of lateral variations in Te, and of the infilling of flexural moats by low-density sediment. It is found that the bias due to the application of an irregular window is not significantly greater than the bias due to the application of a regular window of similar size; that the best-fitting Te is strongly biased by the value associated with the highest-amplitude topography; and that sediment infill raises the admittance at wavelengths corresponding to the width of the moat.";An assessment of the accuracy of admittance and coherence estimates using synthetic data;Not health related;Not health related;0
"S. J. Sanabria; T. Brevett; A. Telichko; J. Dahl";2022;Speed-of-sound (SoS) estimation is important in abdominal ultrasound for both aberration compensation and liver tissue quantification, and there is need to develop SoS reconstruction methods for curvilinear array geometries. In this work, we directly reconstruct SoS from full-synthetic aperture (FSA) datasets. We first introduce scatterer triangulation from Common-Mid-Point (CMP) gathers acquired with curvilinear probe geometries, where a spatio-temporally filter allows selection of scattering signals from a single azimuth direction. Next we develop a ray-tracing interpolation approach and regularized inversion for ultrasound computed tomography reconstruction in polar geometries. Our method allows direct estimation of both scattering locations and absolute time delays for SoS estimation. Experimental SoS estimation in attenuating phantoms is feasible at depths> 120 mm. Experimental SoS reconstruction is demon-strated in layered abdominal phantoms simulating fatty liver disease stages through aberrating subcutaneous tissue layers.;Speed of Sound Imaging with Curvilinear Probes from Full-Synthetic Aperture Data;Not health related;Not health related;0
"J. Seppänen; J. Kainulainen; J. Lemmetyinen; K. Rautiainen; M. Hallikainen; M. Mäkynen";2009;We have studied usage of our airborne L-band 2-D synthetic aperture radiometer HUT-2D for estimation of soil moisture. Measurements were conducted over three sites in Northern and Southern Finland in August 2007. Good results were achieved for bare soil and low vegetation, whereas soil moisture retrieval for forested areas requires further studies.;Soil moisture retrieval from HUT-2D synthetic aperture radiometer data;Not health related;Not health related;0
C. Tzaras;2004;This paper focuses on communication systems in which only the existence of line of sight is required for an acceptable quality of service. A reliable method is presented for minimizing the risks involved in the preliminary study of future deployments by avoiding the use of an analytical description of the environment in which the system is supposed to operate. The results show that statistically generated environments can successfully evaluate the feasibility of future communication systems at regions where no terrain or clutter information is available.;Deployment of communication networks based on synthetic elevation data;Not health related;Not health related;0
"A. N. Belov; A. A. Zarubin; A. A. Savelieva; A. V. Tarlykov";2019;"The article describes a method to generate training data for the neural network used in aircrafts' classification, localization and parameters' estimation problem. The standard description of the approach used, top-level algorithms and their configuration details provided. The simplified problem described, modeled and solved; results also included.";The Use Of Synthetic Data For Training The Neural Network To Classify The Aircrafts;Not health related;Not health related;0
"P. Thogarchety; K. Das";2023;Statistical machine learning models suffer poorly because of class imbalance issue. Real world dataset contains mostly ‘normal’ examples and very few ‘abnormal’ examples and in most of the cases, the primary goal is to identify the abnormal instances. For example, if we want to develop a statistical machine learning model to identify financial fraud using the historical transaction data then we can expect that majority of the data comes from normal/non-fraudulent class, whereas very few examples are fraudulent transactions. Using such imbalanced dataset for training makes machine learning models highly biased towards majority non-fraudulent class. This way, the objective to catch fraudulent transaction instances fails and misclassifying such minority class instances often results in a much higher cost. Hence, a balanced dataset is very much required to train a sound model. Different techniques such as under sampling, oversampling, SMOTE were proposed earlier. In this paper, we propose a novel technique to generate synthetic data using genetic search algorithm. We examined the effectiveness of our proposed algorithm on different datasets and reported in section V.;Synthetic Data Generation Using Genetic Algorithm;Not health related;Not health related;0
"L. Krames; P. Suppa; W. Nahm";2022;In laparoscopic surgery image-guided navigation systems could support the surgeon by providing subsurface information such as the positions of tumors and vessels. For this purpose, one option is to perform a reliable registration of preoperative 3D data and a surface patch from laparo-scopic video data. A robust and automatic 3D-3D registration pipeline for the application during laparoscopic surgery has not yet been found due to application-specific challenges. To gain a better insight, we propose a framework enabling a qualitative and quantitative comparison of different registration approaches. The introduced framework is able to evaluate 3D feature descriptors and registration algorithms by generating and modifying synthetic data from clinical examples. Different confounding factors are considered and thus the reality can be reflected in any simplified or more complex way. Two exemplary experiments with a liver model, using the RANSAC algorithm, showed an increasing registration error for a decreasing size of the surface patch size and after introducing modifications. Moreover, the registration accuracy was dependent on the position and structure of the surface patch. The framework helps to quantitatively assess and optimize the registration pipeline, and hereby suggests future software improvements even with only few clinical examples. Clinical relevance— The introduced framework permits a quantitative and comprehensive comparison of different registration approaches which forms the basis for a supportive navigation tool in laparoscopic surgery.;Generation of Synthetic Data for the Comparison of Different 3D-3D Registration Approaches in Laparoscopic Surgery;health related;health related;0
"M. Grond; W. Brink; B. Herbst";2016;Recognizing text in natural images can be a useful tool for image understanding. We focus on the detection problem, which is to find regions in an image occupied by text. We consider multi-layered convolutional neural networks as a means to classify local regions as text or not, and take a sliding-window approach to scan a full image. For training we generate large synthetic datasets to complement the much smaller available sets of labelled natural images. Our results suggest that larger networks can perform better on this problem, and the highest test accuracy is achieved from training with synthetic data and fine-tuning with natural data.;Text detection in natural images with convolutional neural networks and synthetic training data;Not health related;Not health related;0
"Z. Yuan; F. Xu; S. Xu; P. Tang";2023;Accurate location information is essential for location-based services in indoor environments. In recent years, Deep Neural Networks (DNN) based on Received Signal Strength Indicator (RSSI) fingerprints have shown significant potential in localization performance and cost-effectiveness. However, these methods necessitate a substantial volume of training data to effectively train DNN models, consequently escalating the expenses associated with data collection. This paper proposes a method to enhance RSSI data by generating synthetic RSSI data using Filtered Generative Adversarial Networks (FGANs) and mixing it with partially labeled real data to form a comprehensive dataset to address this issue. Additionally, a Cluster K-Nearest Neighbor (CKNN) algorithm is introduced to classify of the mixed dataset. This data augmentation and classification scheme significantly improves the overall accuracy of indoor localization systems, as validated through simulation experiments. The results demonstrate that the proposed method enhances the localization accuracy of simulated data by 12.66%, while saving data collection costs, thus achieving acceptable localization accuracy.;Using Filtered Synthetic Data and Cluster K-Nearest Neighbor to Enhance the Accuracy of Fingerprint-Based Indoor Localization;Not health related;Not health related;0
"T. Murata; S. Date; Y. Goto; T. Hanawa; T. Harada; M. Ichikawa; H. Lee; M. Munetomo; A. Sugiki";2020;In this paper, we introduce a distribution system of synthesized data of Japanese population using Interdisciplinary Large-scale Information Infra-structures in Japan. Synthetic population is synthesized based on the statistics of the census that are conducted by the government and publicly released. Therefore, the synthesized data have no privacy data. However, it is easy to estimate the compositions of households, working status in a certain area from the synthetic population. Therefore, we currently distribute the synthesized data only for public or academic purposes. For academic purposes, it is important to encourage scholars or researchers to use a large-scale data of households, we define protection levels for the attributes in the synthetic populations. According to the protection levels, we distribute the data with proper attributes to those who try to use them. We encourage researchers to use the synthetic populations to be familiar to large-scale data processing.;Distribution System for Japanese Synthetic Population Data with Protection Level;Not health related;Not health related;0
"L. Dell'Amore; M. Villano; G. Krieger; A. Moreira";2019;Synthetic aperture radar (SAR) remote sensing is very attractive for the systematic observation of dynamic processes on the Earth's surface since it allows high resolution imaging independent of weather conditions and sunlight illumination. Waveform-encoded SAR is a novel SAR concept based on pulse-to-pulse variation of the transmitted waveform that allows focusing the nadir echo and the range ambiguities and suppressing them through a multi-focus post-processing. However, the assessment of the ambiguity suppression performance for such a system is not trivial, as the processing involves a non-linear thresholding and blanking approach. This work proposes a novel methodology, which exploits real TerraSAR-X data to accurately simulate the effect of the range ambiguity on the useful signal and allows for a quantitative assessment of the image quality of a waveform-encoded SAR. The analysis considers different waveform variation schemes (namely up- and down-chirps and cyclically shifted chirps) and a contrast-minimization technique for threshold selection, whose performance is compared to the best achievable one (i.e., the optimum threshold). The results of this work further highlight the potentialities of the waveform-encoded SAR concept and allow accounting for its ambiguity suppression capability in the design of a SAR system.;Waveform-Encoded Synthetic Aperture Radar: Image Quality Assessment Using Satellite Data;Not health related;Not health related;0
"A. N. J. Kukunuri; D. Lal; H. Srigadde; J. Singh; D. Singh";2023;Synthetic aperture radar (SAR) data is a powerful tool in vegetation studies, providing valuable insights into the dynamics of vegetation in a changing environment. However, real-world SAR data acquisition is expensive, time-consuming, and limited, hindering the development of robust machine learning (ML) models for various applications. To enhance the accuracy of ML models, there is a need for generating synthetic data. Therefore, this study focuses on electromagnetic modeling of soil and vegetation to generate synthetic backscatter coefficients at L band using the High-Frequency Structure Simulator (HFSS). We have created different models for various scenarios with varying incident angles, frequencies, and polarizations, enabling us to explore wide range of SAR imaging conditions. The accuracy of the simulated backscattering coefficients is validated by comparing with ALOS2 PALSAR 2 satellite data and observed a very good agreement between the simulated and observed data.;An Approach to Model Soil and Vegetation with HFSS for Synthetic SAR Data Generation;Not health related;Not health related;0
"Z. Wang; P. Myles; A. Tucker";2019;There is increasing interest in the potential of synthetic data to validate and benchmark machine learning algorithms as well as reveal any biases in real-world data used for algorithm development. This paper discusses the key requirements of synthetic data for such purposes and proposes an approach to generating and evaluating synthetic data that meets these requirements. We propose a framework to generate and evaluate synthetic data with the aim of simultaneously preserving the complexities of ground truth data in the synthetic data whilst also ensuring privacy. We include as a case study, a proof-of-concept synthetic dataset modelled on UK primary care data to demonstrate the application of this framework.;Generating and Evaluating Synthetic UK Primary Care Data: Preserving Data Utility & Patient Privacy;health related;Not health related;1
"Q. Yang; Z. Wang; K. Guo; C. Cai; X. Qu";2023;Deep learning (DL) has driven innovation in the field of computational imaging. One of its bottlenecks is unavailable or insufficient training data. This article reviews an emerging paradigm, imaging physics-based data synthesis (IPADS), that can provide huge training data in biomedical magnetic resonance (MR) without or with few real data. Following the physical law of MR, IPADS generates signals from differential equations or analytical solution models, making learning more scalable and explainable and better protecting privacy. Key components of IPADS learning, including signal generation models, basic DL network structures, enhanced data generation, and learning methods, are discussed. Great IPADS potential has been demonstrated by representative applications in fast imaging, ultrafast signal reconstruction, and accurate parameter quantification. Finally, open questions and future work are discussed.;Physics-Driven Synthetic Data Learning for Biomedical Magnetic Resonance: The imaging physics-based data synthesis paradigm for artificial intelligence;health related;health related;1
"A. S. Dina; A. B. Siddique; D. Manivannan";2022;Attacks on computer networks have increased significantly in recent days, due in part to the availability of sophisticated tools for launching such attacks as well as the thriving underground cyber-crime economy to support it. Over the past several years, researchers in academia and industry used machine learning (ML) techniques to design and implement Intrusion Detection Systems (IDSes) for computer networks. Many of these researchers used datasets collected by various organizations to train ML classifiers for detecting intrusions. In many of the datasets used in training ML classifiers in such systems, data are imbalanced (i.e., not all classes had equal number of samples). ML classifiers trained with such imbalanced datasets may produce unsatisfactory results. Traditionally, researchers used over-sampling and under-sampling for balancing data in datasets to overcome this problem. In this work, in addition to random over-sampling, we also used a synthetic data generation method, called Conditional Generative Adversarial Network (CTGAN), to balance data and study their effect on the performance of various widely used ML classifiers. To the best of our knowledge, no one else has used CTGAN to generate synthetic samples to balance intrusion detection datasets. Based on extensive experiments using widely used datasets NSL-KDD and UNSW-NB15, we found that training ML classifiers on datasets balanced with synthetic samples generated by CTGAN increased their prediction accuracy by up to 8% and improved their MCC score by up to 13%, compared to training the same ML classifiers over imbalanced datasets. We also show that this approach consistently performs better than some of the recently proposed state-of-the-art IDSes on both datasets. Our experiments also demonstrate that the accuracy of some ML classifiers trained over datasets balanced with random over-sampling decline compared to the same ML classifiers trained over original imbalanced dataset.;Effect of Balancing Data Using Synthetic Data on the Performance of Machine Learning Classifiers for Intrusion Detection in Computer Networks;Not health related;Not health related;0
"I. Idehen; W. Jang; T. Overbye";2019;It is critical that the qualities and features of synthetically-generated, PMU measurements used for grid analysis matches those of measurements obtained from field-based PMUs. This ensures that analysis results generated by researchers during grid studies replicate those outcomes typically expected by engineers in real-life situations. In this paper, essential features associated with industry PMU-derived data measurements are analyzed for input considerations in the generation of vast amounts of synthetic power system data. Inherent variabilities in PMU data as a result of the random dynamics in power system operations, oscillatory contents, and the prevalence of bad data are presented. Statistical results show that in the generation of large datasets of synthetic, grid measurements, an inclusion of different data anomalies, ambient oscillation contents, and random cases of missing data samples due to packet drops helps to improve the realism of experimental data used in power systems analysis.;PMU Data Feature Considerations for Realistic, Synthetic Data Generation;Not health related;Not health related;0
"P. Georg Olofsson Zwilgmeyer; M. Yip; A. Langeland Teigen; R. Mester; A. Stahl";2021;Underwater visual perception requires being able to deal with bad and rapidly varying illumination and with reduced visibility due to water turbidity. The verification of such algorithms is crucial for safe and efficient underwater exploration and intervention operations. Ground truth data play an important role in evaluating vision algorithms. However, obtaining ground truth from real underwater environments is in general very hard, if possible at all.In a synthetic underwater 3D environment, however, (nearly) all parameters are known and controllable, and ground truth data can be absolutely accurate in terms of geometry. In this paper, we present the $VAROS$ environment, our approach to generating highly realistic under-water video and auxiliary sensor data with precise ground truth, built around the Blender modeling and rendering environment. $VAROS$ allows for physically realistic motion of the simulated underwater (UW) vehicle including moving illumination. Pose sequences are created by first defining waypoints for the simulated underwater vehicle which are expanded into a smooth vehicle course sampled at IMU data rate (200 Hz). This expansion uses a vehicle dynamics model and a discrete-time controller algorithm that simulates the sequential following of the waypoints.The scenes are rendered using the raytracing method, which generates realistic images, integrating direct light, and indirect volumetric scattering. The $VAROS$ dataset version 1 provides images, inertial measurement unit (IMU) and depth gauge data, as well as ground truth poses, depth images and surface normal images.;The VAROS Synthetic Underwater Data Set: Towards realistic multi-sensor underwater data with ground truth;Not health related;Not health related;0
"F. Poucin; A. Kraus; M. Simon";2021;A major issue related to computer vision for the automotive industry is that real-world perception models require huge amount of well-annotated data to achieve decent performance. While this data is very expensive to collect and annotate, synthetically generated images seem to be an efficient alternative to solve this problem. More and more public data sets, composed of synthetic data, are available in various domains, however, there is too little concrete methodology to use them properly. In this paper, we propose a simple approach combining the use of synthetic and real images to boost instance segmentation. We mention some pre-processing requirements as harmonizing instance labeling and removing non-valuable instances from synthetic images. We present our training strategy based on data set mixing, and show that it overcomes the domain shift between real and synthetic data sets. A comparison study with other training approaches, such as fine-tuning techniques, highlights the benefits of our method, which boosts network performances on both real and synthetic image inferences.;Boosting Instance Segmentation with Synthetic Data: A study to overcome the limits of real world data sets;Not health related;Not health related;0
"B. Ramanan; L. Drabeck; T. Woo; T. Cauble; A. Rana";2019;Voice interfaces are fast becoming an important human-machine interfaces, and Wake Word Engines (WWEs) are a critical part of modern voice interfaces. There are recent advancements in applying Deep Learning (DL) or Deep Neural Network architectures for WWE construction. Similar to other applications of DL however, achieving good accuracy strongly depends on training using the right type of dataset - a task that traditionally requires significant time and human effort. In this paper, we present novel techniques for curating WWE datasets that significantly minimizes the need for data collection from humans. More specifically, we investigated two techniques: (1) Using “Found Data”: we have created automated data curation pipelines for locating and processing data from public sources (e.g., YouTube), and (2) Using Synthetic Data: we explored the use of Synthesized Speech (using Text to Speech and Voice Conversion) to synthetically generate WWE datasets. Using our techniques, we are able to train WWEs that demonstrate a level of performance that is on-par with the WWEs trained with data from expensive human collection (e.g., Mechanical Turk). For example, in our experiments using wake words `Computer' and `Shannon', for a given False Alarm rate of 1 per hour, WWEs trained with our novel methods exhibit a False Reject Rate, averaged across four different test environments, of 0.9% and 1.6% respectively compared to the 1.0% and 0.8% for the baseline trained using data from human collection. Cycle time savings of more than an order of magnitude is possible by utilizing these methods.;Eliminating Data Collection Bottleneck for Wake Word Engine Training Using Found and Synthetic Data;Not health related;Not health related;0
"J. -H. Lee; I. -Y. Kim; C. M. O'Keefe";2011;This paper concerns the use of synthetic data for protecting the confidentiality of business data during statistical analysis. Synthetic datasets are constructed by replacing sensitive values in a confidential dataset with draws from statistical models estimated on the confidential dataset. Unfortunately, the process of generating effective statistical models can be a difficult and labour-intensive task. Recently, it has been proposed to use easily-implemented methods from machine learning instead of statistical model estimation in the data synthesis task. J. Drechsler and J.P. Reiter [1] have conducted an evaluation of four such methods, and have found that regression trees could give rise to synthetic datasets which provide reliable analysis results as well as low disclosure risks. Their conclusion was based on simulations using a subset of the 2002 Uganda census public use file, and it is an interesting question whether the same conclusion applies to other types of data with different characteristics. For example, business data have quite different characteristics from population census and survey data. Business data generally have few variables that are mostly categorical, and often have highly skewed distributions with outliers. In this paper we investigate the applicability of regression-tree-based methods for constructing synthetic business data. We give a detailed example comparing exploratory data analysis and linear regression results under two variants of a regression-tree-based synthetic data approach. We also include an evaluation of the analysis results with respect to the results of analysis of the original data. We further investigate the impact of different stopping criteria on performance. Our example provides evidence that synthesisers based on regression trees may not be immediately applicable in the context of business data. Further investigation, including further simulation studies with larger datasets, is certainly indicated.;Applicability of Regression-Tree-Based Synthetic Data Methods for Business Data;Not health related;Not health related;0
"Y. Cheng; L. Zhang; A. Li";2023;Federated learning (FL) enables large amounts of participants to construct a global learning model, while storing training data privately at local client devices. A fundamental issue in FL systems is the susceptibility to the highly skewed distributed data. A series of methods have been proposed to mitigate the Non-IID problem by limiting the distances between local models and the global model, but they cannot address the root cause of skewed data distribution eventually. Some methods share extra samples from the server to clients, which requires comprehensive data collection by the server and may raise potential privacy risks. In this work, we propose an efficient and adaptive framework, named Generative Federated Learning (GFL), to solve the skewed data problem in FL systems in a privacy-friendly way. We introduce Generative Adversarial Networks (GAN) into FL to generate synthetic data, which can be used by the server to balance data distributions. To keep the distribution and membership of clients' data private, the synthetic samples are generated with random distributions and protected by a differential privacy mechanism. The results show that GFL significantly outperforms existing approaches in terms of achieving more accurate global models (e.g., 17% - 50% higher accuracy) as well as building global models with faster convergence speed without increasing much computation or communication costs.;GFL: Federated Learning on Non-IID Data via Privacy-Preserving Synthetic Data;Not health related;Not health related;0
"B. Fu; M. Klemt; F. Boutros; N. Damer";2023;In recent years, advances in deep learning techniques and large-scale identity-labeled datasets have enabled facial recognition algorithms to rapidly gain performance. However, due to privacy issues, ethical concerns, and regulations governing the processing, transmission, and storage of biometric samples, several publicly available face image datasets are being withdrawn by their creators. The reason is that these datasets are mostly crawled from the web with the possibility that not all users had properly consented to processing their biometric data. To mitigate this problem, synthetic face images from generative approaches are motivated to substitute the need for authentic face images to train and test face recognition. In this work, we investigate both the relation between synthetic face image data and the generator authentic training data and the relation between the authentic data and the synthetic data in general under two aspects, i.e. the general image quality and face image quality. The first term refers to perceived image quality and the second measures the utility of a face image for automatic face recognition algorithms. To further quantify these relations, we build the analyses under two terms denoted as the dissimilarity in quality values expressing the general difference in quality distributions and the dissimilarity in quality diversity expressing the diversity in the quality values.;On the Quality and Diversity of Synthetic Face Data and its Relation to the Generator Training Data;Not health related;Not health related;0
"U. Javaid; Z. Li; M. N. Aman; D. Shao; K. Yee; B. Sikdar";2022;Data collaboration with cloud technologies is becoming more popular for personal use as well as business applications. Due to the increasing data protection regulations worldwide, different cryptographic techniques have been designed to enable secure data sharing for a user or a group of users. Although, these techniques have seen enterprise adoption, they fail to offer data visibility as data remains encrypted throughout the data sharing routine. This is why these techniques fail to offer a key features to data users, e.g., joining different datasets together and sharing it with all the users involved. This paper presents a blockchain based architecture for secure data collaboration in cloud using differentially private synthetic data and trusted execution environment (TEE). The proposed solution protects data confidentiality and integrity with TEEs, supports public-key infrastructure (PKI) with blockchain, and prevents privacy leakages with synthetic data. The results show that our synthetic data performs as good as real data and demonstrates how different users can securely aggregate their datasets and openly share among themselves.;Blockchain based Secure Group Data Collaboration in Cloud with Differentially Private Synthetic Data and Trusted Execution Environment;Not health related;Not health related;0
"Z. Wang; P. Myles; A. Jain; J. L. Keidel; R. Liddi; L. Mackillop; C. Velardo; A. Tucker";2021;Synthetic data offer a number of advantages over using ground truth data when working with private and personal information about individuals. Firstly, the risk of identifying individuals is reduced considerably, which enables the sharing of data for analysis amongst more organisations. Secondly, the fine tuning of synthetic datapoints to suit particular modelling and analyses could help to build more suitable models that can avoid biases found in the original ground truth data. In this paper we explore how a probabilistic synthetic data generator can be used to model data with high enough fidelity that it can be used to develop and validate state-of-the-art machine learning models. In particular, we use a Bayesian network model trained on gestational diabetes data, generated from a mobile health app collected from a number of health trusts in the UK. These data are used to train and test an established machine learning model developed by Sensyne Health using real-world data, and the resulting performance is compared to performance on ground truth data. In addition, a clinical validation is undertaken to explore if human experts can differentiate real patients from synthetic ones. We demonstrate that the Bayesian network synthetic data generator is able to mimic the ground truth closely enough to make it difficult for a human expert to distinguish between the two. We show that the data generator captures the interactions between features and the multivariate distributions close enough to enable classifiers to be inferred that imitate the key performance characteristics of models inferred from ground truth data. What is more, we demonstrate that the discovered mis-classifications found when testing using the synthetic data, are as informative as when testing using ground truth data.;Evaluating a Longitudinal Synthetic Data Generator using Real World Data;Not health related;Not health related;1
"T. Lindgren; O. Steinert";2022;Data driven prognostic models are becoming more prevalent in many areas, ranging from heavy trucks to gas turbines. One aspect of certain prognostic models is the need for labeled failures, which then can be used as positive examples, when modelling the prognostic problem. Unfortunately, standard algorithms for creating prognostic models can suffer when labeled data is unbalanced, w.r.t. class distribution, leading to prognostic models with poor performance. In this paper we present a methodology for creating synthetic data that can be used to augment the underrepresented class and hence dramatically increase performance of the data driven predictive model. In our study we utilize data collected from heavy trucks and focus on predicting failure of one engine component that is crucial for the operation of heavy trucks. We examine different way of generating synthetic examples in a low dimensional setting, it is found that three methods out of the six methods studied does not improve performance compared to using only the original data. The other three methods based on interpolation is superior to only using the original data, with SMOTE outperforming the two other interpolation methods. SMOTE lowers the estimated cost on test data, compared to using a model trained on the original data set only, with 67%.;Low dimensional synthetic data generation for improving data driven prognostic models;Not health related;Not health related;0
"R. van Kempen; B. Lampe; L. Reiher; T. Woopen; T. Beemelmanns; L. Eckstein";2022;In perception tasks of automated vehicles (AVs) data-driven have often outperformed conventional approaches. This motivated us to develop a data-driven methodology to compute occupancy grid maps (OGMs) from lidar measurements. Our approach extends previous work such that the estimated environment representation now contains an additional layer for cells occupied by dynamic objects. Earlier solutions could only distinguish between free and occupied cells. The information whether an obstacle could move plays an important role for planning the behavior of an AV. We present two approaches to generating training data. One approach extends our previous work on using synthetic training data so that OGMs with the three aforementioned cell states are generated. The other approach uses manual annotations from the nuScenes [1] dataset to create training data. We compare the performance of both models in a quantitative analysis on unseen data from the real-world dataset. Next, we analyze the ability of both approaches to cope with a domain shift, i.e. when presented with lidar measurements from a different sensor on a different vehicle. We propose using information gained from evaluation on real-world data to further close the reality gap and create better synthetic data that can be used to train occupancy grid mapping models for arbitrary sensor configurations. Code is available at https://github.com/ika-rwth-aachen/DEviLOG.;Data-Driven Occupancy Grid Mapping using Synthetic and Real-World Data;Not health related;Not health related;0
"C. Brander; C. Cioflan; V. Niculescu; H. Müller; T. Polonelli; M. Magno; L. Benini";2023;Deep Learning algorithms and models greatly benefit from the release of large-scale datasets, also including synthetically generated data, when real-life data is scarce. Multimodal datasets feature more descriptive environmental information than single-sensor ones, but they are generally small and not widely accessible. In this paper, we construct a synthetically-generated image classification dataset consisting of grayscale camera images and depth information acquired from an 8_8-pixel Time-of-Flight sensor. We propose and evaluate six Convolutional Neural Network-based feature-level fusion models to integrate the multimodal data, outperforming the accuracy of the camera-only model by up to 17% in real-world settings. By pretraining the model on synthetically-generated sample pairs, followed by fine-tuning it with only 16 real-domain samples, we outperform a non-pretrained counterpart by 35% while maintaining the storage constraints in the order of hundreds of kB. Our proposed convolutional model, pretrained on both synthetic and real-world sensor data, achieves a top-1 accuracy of 86.48%, proving the benefits of using multimodal datasets to train feature-level data fusion neural networks. Low-power emerging embedded microcontrollers, such as multi-core RISC-V systems-on-chip, are perfect candidates for running our model due to their reduced power consumption and parallel computing capabilities that speed up inference.;Improving Data-Scarce Image Classification Through Multimodal Synthetic Data Pretraining;Not health related;Not health related;0
"M. V. Phanindra Kumar; A. Thayyilravi; R. Karthika; C. R. Manoj; R. Kumar Soni";2023;As autonomous driving technology advances, the need for 3D LiDAR point cloud data has grown, particularly for handling unpredictable daily travel scenarios. However, it typically requires large annotated datasets for training, posing challenges in the 3D semantic segmentation models which is time-consuming and expensive. To address this, synthetic data from simulators is considered, but it lacks realism compared to actual LiDAR data. This study introduces a method that translates synthetic point clouds to closely resemble real data, enabling their use in real-time applications with limited actual LiDAR data. The proposed method combines Point Cloud Translation (PCT) and Domain Adaptation (DA) techniques, along with the state-of-the-art Cylinder3D segmentation network for 19 classes. Along with the above generation, comparative evaluations of various mixing techniques on synthetic and domain-translated data determine the optimal ratios for 3D semantic segmentation when combined with real data. SynLiDAR serves as the synthetic source data, and Semantic Kitti serves as the target data. By combining synthetic and domain-translated data with real data, the proposed model achieves mean Intersection over Union (mIOU) scores of 63.954 and 64.578, surpassing the mIOU of real data alone using the Cylinder3D segmentation network.;An Approach to Translate Synthetic Lidar Point Cloud Data Similar to Real Lidar Data and Evaluate Using Deep Learning Techniques;Not health related;Not health related;0
"C. Pandey; V. Tiwari; J. J. P. C. Rodrigues; D. S. Roy";2024;In an era of 5 G smart cities, precise traffic prediction remains elusive due to limited real-world data. Our paper introduces a novel approach using Generative Adversarial Networks (GANs) to create synthetic traffic data that closely mimics real-world statistics. This artificial dataset enhances our new 5GT-GAN-NET-based prediction model. The result is a significant boost in prediction accuracy, with Mean Square Error (MSE) reduced to 0.000346 and Mean Absolute Error (MAE) to 0.00685. Compared to benchmarks, our model improves MSE and MAE by up to 95.45% and 87.31%, respectively. User privacy remains a cornerstone of our approach, crucial for smart city applications. Our predictive capabilities enable more efficient resource allocation by service providers, increasing communication infrastructure reliability. Although tailored for smart cities, the approach is adaptable to other fields facing data scarcity and privacy concerns. Our research highlights the potential of GANs in generating large, accurate datasets for traffic prediction in 5 G environments while prioritizing user privacy.;5GT-GAN-NET: Internet Traffic Data Forecasting With Supervised Loss Based Synthetic Data Over 5 G;Not health related;Not health related;0
"S. Luo; D. Chen; C. Wang";2023;Data-free knowledge distillation (DFKD) aims to obtain a lightweight student model without original training data. Existing works generally synthesize data from the pretrained teacher model to replace the original training data for student learning. To more effectively train the student model, the synthetic data shall be customized to the current student learning ability. However, this is ignored in the existing DFKD methods and thus negatively affects the student training. To address this issue, we propose Customizing Synthetic Data for Data-Free Student Learning (CSD) in this paper, which achieves adaptive data synthesis using a self-supervised augmented auxiliary task to estimate the student learning ability. That is, data synthesis is dynamically adjusted to enlarge the cross entropy between the labels and the predictions from the self-supervised augmented task, thus generating the hard samples for the student model. The experiments on various datasets and teacher-student models show the effectiveness of our proposed method. Code is available at: https://github.com/luoshiya/CSD;Customizing Synthetic Data for Data-Free Student Learning;Not health related;Not health related;0
"D. Peek; M. P. Skerritt; S. Chalup";2023;This research uses deep learning to estimate the topology of manifolds represented by sparse, unordered point cloud scenes in 3D. A new labelled dataset was synthesised to train neural networks and evaluate their ability to estimate the genus of these manifolds. This data used random homeomorphic deformations to provoke the learning of visual topological features. We demonstrate that deep learning models could extract these features and discuss some advantages over existing topological data analysis tools that are based on persistent homology. Semantic segmentation was used to provide additional geometric information in conjunction with topological labels. Common point cloud multi-layer perceptron and transformer networks were both used to compare the viability of these methods. The experimental results of this pilot study support the hypothesis that, with the aid of sophisticated synthetic data generation, neural networks can perform segmentation-based topological data analysis. While our study focused on simulated data, the accuracy achieved suggests a potential for future applications using real data.;Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data;Not health related;Not health related;0
"S. Shetty; A. V.S.; A. Mahale";2023;Radiology is a field of medicine dealing with diagnostic images to detect diseases for further treatment. Collecting and annotating diagnostic images like Magnetic Resonance Imaging (MRI) and X-Ray is a rigorous and time-consuming process. Deep Learning methods are widely utilized for disease classification and prediction from diagnostic images, but they demand substantial amounts of training data. Additionally, certain diseases are uncommon in large patient cohorts, posing difficulties in obtaining sufficient imaging samples to construct accurate deep learning models. Data augmentation techniques are commonly used to overcome this challenge of limited data. These techniques involve applying geometric transformations such as rotation, cropping, zooming, flipping, and other similar operations to the images to enlarge the dataset artificially. Another possible way of expanding the dataset is by synthesizing data to generate artificial medical images by mimicking the original images. This study presents RAD-DCGAN: A Deep Convolutional Generative Adversarial Network to produce high-resolution synthetic radiology images from the X-ray and MRI images collected from a private medical hospital (KMC Hospital, India). We aim to determine the most effective technique for enhancing the performance of radiology image classifiers by comparing and evaluating the proposed RAD-DCGAN with the standard data augmentation strategy. Our empirical evaluation, which involved eight standard deep learning models, demonstrated that deep learning classifiers trained on synthetic radiology data outperformed those trained on standard augmented data. The utilization of the RAD-DCGAN model for training and testing deep learning models on synthetic data has demonstrated a notable improvement of 4-5% in accuracy compared to conventional augmentation techniques. This signifies the state-of-the-art performance achieved by the RAD-DCGAN model in enhancing the accuracy of deep learning models.;Data Augmentation vs. Synthetic Data Generation: An Empirical Evaluation for Enhancing Radiology Image Classification;health related;Not health related;1
"A. X. Wang; S. S. Chukova; B. P. Nguyen";2023;Churn prediction is a critical operation for many businesses because acquiring new clients often costs more than retaining existing ones. Therefore, being able to detect customer churn early and take marketing actions based on artificial intelligence (AI) systems is imperative for businesses. In general, algorithms and data are two integral components of any AI system. Therefore, the research of AI systems can be classified into two groups: model-centric AI and data-centric AI. While model-centric AI is developed to improve the performance of specific models, data-centric AI aims to improve the quality of the data for downstream machine learning tasks. During model development, while the model-centric approach is to use the same data and iterate on the model hyper-parameters, architecture, and other configurations, the data-centric approach is to improve existing data or integrate new data, then train and evaluate the machine learning algorithms. To the best of our knowledge, no previous research attempts to make a comparative study from a data-centric AI perspective for churn prediction. Therefore, to fill the gap, this study presents a comparative study of the most widely used data synthesis algorithms with different data strategies on the problem of churn prediction. The main focus of this study is to investigate whether we can improve churn prediction by substituting, balancing, and augmenting real data with data synthesis. The main goal of this study is to analyse and benchmark the best data-centric resampling methods for churn prediction. We expect that our study will shed some light on future projects and in other domains.;Data-Centric AI to Improve Churn Prediction with Synthetic Data;Not health related;Not health related;0
"E. Davarci; E. Anarim";2023;Smart devices equipped with various sensors enable the acquisition of users’ behavioral biometrics. These sensor data capture variations in users’ interactions with the devices, which can be analyzed to extract valuable information such as user activity, age group, and gender. In this study, we investigate the feasibility of using gait data for gender detection of users. To achieve this, we propose a novel gender detection scheme based on a deep learning approach, incorporating synthetic data generation and continuous wavelet transform (CWT). In this scheme, the real dataset is first divided into training and test datasets, and then synthetic data are intelligently generated using various techniques to augment the existing training data. Subsequently, CWT is used as the feature extraction module, and its outputs are fed into a deep learning model to detect the gender of users. Different deep learning models, including convolutional neural network (CNN) and long short-term memory (LSTM), are employed in classification. Consequently, we evaluate our proposed framework on different publicly available datasets. On the BOUN Sensor dataset, we obtain an accuracy of 94.83%, marking a substantial 6.5% enhancement over the prior highest rate of 88.33%. Additionally, we achieve 86.27% and 88.15% accuracy on the OU-ISIR Android and OU-ISIR Center IMUZ datasets, respectively. Our experimental results demonstrate that our proposed model achieves high detection rates and outperforms previous methods across all datasets.;Gender Detection Based on Gait Data: A Deep Learning Approach With Synthetic Data Generation and Continuous Wavelet Transform;Not health related;Not health related;0
"R. D. Santos; J. Aguilar; M. D. R-Moreno";2022;"In a smart environment like the smart grids, it is necessary to have knowledge models that allow solving emerging problems. However, large datasets are required to automatically create these models, which in the vast majority of cases are not available. Therefore, in these environments is essential to have a data generator for each context. In this paper, we propose a synthetic data generation system based on the variational autoencoder (VAE) technique and linked data paradigm, to create larger datasets from small datasets acting as samples. Specifically, a Linked Data-based dataset extractor is proposed, which allows obtaining samples of data in a particular context. Then, a VAE is trained with these samples of data available in a specific context, to learn the latent distribution that characterizes the dataset, allowing the production of new records that are similar to those of the original dataset. Finally, several case studies in the field of energy management are carried out. In one of them, the process that follows our approach is described in detail; and then, another 3 more are considered to evaluate its ability to automate the data generation process for smart energy management. The results show how our synthetic data generation system can be used to obtain synthetic datasets in different energy contexts.";A synthetic Data Generator for Smart Grids based on the Variational-Autoencoder Technique and Linked Data Paradigm;Not health related;Not health related;0
"A. Gupta; A. Vedaldi; A. Zisserman";2016;In this paper we introduce a new method for text detection in natural images. The method comprises two contributions: First, a fast and scalable engine to generate synthetic images of text in clutter. This engine overlays synthetic text to existing background images in a natural way, accounting for the local 3D scene geometry. Second, we use the synthetic images to train a Fully-Convolutional Regression Network (FCRN) which efficiently performs text detection and bounding-box regression at all locations and multiple scales in an image. We discuss the relation of FCRN to the recently-introduced YOLO detector, as well as other end-to-end object detection systems based on deep learning. The resulting detection network significantly out performs current methods for text detection in natural images, achieving an F-measure of 84.2% on the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per second on a GPU.;Synthetic Data for Text Localisation in Natural Images;Not health related;Not health related;0
"J. Tremblay; A. Prakash; D. Acuna; M. Brophy; V. Jampani; C. Anil; T. To; E. Cameracci; S. Boochoon; S. Birchfield";2018;We present a system for training deep neural networks for object detection using synthetic images. To handle the variability in real-world data, the system relies upon the technique of domain randomization, in which the parameters of the simulator-such as lighting, pose, object textures, etc.-are randomized in non-realistic ways to force the neural network to learn the essential features of the object of interest. We explore the importance of these parameters, showing that it is possible to produce a network with compelling performance using only non-artistically-generated synthetic data. With additional fine-tuning on real data, the network yields better performance than using real data alone. This result opens up the possibility of using inexpensive synthetic data for training neural networks while avoiding the need to collect large amounts of hand-annotated real-world data or to generate high-fidelity synthetic worlds-both of which remain bottlenecks for many applications. The approach is evaluated on bounding box detection of cars on the KITTI dataset.;Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization;Not health related;Not health related;0
"X. Wang; L. Xie; C. Dong; Y. Shan";2021;Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly.;Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data;Not health related;Not health related;0
"Q. Wang; J. Gao; W. Lin; Y. Yuan";2019;"Recently, counting the number of people for crowd scenes is a hot topic because of its widespread applications (e.g. video surveillance, public security). It is a difficult task in the wild: changeable environment, large-range number of people cause the current methods can not work well. In addition, due to the scarce data, many methods suffer from over-fitting to a different extent. To remedy the above two problems, firstly, we develop a data collector and labeler, which can generate the synthetic crowd scenes and simultaneously annotate them without any manpower. Based on it, we build a large-scale, diverse synthetic dataset. Secondly, we propose two schemes that exploit the synthetic data to boost the performance of crowd counting in the wild: 1) pretrain a crowd counter on the synthetic data, then finetune it using the real data, which significantly prompts the model's performance on real data; 2) propose a crowd counting method via domain adaptation, which can free humans from heavy data annotations. Extensive experiments show that the first method achieves the state-of-the-art performance on four real datasets, and the second outperforms our baselines. The dataset and source code are available at https://gjy3035.github.io/GCC-CL/.";Learning From Synthetic Data for Crowd Counting in the Wild;Not health related;Not health related;0
"S. Sankaranarayanan; Y. Balaji; A. Jain; S. N. Lim; R. Chellappa";2018;Visual Domain Adaptation is a problem of immense importance in computer vision. Previous approaches showcase the inability of even deep neural networks to learn informative representations across domain shift. This problem is more severe for tasks where acquiring hand labeled data is extremely hard and tedious. In this work, we focus on adapting the representations learned by segmentation networks across synthetic and real domains. Contrary to previous approaches that use a simple adversarial objective or superpixel information to aid the process, we propose an approach based on Generative Adversarial Networks (GANs) that brings the embeddings closer in the learned feature space. To showcase the generality and scalability of our approach, we show that we can achieve state of the art results on two challenging scenarios of synthetic to real domain adaptation. Additional exploratory experiments show that our approach: (1) generalizes to unseen domains and (2) results in improved alignment of source and target distributions.;Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation;Not health related;Not health related;0
"E. Richardson; M. Sela; R. Kimmel";2016;Fast and robust three-dimensional reconstruction of facial geometric structure from a single image is a challenging task with numerous applications. Here, we introduce a learning-based approach for reconstructing a three-dimensional face from a single image. Recent face recovery methods rely on accurate localization of key characteristic points. In contrast, the proposed approach is based on a Convolutional-Neural-Network (CNN) which extracts the face geometry directly from its image. Although such deep architectures outperform other models in complex computer vision problems, training them properly requires a large dataset of annotated examples. In the case of three-dimensional faces, currently, there are no large volume data sets, while acquiring such big-data is a tedious task. As an alternative, we propose to generate random, yet nearly photo-realistic, facial images for which the geometric form is known. The suggested model successfully recovers facial shapes from real images, even for faces with extreme expressions and under various lighting conditions.;3D Face Reconstruction by Learning from Synthetic Data;Not health related;Not health related;0
"N. Patki; R. Wedge; K. Veeramachaneni";2016;The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault (SDV), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name SDV. When implementing the SDV, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a state-of-the-art multivariate modeling approach to model this data. The SDV iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the SDV to synthesize data by sampling from any part of the database. After building the SDV, we used it to generate synthetic data for five different publicly available datasets. We then published these datasets, and asked data scientists to develop predictive models for them as part of a crowdsourced experiment. By analyzing the outcomes, we show that synthetic data can successfully replace original data for data science. Our analysis indicates that there is no significant difference in the work produced by data scientists who used synthetic data as opposed to real data. We conclude that the SDV is a viable solution for synthetic data generation.;The Synthetic Data Vault;Not health related;Not health related;0
"A. Atapour-Abarghouei; T. P. Breckon";2018;Monocular depth estimation using learning-based approaches has become promising in recent years. However, most monocular depth estimators either need to rely on large quantities of ground truth depth data, which is extremely expensive and difficult to obtain, or predict disparity as an intermediary step using a secondary supervisory signal leading to blurring and other artefacts. Training a depth estimation model using pixel-perfect synthetic data can resolve most of these issues but introduces the problem of domain bias. This is the inability to apply a model trained on synthetic data to real-world scenarios. With advances in image style transfer and its connections with domain adaptation (Maximum Mean Discrepancy), we take advantage of style transfer and adversarial training to predict pixel perfect depth from a single real-world color image based on training over a large corpus of synthetic environment data. Experimental results indicate the efficacy of our approach compared to contemporary state-of-the-art techniques.;Real-Time Monocular Depth Estimation Using Synthetic Data with Domain Adaptation via Image Style Transfer;Not health related;Not health related;0
"J. A. Jensen; O. Holm; L. J. Jerisen; H. Bendsen; S. I. Nikolov; B. G. Tomov; P. Munk; M. Hansen; K. Salomonsen; J. Hansen; K. Gormsen; H. M. Pedersen; K. L. Gammelmark";2005;Conventional ultrasound systems acquire ultrasound data sequentially one image line at a time. The architecture of these systems is therefore also sequential in nature and processes most of the data in a sequential pipeline. This often makes it difficult to implement radically different imaging strategies on the platforms and makes the scanners less accessible for research purposes. A system designed for imaging research flexibility is the prime concern. The possibility of sending out arbitrary signals and the storage of data from all transducer elements for 5 to 10 seconds allows clinical evaluation of synthetic aperture and 3D imaging. This paper describes a real-time system specifically designed for research purposes. The system can acquire multichannel data in real-time from multi-element ultrasound transducers, and can perform some real-time processing on the acquired data. The system is capable of performing real-time beamforming for conventional imaging methods using linear, phased, and convex arrays. Image acquisition modes can be intermixed, and this makes it possible to perform initial trials in a clinical environment with new imaging modalities for synthetic aperture imaging, 2D and 3D B-mode, and velocity imaging using advanced coded emissions. The system can be used with 128-element transducers and can excite 128 transducer elements and receive and sample data from 64 channels simultaneously at 40 MHz with 12-bit precision. Two-to-one multiplexing in receive can be used to cover 128 receive channels. Data can be beamformed in real time using the system's 80 signal processing units, or it can be stored directly in RAM. The system has 16 Gbytes RAM and can, thus, store more than 3.4 seconds of multichannel data. It is fully software programmable and its signal processing units can also be reconfigured under software control. The control of the system is done over a 100-Mbits/s Ethernet using C and Matlab. Programs for doing, e.g., B-mode imaging can be written directly in Matlab and executed on the system over the net from any workstation running Matlab. The overall system concept is presented along with its implementation and examples of B-mode and in vivo synthetic aperture flow imaging.;Ultrasound research scanner for real-time synthetic aperture data acquisition;health related;Not health related;1
"O. Frey; C. Magnard; M. Ruegg; E. Meier";2009;Standard focusing of data from synthetic aperture radar (SAR) assumes a straight recording track of the sensor platform. Small nonlinearities of airborne platform tracks are corrected for during a motion-compensation step while maintaining the assumption of a linear flight path. This paper describes the processing of SAR data acquired from nonlinear tracks, typical of sensors mounted on small aircraft or drones flying at low altitude. Such aircraft do not fly along straight tracks, but the trajectory depends on topography, influences of weather and wind, or the shape of areas of interest such as rivers or traffic routes. Two potential approaches for processing SAR data from such highly nonlinear flight tracks are proposed, namely, a patchwise frequency-domain processing and mosaicking technique and a time-domain back-projection-based technique. Both are evaluated with the help of experimental data featuring tracks with altitude changes, a double bend, a 90deg curve, and a linear flight track. In order to assess the quality of the focused data, close-ups of amplitude images are compared, impulse response functions of a point target are analyzed, and the coherence is evaluated. The experimental data were acquired by the German Aerospace Center's E-SAR L-band system.;Focusing of Airborne Synthetic Aperture Radar Data From Highly Nonlinear Flight Tracks;Not health related;Not health related;0
"Jiancheng Shi; J. Dozier";1995;In hydrological investigations, modeling and forecasting of snow melt runoff require timely information about spatial variability of snow properties, among them the liquid water content-snow wetness-in the top layer of a snow pack. The authors' polarimetric model shows that scattering mechanisms control the relationship between snow wetness and the copolarization signals in data from a multi-parameter synthetic aperture radar. Along with snow wetness, the surface roughness and local incidence angle also affect the copolarization signals, making them either larger or smaller depending on the snow parameters, surface roughness, and incidence angle. The authors base their algorithm for retrieving snow wetness from SIR-C/X-SAR on a first-order scattering model that includes both surface and volume scattering. It is applicable for incidence angles from 25/spl deg/-70/spl deg/ and for surface roughness with rms height /spl les/7 mm and correlation length /spl les/25 cm. Comparison with ground measurements showed that the absolute error in snow wetness inferred from the imagery was within 2.5% at 95% confidence interval. Typically the free liquid water content of snow ranges from 0% to 15% by volume. The authors conclude that a C-band polarimetric SAR can provide useful estimates of the wetness of the top layers of seasonal snow packs.<>;Inferring snow wetness using C-band data from SIR-C's polarimetric synthetic aperture radar;Not health related;Not health related;0
"J. Liu; F. Qu; X. Hong; H. Zhang";2019;The limited fault information caused by small fault data samples is a major problem in wind turbine (WT) fault detection. This paper proposes a small-sample WT fault detection method with the synthetic fault data using generative adversarial nets (GANs). First, based on prior knowledge, a rough fault data generation process is developed to transform the normal data to the rough fault data. Second, a rough fault data refiner is developed by GANs to make the rough fault data more similar with the real fault data. Moreover, to make the generated data better suited to the WT conditions, GANs are improved in both the generative model and the discriminative model. Third, artificial intelligence (AI)-based WT fault detection models can be well trained by using only the generated data in the condition of small fault data sample. Finally, three groups of generated data evaluation experiments and four groups of WT fault detection comparative experiments are conducted using real WT data collected from a wind farm in northern China. The results indicate that the method proposed in this paper is effective.;A Small-Sample Wind Turbine Fault Detection Method With Synthetic Fault Data Using Generative Adversarial Nets;Not health related;Not health related;0
"Y. Chen; W. Li; X. Chen; L. Van Gool";2019;"As an alternative to manual pixel-wise annotation, synthetic data has been increasingly used for training semantic segmentation models. Such synthetic images and semantic labels can be easily generated from virtual 3D environments. In this work, we propose an approach to cross-domain semantic segmentation with the auxiliary geometric information, which can also be easily obtained from virtual environments. The geometric information is utilized on two levels for reducing domain shift: on the input level, we augment the standard image translation network with the geometric information to translate synthetic images into realistic style; on the output level, we build a task network which simultaneously performs semantic segmentation and depth estimation. Meanwhile, adversarial training is applied on the joint output space to preserve the correlation between semantics and depth. The proposed approach is validated on two pairs of synthetic to real dataset: from Virtual KITTI to KITTI, and from SYNTHIA to Cityscapes, where we achieve a clear performance gain compared to the baselines and various competing methods, demonstrating the effectiveness of the geometric information for cross-domain semantic segmentation.";Learning Semantic Segmentation From Synthetic Data: A Geometrically Guided Input-Output Adaptation Approach;Not health related;Not health related;0
"E. Rignot; R. Chellappa";1992;A statistical image model is proposed for segmenting polarimetric synthetic aperture radar (SAR) data into regions of homogeneous and similar polarimetric backscatter characteristics. A model for the conditional distribution of the polarimetric complex data is combined with a Markov random field representation for the distribution of the region labels to obtain the posterior distribution. Optimal region labeling of the data is then defined as maximizing the posterior distribution of the region labels given the polarimetric SAR complex data (maximum a posteriori (MAP) estimate). Two procedures for selecting the characteristics of the regions are then discussed. Results using real multilook polarimetric SAR complex data are given to illustrate the potential of the two selection procedures and evaluate the performance of the MAP segmentation technique. It is also shown that dual polarization SAR data can yield segmentation resultS similar to those obtained with fully polarimetric SAR data.<>;Segmentation of polarimetric synthetic aperture radar data;Not health related;Not health related;0
"M. Danielczuk; M. Matl; S. Gupta; A. Li; A. Lee; J. Mahler; K. Goldberg";2019;The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive hand-labeled datasets are available. As generating these datasets is time-consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any hand-labeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15% in Average Precision and 20% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at https://bit.ly/2letCuE.;Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data;Not health related;Not health related;0
"Z. Tang; M. Naphade; S. Birchfield; J. Tremblay; W. Hodge; R. Kumar; S. Wang; X. Yang";2019;In comparison with person re-identification (ReID), which has been widely studied in the research community, vehicle ReID has received less attention. Vehicle ReID is challenging due to 1) high intra-class variability (caused by the dependency of shape and appearance on viewpoint), and 2) small inter-class variability (caused by the similarity in shape and appearance between vehicles produced by different manufacturers). To address these challenges, we propose a Pose-Aware Multi-Task Re-Identification (PAMTRI) framework. This approach includes two innovations compared with previous methods. First, it overcomes viewpoint-dependency by explicitly reasoning about vehicle pose and shape via keypoints, heatmaps and segments from pose estimation. Second, it jointly classifies semantic vehicle attributes (colors and types) while performing ReID, through multi-task learning with the embedded pose representations. Since manually labeling images with detailed pose and attribute information is prohibitive, we create a large-scale highly randomized synthetic dataset with automatically annotated vehicle attributes for training. Extensive experiments validate the effectiveness of each proposed component, showing that PAMTRI achieves significant improvement over state-of-the-art on two mainstream vehicle ReID benchmarks: VeRi and CityFlow-ReID.;PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data;Not health related;Not health related;0
"A. Prakash; S. Boochoon; M. Brophy; D. Acuna; E. Cameracci; G. State; O. Shapira; S. Birchfield";2019;We present structured domain randomization (SDR), a variant of domain randomization (DR) that takes into account the structure of the scene in order to add context to the generated data. In contrast to DR, which places objects and distractors randomly according to a uniform probability distribution, SDR places objects and distractors randomly according to probability distributions that arise from the specific problem at hand. In this manner, SDR-generated imagery enables the neural network to take the context around an object into consideration during detection. We demonstrate the power of SDR for the problem of 2D bounding box car detection, achieving competitive results on real data after training only on synthetic data. On the KITTI easy, moderate, and hard tasks, we show that SDR outperforms other approaches to generating synthetic data (VKITTI, Sim 200k, or DR), as well as real data collected in a different domain (BDD100K). Moreover, synthetic SDR data combined with real KITTI data outperforms real KITTI data alone.;Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data;Not health related;Not health related;0
"L. Zhang; A. Gonzalez-Garcia; J. van de Weijer; M. Danelljan; F. S. Khan";2019;The usage of both off-the-shelf and end-to-end trained deep networks have significantly improved the performance of visual tracking on RGB videos. However, the lack of large labeled datasets hampers the usage of convolutional neural networks for tracking in thermal infrared (TIR) images. Therefore, most state-of-the-art methods on tracking for TIR data are still based on handcrafted features. To address this problem, we propose to use image-to-image translation models. These models allow us to translate the abundantly available labeled RGB data to synthetic TIR data. We explore both the usage of paired and unpaired image translation models for this purpose. These methods provide us with a large labeled dataset of synthetic TIR sequences, on which we can train end-to-end optimal features for tracking. To the best of our knowledge, we are the first to train end-to-end features for TIR tracking. We perform extensive experiments on the VOT-TIR2017 dataset. We show that a network trained on a large dataset of synthetic TIR data obtains better performance than one trained on the available real TIR data. Combining both data sources leads to further improvement. In addition, when we combine the network with motion features, we outperform the state of the art with a relative gain of over 10%, clearly showing the efficiency of using synthetic data to train end-to-end TIR trackers.;Synthetic Data Generation for End-to-End Thermal Infrared Tracking;Not health related;Not health related;0
"L. Giustarini; R. Hostache; D. Kavetski; M. Chini; G. Corato; S. Schlaffer; P. Matgen";2016;Probabilistic flood mapping offers flood managers, decision makers, insurance agencies, and humanitarian relief organizations a useful characterization of uncertainty in flood mapping delineation. Probabilistic flood maps are also of high interest for data assimilation into numerical models. The direct assimilation of probabilistic flood maps into hydrodynamic models would be beneficial because it would eliminate the intermediate step of having to extract water levels first. This paper introduces a probabilistic flood mapping procedure based on synthetic aperture radar (SAR) data. Given a SAR image of backscatter values, we construct a total histogram of backscatter values and decompose this histogram into probability distribution functions of backscatter values associated with flooded (open water) and non-flooded pixels, respectively. These distributions are then used to estimate, for each pixel, its probability of being flooded. The new approach improves on binary SAR-based flood mapping procedures, which do not inform on the uncertainty in the pixel state. The proposed approach is tested using four SAR images from two floodplains, i.e., the Severn River (U.K.) and the Red River (U.S.). In all four test cases, reliability diagrams, with error values ranging from 0.04 to 0.23, indicate a good agreement between the SAR-derived probabilistic flood map and an independently available validation map, which is obtained from aerial photography.;Probabilistic Flood Mapping Using Synthetic Aperture Radar Data;Not health related;Not health related;0
"A. Handa; V. Patraucean; V. Badrinarayanan; S. Stent; R. Cipolla";2016;Scene understanding is a prerequisite to many high level tasks for any automated intelligent machine operating in real world environments. Recent attempts with supervised learning have shown promise in this direction but also highlighted the need for enormous quantity of supervised data- performance increases in proportion to the amount of data used. However, this quickly becomes prohibitive when considering the manual labour needed to collect such data. In this work, we focus our attention on depth based semantic per-pixel labelling as a scene understanding problem and show the potential of computer graphics to generate virtually unlimited labelled data from synthetic 3D scenes. By carefully synthesizing training data with appropriate noise models we show comparable performance to state-of-the-art RGBD systems on NYUv2 dataset despite using only depth data as input and set a benchmark on depth-based segmentation on SUN RGB-D dataset.;Understanding RealWorld Indoor Scenes with Synthetic Data;Not health related;Not health related;0
"F. J. Meyer; J. B. Nicoll; A. P. Doulgeris";2013;Radio frequency interference (RFI) is a known issue in low-frequency radar remote sensing. In synthetic aperture radar (SAR) image processing, RFI can cause severe degradation of image quality, distortion of polarimetric signatures, and an increase of the SAR phase noise level. To address this issue, a processing system was developed that is capable of reliably detecting, characterizing, and mitigating RFI signatures in SAR observations. In addition to being the basis for image correction, the robust RFI-detection algorithms developed in this paper are used to retrieve a wealth of RFI-related information that allows for mapping, characterizing, and classifying RFI signatures across large spatial scales. The extracted RFI information is expected to be valuable input for SAR-system design, sensor operations, and the development of effective RFI-mitigation strategies. The concepts of RFI detection, analysis, and mapping are outlined. Large-scale RFI mapping results are shown. In case studies, the benefit of detailed RFI information for customized RFI filtering and sensor operations is exemplified.;Correction and Characterization of Radio Frequency Interference Signatures in L-Band Synthetic Aperture Radar Data;Not health related;Not health related;0
"C. Dewi; R. -C. Chen; Y. -T. Liu; X. Jiang; K. D. Hartomo";2021;Convolutional Neural Networks (CNN) achieves perfection in traffic sign identification with enough annotated training data. The dataset determines the quality of the complete visual system based on CNN. Unfortunately, databases for traffic signs from the majority of the world's nations are few. In this scenario, Generative Adversarial Networks (GAN) may be employed to produce more realistic and varied training pictures to supplement the actual arrangement of images. The purpose of this research is to describe how the quality of synthetic pictures created by DCGAN, LSGAN, and WGAN is determined. Our work combines synthetic images with original images to enhance datasets and verify the effectiveness of synthetic datasets. We use different numbers and sizes of images for training. Likewise, the Structural Similarity Index (SSIM) and Mean Square Error (MSE) were employed to assess picture quality. Our study quantifies the SSIM difference between the synthetic and actual images. When additional images are used for training, the synthetic image exhibits a high degree of resemblance to the genuine image. The highest SSIM value was achieved when using 200 total images as input and 32_32 image size. Further, we augment the original picture dataset with synthetic pictures and compare the original image model to the synthesis image model. For this experiment, we are using the latest iterations of Yolo, Yolo V3, and Yolo V4. After mixing the real image with the synthesized image produced by LSGAN, the recognition performance has been improved, achieving an accuracy of 84.9% on Yolo V3 and an accuracy of 89.33% on Yolo V4.;Yolo V4 for Advanced Traffic Sign Recognition With Synthetic Training Data Generated by Various GAN;Not health related;Not health related;1
"E. Wood; T. Baltru_aitis; C. Hewitt; S. Dziadzio; T. J. Cashman; J. Shotton";2021;We demonstrate that it is possible to perform face-related computer vision in the wild using synthetic data alone. The community has long enjoyed the benefits of synthesizing training data with graphics, but the domain gap between real and synthetic data has remained a problem, especially for human faces. Researchers have tried to bridge this gap with data mixing, domain adaptation, and domain-adversarial training, but we show that it is possible to synthesize data with minimal domain gap, so that models trained on synthetic data generalize to real in-the-wild datasets. We describe how to combine a procedurally-generated parametric 3D face model with a comprehensive library of hand-crafted assets to render training images with unprecedented realism and diversity. We train machine learning systems for face-related tasks such as landmark localization and face parsing, showing that synthetic data can both match real data in accuracy as well as open up new approaches where manual labeling would be impossible.;Fake it till you make it: face analysis in the wild using synthetic data alone;Not health related;Not health related;0
"S. De; L. Bruzzone; A. Bhattacharya; F. Bovolo; S. Chaudhuri";2018;The classification of urban areas in polarimetric synthetic aperture radar (PolSAR) data is a challenging task. Moreover, urban structures oriented away from the radar line of sight pose an additional complexity in the classification process. The characterization of such areas is important for disaster relief and urban sprawl monitoring applications. In this paper, a novel technique based on deep learning is proposed, which leverages a synthetic target database for data augmentation. The PolSAR dataset is rotated by uniform steps and collated to form a reference database. A stacked autoencoder network is used to transform the information in the augmented dataset into a compact representation. This significantly improves the generalization capabilities of the network. Finally, the classification is performed by a multilayer perceptron network. The modular architecture allows for easy optimization of the hyperparameters. The synthetic target database is created and the classification performance is evaluated on an Lband airborne UAVSAR dataset and L-band space-borne ALOS-2 dataset acquired over San Francisco, USA. The proposed technique shows an overall accuracy of 91.3%. An improvement over state-of-the-art techniques is achieved, especially in urban areas rotated away from the radar line of sight.;A Novel Technique Based on Deep Learning and a Synthetic Target Database for Classification of Urban Areas in PolSAR Data;Not health related;Not health related;0
"M. W. Lang; E. S. Kasischke";2008;"Hydrology (i.e., inundation and soil moisture) is the most important abiotic factor controlling wetland function and extent, and scientists predict that wetland hydrology can be significantly altered over relatively short timescales due to climate change and anthropogenic impact. Whereas broadscale hydrology is difficult to monitor in forested wetlands with ground-based and optical remote sensing methods, C-band synthetic aperture radar (SAR) systems have the potential to improve the capability to monitor forested wetland hydrology. In this study, we examined the use of Environmental Satellite Advanced SAR (C-HH and C-VV) data for monitoring levels of inundation and soil moisture throughout the year in a typical Mid-Atlantic floodplain and some of the main limitations inherent to C-band data (i.e., polarization and plant phenology) in this environment. The relationships between the backscatter coefficient $\sigma^{0}$ and inundation, soil moisture, tree basal area, tree height, and forest canopy closure were examined. Significant differences in C-HH $\sigma^{0}$ were found between forested areas of varying hydrology (0%–60% area inundated) throughout the year and in C-VV $ \sigma^{0}$ during the leaf-off season. As expected, C-HH SAR backscatter was better correlated with inundation and soil moisture than was C-VV SAR backscatter, and the correlations between both polarizations of backscatter and hydrology were stronger during the leaf-off season (C-HH leaf-off  $r^{2} = 0.50$, leaf-on $r^{2} = 0.39$; C-VV leaf-off $r^{2} = 0.21$, leaf-on  $r^{2} = 0.19$; all significant at $p _ 0.0001$  level). Based on our findings, we concluded that the C-HH data are useful for monitoring hydrology beneath forest canopies throughout the year, whereas the C-VV data can be used during the leaf-off season. Our findings support previous studies that concluded that C-band imagery can be used to monitor forested wetland hydrology in large floodplains that are fully inundated. However, this study used detailed in situ measurements and demonstrated that C-band SAR data can also be used to monitor forested wetland hydrology in smaller partially inundated floodplains, which are more common in the Mid-Atlantic.";Using C-Band Synthetic Aperture Radar Data to Monitor Forested Wetland Hydrology in Maryland's Coastal Plain, USA;Not health related;Not health related;0
"M. Tao; F. Zhou; Z. Zhang";2016;Radio frequency interference is a major issue for synthetic aperture radar (SAR) imaging. Especially with the presence of wideband interference (WBI), the signal-to-interference ratio (SIR) of the measurements is greatly degraded, thus making it difficult to produce a high-quality SAR image. Compared with narrow-band interference (NBI), WBI occupies a larger bandwidth and is more complicated to deal with. This paper addresses the detection and mitigation of WBI in high-resolution airborne SAR data. First, a WBI-corrupted echo is characterized in the time-frequency representation by utilizing the short-time Fourier transform. In this way, the original range-spectrum WBI mitigation problem can be simplified into a series of instantaneous-spectrum NBI mitigation problems. For each instantaneous spectrum, the existence of interference signal can be identified according to the negentropy-based statistical test. Furthermore, the interference signal is mitigated by notch filtering or eigensubspace filtering. The experimental results of the simulated data, as well as real measured data sets, show that the proposed scheme is effective in suppressing the interference signal and in obtaining a high-quality image.;Wideband Interference Mitigation in High-Resolution Airborne Synthetic Aperture Radar Data;Not health related;Not health related;0
"M. H. Skjelvareid; T. Olofsson; Y. Birkelund; Y. Larsen";2011;The synthetic aperture focusing technique (SAFT) is used to create focused images from ultrasound scans. SAFT has traditionally been applied only for imaging in a single medium, but the recently introduced phase shift migration (PSM) algorithm has expanded the use of SAFT to multilayer structures. In this article we present a similar focusing algorithm called multi-layer omega-k (MULOK), which combines PSM and the _-k algorithm to perform multilayer imaging more efficiently. The asymptotic complexity is shown to be lower for MULOK than for PSM, and this is confirmed by comparing execution times for implementations of both algorithms. To facilitate the complexity analysis, a detailed description of algorithm implementation is included, which also serves as a guide for readers interested in practical implementation. Using data from an experiment with a multilayered structure, we show that there is essentially no difference in image quality between the two algorithms.;Synthetic aperture focusing of ultrasonic data from multilayered media using an omega-K algorithm;Not health related;Not health related;0
"B. Wen; C. Mitash; B. Ren; K. E. Bekris";2020;"Tracking the 6D pose of objects in video sequences is important for robot manipulation. This task, however, introduces multiple challenges: (i) robot manipulation involves significant occlusions; (ii) data and annotations are troublesome and difficult to collect for 6D poses, which complicates machine learning solutions, and (iii) incremental error drift often accumulates in long term tracking to necessitate re-initialization of the object's pose. This work proposes a data-driven optimization approach for long-term, 6D pose tracking. It aims to identify the optimal relative pose given the current RGB-D observation and a synthetic image conditioned on the previous best estimate and the object's model. The key contribution in this context is a novel neural network architecture, which appropriately disentangles the feature encoding to help reduce domain shift, and an effective 3D orientation representation via Lie Algebra. Consequently, even when the network is trained only with synthetic data can work effectively over real images. Comprehensive experiments over benchmarks - existing ones as well as a new dataset with significant occlusions related to object manipulation - show that the proposed approach achieves consistently robust estimates and outperforms alternatives, even though they have been trained with real images. The approach is also the most computationally efficient among the alternatives and achieves a tracking frequency of 90.9Hz.";se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains;Not health related;Not health related;0
"G. Ferrara; M. Migliaccio; F. Nunziata; A. Sorrentino";2011;An electromagnetic model, based on the generalized-K (GK) speckle distribution, has been developed to read the scattering features associated with metallic objects at sea in full resolution single-look complex (SLC) synthetic aperture radar (SAR) data. The sensitivity of the GK parameters is investigated in both copolarized and cross-polarized SAR data. It is shown that a proper combination of GK parameters exhibits a completely different behavior with respect to sea surface with and without metallic objects, when measured over cross-polarized SAR data. As a matter of fact, a simple and very effective approach to observe metallic objects in full resolution SLC cross-polarized SAR data, is proposed. Experiments undertaken over a large data set consisting of RADARSAT-2 SLC fine quad polarization SAR data, confirm model predictions, and show that the proposed approach is both physically-based and operationally effective, since it is able to provide a logical true and false output.;Generalized-K (GK)-Based Observation of Metallic Objects at Sea in Full-Resolution Synthetic Aperture Radar (SAR) Data: A Multipolarization Study;Not health related;Not health related;0
"S. Tripathi; S. Chandra; A. Agrawal; A. Tyagi; J. M. Rehg; V. Chari";2019;We present a task-specific approach to synthetic data generation. Our framework employs a trainable synthesizer network that is optimized to produce meaningful training samples by assessing the strengths and weaknesses of a `target' classifier. The synthesizer and target networks are trained in an adversarial manner wherein each network is updated with a goal to outdo the other. Additionally, we ensure the synthesizer generates realistic data by pairing it with a discriminator trained on real-world images. Further, to make the target classifier invariant to blending artefacts, we introduce these artefacts to background regions of the training images so the target does not over-fit to them. We demonstrate the efficacy of our approach by applying it to different target networks including a classification network on AffNIST [46], and two object detection networks (SSD, Faster-RCNN) on different datasets. On the AffNIST benchmark, our approach is able to surpass the baseline results with just half the training examples. On the VOC person detection benchmark, we show improvements of up to 2.7% as a result of our data augmentation. Similarly on the GMU detection benchmark, we report a performance boost of 3.5% in mAP over the baseline method, outperforming the previous state of the art approaches by as much as 7.5% in individual categories.;Learning to Generate Synthetic Data via Compositing;Not health related;Not health related;0
"R. Torkzadehmahani; P. Kairouz; B. Paten";2019;Generative Adversarial Networks (GANs) are one of the well-known models to generate synthetic data including images, especially for research communities that cannot use original sensitive datasets because they are not publicly accessible. One of the main challenges in this area is to preserve the privacy of individuals who participate in the training of the GAN models. To address this challenge, we introduce a Differentially Private Conditional GAN (DP-CGAN) training framework based on a new clipping and perturbation strategy, which improves the performance of the model while preserving privacy of the training dataset. DP-CGAN generates both synthetic data and corresponding labels and leverages the recently introduced Renyi differential privacy accountant to track the spent privacy budget. The experimental results show that DP-CGAN can generate visually and empirically promising results on the MNIST dataset with a single-digit epsilon parameter in differential privacy.;DP-CGAN: Differentially Private Synthetic Data and Label Generation;Not health related;Not health related;0
"A. Kortylewski; B. Egger; A. Schneider; T. Gerig; A. Morel-Forster; T. Vetter";2019;It is well known that deep learning approaches to face recognition suffer from various biases in the available training data. In this work, we demonstrate the large potential of synthetic data for analyzing and reducing the negative effects of dataset bias on deep face recognition systems. In particular we explore two complementary application areas for synthetic face images: 1) Using fully annotated synthetic face images we can study the face recognition rate as a function of interpretable parameters such as face pose. This enables us to systematically analyze the effect of different types of dataset biases on the generalization ability of neural network architectures. Our analysis reveals that deeper neural network architectures can generalize better to unseen face poses. Furthermore, our study shows that current neural network architectures cannot disentangle face pose and facial identity, which limits their generalization ability. 2) We pre-train neural networks with large-scale synthetic data that is highly variable in face pose and the number of facial identities. After a subsequent fine-tuning with real-world data, we observe that the damage of dataset bias in the real-world data is largely reduced. Furthermore, we demonstrate that the size of real-world datasets can be reduced by 75% while maintaining competitive face recognition performance. The data and software used in this work are publicly available.;Analyzing and Reducing the Damage of Dataset Bias to Face Recognition With Synthetic Data;Not health related;Not health related;0
"D. L. Evans; T. G. Farr; J. J. van Zyl";1992;Radar remote sensing data provide a unique perspective on the Earth's crust and the processes that have influenced its evolution. Physically based models are required, however, to relate the geophysical quantities being measured by the radar sensor to useful geologic information. Synthetic aperture radar (SAR) data over the Cima volcanic field in the Mojave Desert of California are quantitatively connected with microtopography through inversion of a radar backscatter model. Changes in surface roughness inferred from the derived microtopography are modeled and found to be consistent with aeolian mantling as surfaces age. Estimated rates of aeolian deposition for the Cima area are compared to the Lunar Crater volcanic field in Nevada. Rates of deposition appear to be higher at Cima volcanic field, most likely because of its proximity to Soda Lake, the main source of the aeolian material.<>;Estimates of surface roughness derived from synthetic aperture radar (SAR) data;Not health related;Not health related;0
"S. Caorsi; A. Massa; M. Pastorino; A. Randazzo";2003;Phaseless data are used to evaluate the application of an electromagnetic inverse-scattering-based procedure for the detection of cylindrical inhomogeneities, which are schematized as multilayer infinite dielectric cylinders with elliptic cross sections. The electromagnetic inverse problem is recast as a global optimization problem and iteratively solved by an efficient memetic algorithm, which combines deterministic and stochastic concepts. Moreover, a recursive analytical procedure is used for the forward-scattering computation. The possibility of localizing and reconstructing the scatterers by using phaseless input data, which would greatly simplify the design of the imaging apparatus, is evaluated both with reference to synthetically produced data and by means of experimental data obtained by a microwave tomograph.;Electromagnetic detection of dielectric scatterers using phaseless synthetic and real data and the memetic algorithm;Not health related;Not health related;0
"M. Fabbri; G. Brasó; G. Maugeri; O. Cetintas; R. Gasparini; A. O_ep; S. Calderara; L. Leal-Taixé; R. Cucchiara";2021;Deep learning-based methods for video pedestrian detection and tracking require large volumes of training data to achieve good performance. However, data acquisition in crowded public environments raises data privacy concerns – we are not allowed to simply record and store data without the explicit consent of all participants. Furthermore, the annotation of such data for computer vision applications usually requires a substantial amount of manual effort, especially in the video domain. Labeling instances of pedestrians in highly crowded scenarios can be challenging even for human annotators and may introduce errors in the training data. In this paper, we study how we can advance different aspects of multi-person tracking using solely synthetic data. To this end, we generate MOTSynth, a large, highly diverse synthetic dataset for object detection and tracking using a rendering game engine. Our experiments show that MOTSynth can be used as a replacement for real data on tasks such as pedestrian detection, re-identification, segmentation, and tracking.;MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?;Not health related;Not health related;0
"C. Zhang; S. R. Kuppannagari; R. Kannan; V. K. Prasanna";2018;The availability of fine grained time series data is a pre-requisite for research in smart-grids. While data for transmission systems is relatively easily obtainable, issues related to data collection, security and privacy hinder the widespread public availability/accessibility of such datasets at the distribution system level. This has prevented the larger research community from effectively applying sophisticated machine learning algorithms to significantly improve the distribution-level accuracy of predictions and increase the efficiency of grid operations. Synthetic dataset generation has proven to be a promising solution for addressing data availability issues in various domains such as computer vision, natural language processing and medicine. However, its exploration in the smart grid context remains unsatisfactory. Previous works have tried to generate synthetic datasets by modeling the underlying system dynamics: an approach which is difficult, time consuming, error prone and often times infeasible in many problems. In this work, we propose a novel data-driven approach to synthetic dataset generation by utilizing deep generative adversarial networks (GAN) to learn the conditional probability distribution of essential features in the real dataset and generate samples based on the learned distribution. To evaluate our synthetically generated dataset, we measure the maximum mean discrepancy (MMD) between real and synthetic datasets as probability distributions, and show that their sampling distance converges. To further validate our synthetic dataset, we perform common smart grid tasks such as k-means clustering and short-term prediction on both datasets. Experimental results show the efficacy of our synthetic dataset approach: the real and synthetic datasets are indistinguishable by solely examining the output of these tasks.;Generative Adversarial Network for Synthetic Time Series Data Generation in Smart Grids;Not health related;Not health related;0
"Z. Wan; Y. Zhang; H. He";2017;Discovering pattern from imbalanced data plays an important role in numerous applications, such as health service, cyber security, and financial engineering. However, the imbalanced data greatly compromise the performance of most learning algorithms. Recently, various synthetic sampling methods have been proposed to balance the dataset. Although these methods have achieved great success in many datasets, they are less effective for high-dimensional data, such as the image. In this paper, we propose a variational autoencoder (VAE) based synthetic data generation method for imbalanced learning. VAE can produce new samples which are similar to those in the original dataset, but not exactly the same. We evaluate and compare our proposed method with the traditional synthetic sampling methods on various datasets under five evaluation metrics. The experimental results demonstrate the effectiveness of the proposed method.;Variational autoencoder based synthetic data generation for imbalanced learning;Not health related;Not health related;0
"D. W. Wilson; H. H. Barrett; E. W. Clarkson";2000;A novel SPECT collimation method, termed the synthetic collimator, is proposed. The synthetic collimator employs a multiple-pinhole aperture and a high-resolution detector. The problem of multiplexing, normally associated with multiple pinholes, is reduced by obtaining projections at a number of pinhole-detector distances. Projections with little multiplexing are collected at small pinhole-detector distances and high-resolution projections are collected at greater pinhole-detector distances. These projections are then reconstructed using the ML-EM algorithm. It is demonstrated through computer simulations that the synthetic collimator has superior resolution properties to a high-resolution parallel-beam (HRPB) collimator and a specially built ultra-high-resolution parallel-beam (UHRPB) collimator designed for the authors' 0.38-mm pixel CdZnTe detectors. It is also shown that reconstructing images in three dimensions is superior to reconstructing them in two dimensions. The advantages of a high-resolution synthetic collimator over the parallel-hole collimators are apparently reduced in the presence of statistical noise. However, a high-sensitivity synthetic collimator was designed which again shows superior properties to the parallel-hole collimators. Finally, it is demonstrated that, for the cases studied, high-resolution detectors are necessary for the proper functionality of the synthetic collimator.;Reconstruction of two- and three-dimensional images from synthetic-collimator data;Not health related;Not health related;0
"J. J. Legarsky; S. P. Gogineni; T. L. Akins";2001;The authors developed a synthetic aperture radar (SAR) processing algorithm for airborne/spaceborne ice-sounding radar systems and applied it to data collected in Greenland. By using focused SAR (phase-corrected coherent averaging), they improved along-track resolution by a factor of four and provided a 6-dB processing gain over unfocused SAR (coherent averaging without phase correction) based on a point-target analysis for a Greenland ice-sounding data set. Also, They demonstrated that the focused-SAR processing reduced clutter and enabled them to identify bedrock-interface returns buried in clutter. Using focused-SAR technique, they processed data collected over a key 360-km-long portion of the 2000-m contour line of southwest Greenland. To the best of their knowledge, these are the first high-quality radar ice thickness measurements over this key location. Moreover, these ice-thickness measurements have been used for improving mass-balance estimates of the Greenland ice sheet.;Focused synthetic aperture radar processing of ice-sounder data collected over the Greenland ice sheet;Not health related;Not health related;0
"W. Feng; G. Dauphin; W. Huang; Y. Quan; W. Bao; M. Wu; Q. Li";2019;"Rotation forest (RoF) is a powerful ensemble classifier and has attracted substantial attention due to its performance in hyperspectral data classification. Multi-class imbalance learning is one of the biggest challenges in machine learning and remote sensing. The standard technique for constructing RoF ensemble tends to increase the overall accuracy; RoF has difficulty to sufficiently recognize the minority class. This paper proposes a novel dynamic SMOTE (synthetic minority oversampling technique)-based RoF algorithm for the multi-class imbalance problem. The main idea of the proposed method is to dynamically balance the class distribution before building each rotation decision tree. A resampling rate is set in each iteration (ranging from 10% in the first iteration to 100% in the last) and this ratio defines the number of minority class instances randomly resampled (with replacement) from the original dataset in each iteration. The rest of the minority class instances are generated by the SMOTE method. The reported results on three real hyperspectral datasets show that the proposed method can get better performance than random forest, RoF, and some popular data sampling methods.";Dynamic Synthetic Minority Over-Sampling Technique-Based Rotation Forest for the Classification of Imbalanced Hyperspectral Data;Not health related;Not health related;0
"T. A. Le; A. G. Baydin; R. Zinkov; F. Wood";2017;We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data.;Using synthetic data to train neural networks is model-based reasoning;Not health related;Not health related;0
"B. Tang; H. He";2015;In imbalanced learning, most standard classification algorithms usually fail to properly represent data distribution and provide unfavorable classification performance. More specifically, the decision rule of minority class is usually weaker than majority class, leading to many misclassification of expensive minority class data. Motivated by our previous work ADASYN [1], this paper presents a novel kernel based adaptive synthetic over-sampling approach, named KernelADASYN, for imbalanced data classification problems. The idea is to construct an adaptive over-sampling distribution to generate synthetic minority class data. The adaptive over-sampling distribution is first estimated with kernel density estimation methods and is further weighted by the difficulty level for different minority class data. The classification performance of our proposed adaptive over-sampling approach is evaluated on several real-life benchmarks, specifically on medical and healthcare applications. The experimental results show the competitive classification performance for many real-life imbalanced data classification problems.;KernelADASYN: Kernel based adaptive synthetic data generation for imbalanced learning;Not health related;Not health related;0
"J. Shermeyer; T. Hossler; A. V. Etten; D. Hogan; R. Lewis; D. Kim";2021;RarePlanes is a unique open-source machine learning dataset that incorporates both real and synthetically generated satellite imagery. The RarePlanes dataset specifically focuses on the value of synthetic data to aid computer vision algorithms in their ability to automatically detect aircraft and their attributes in satellite imagery. Although other synthetic/real combination datasets exist, RarePlanes is the largest openly-available very-high resolution dataset built to test the value of synthetic data from an overhead perspective. Previous research has shown that synthetic data can reduce the amount of real training data needed and potentially improve performance for many tasks in the computer vision domain. The real portion of the dataset consists of 253 Maxar WorldView-3 satellite scenes spanning 112 locations and 2,142km2 with 14,700 hand-annotated aircraft. The accompanying synthetic dataset is generated via AI. Reverie's simulation platform and features 50,000 synthetic satellite images simulating a total area of 9331.2km2 with ~ 630,000 aircraft annotations. Both the real and synthetically generated aircraft feature 10 fine grain attributes including: aircraft length, wingspan, wing-shape, wing-position, wingspan class, propulsion, number of engines, number of vertical-stabilizers, presence of canards, and aircraft role. Finally, we conduct extensive experiments to evaluate the real and synthetic datasets and compare performances. By doing so, we show the value of synthetic data for the task of detecting and classifying aircraft from an overhead perspective.;RarePlanes: Synthetic Data Takes Flight;Not health related;Not health related;0
"P. Sundaresan; J. Grannen; B. Thananjeyan; A. Balakrishna; M. Laskey; K. Stone; J. E. Gonzalez; K. Goldberg";2020;Robotic manipulation of deformable 1D objects such as ropes, cables, and hoses is challenging due to the lack of high-fidelity analytic models and large configuration spaces. Furthermore, learning end-to-end manipulation policies directly from images and physical interaction requires significant time on a robot and can fail to generalize across tasks. We address these challenges using interpretable deep visual representations for rope, extending recent work on dense object descriptors for robot manipulation. This facilitates the design of interpretable and transferable geometric policies built on top of the learned representations, decoupling visual reasoning and control. We present an approach that learns point-pair correspondences between initial and goal rope configurations, which implicitly encodes geometric structure, entirely in simulation from synthetic depth images. We demonstrate that the learned representation — dense depth object descriptors (DDODs) — can be used to manipulate a real rope into a variety of different arrangements either by learning from demonstrations or using interpretable geometric policies. In 50 trials of a knot-tying task with the ABB YuMi Robot, the system achieves a 66% knot-tying success rate from previously unseen configurations. See https://tinyurl.com/rope-learning for supplementary material and videos.;Learning Rope Manipulation Policies Using Dense Object Descriptors Trained on Synthetic Depth Data;Not health related;Not health related;0
"J. Schulz-Stellenfleth; S. Lehner";2004;"A method is presented to derive two-dimensional sea surface elevation fields from complex synthetic aperture radar (SAR) data. Applied to spaceborne SAR data as acquired by European Remote Sensing 2 (ERS-2) or the Environmental Satellite (ENVISAT), the method allows to analyze the structure of ocean wave fields, e.g., wave grouping or individual wave heights on a global scale. The technique, thus, provides wave parameters not obtained with common SAR wave retrieval schemes, which are designed to estimate the 2-D wave spectrum, i.e., second-order statistical moments of the wave field. Estimates of sea surface elevation fields are obtained based on the existing theory of SAR ocean wave imaging, i.e., the modulation of the SAR image intensity due real aperture radar and motion-related effects. A power series expansion is derived for SAR intensity images that enables the analysis of nonlinear effects as well as to derive a quasi-linear approximation of the SAR imaging model in the spatial domain. A statistical analysis is performed based on a global dataset of 2D wave spectra provided by the European Centre for Medium-Range Weather Forecast. Distributions are given for the relative error of the quasi-linear approximation in the spatial domain. It is shown that the error can be reduced by smoothing the SAR image in the azimuthal direction at the cost of lower resolution. Smoothed elevation fields are retrieved by the minimization of a cost function defined in the Fourier domain based on the quasi-linear approximation of the imaging process. A multilook technique is applied to infer the information on wave propagation directions, which is required because the SAR transfer function is non-Hermitian, i.e., the SAR image is not determined by the ""frozen"" sea surface, but wave motion has a significant impact. The method is applied to simulated SAR images as well as to data acquired by ERS-2. The errors of the retrieved wave field due to image noise, uncertainties in the SAR imaging model, and bandwidth limitations are analyzed. In particular, the fact that the estimated elevation field is smoothed due to the finite system resolution and smearing effects associated with wave motion is discussed. A statistical test is proposed to check the homogeneity of the SAR image. The method makes sure that atmospheric effects are not misinterpreted as being caused by ocean waves.";Measurement of 2-D sea surface elevation fields using complex synthetic aperture radar data;Not health related;Not health related;0
"H. Qiu; B. Yu; D. Gong; Z. Li; W. Liu; D. Tao";2021;With the recent success of deep neural networks, remarkable progress has been achieved on face recognition. However, collecting large-scale real-world training data for face recognition has turned out to be challenging, especially due to the label noise and privacy issues. Meanwhile, existing face recognition datasets are usually collected from web images, lacking detailed annotations on attributes (e.g., pose and expression), so the influences of different attributes on face recognition have been poorly investigated. In this paper, we address the above-mentioned issues in face recognition using synthetic face images, i.e., SynFace. Specifically, we first explore the performance gap between recent state-of-the-art face recognition models trained with synthetic and real face images. We then analyze the underlying causes behind the performance gap, e.g., the poor intraclass variations and the domain gap between synthetic and real face images. Inspired by this, we devise the SynFace with identity mixup (IM) and domain mixup (DM) to mitigate the above performance gap, demonstrating the great potentials of synthetic data for face recognition. Furthermore, with the controllable face synthesis model, we can easily manage different factors of synthetic face generation, including pose, expression, illumination, the number of identities, and samples per identity. Therefore, we also perform a systematically empirical analysis on synthetic face images to provide some insights on how to effectively utilize synthetic data for face recognition.;SynFace: Face Recognition with Synthetic Data;Not health related;Not health related;0
"F. Zhou; M. Tao";2015;Narrow-band interference (NBI) poses a hindrance to high-quality imaging for synthetic aperture radar (SAR). It is an under-determined single-channel separation problem. In this paper, we addressed the NBI suppression problem by introducing two advanced data-driven nonparametric techniques: 1) the eigen subspace filtering and 2) the independent component analysis (ICA). Both of these two methods utilize the statistical difference between the useful radar echoes and NBI. The interference-contaminated pulse is decomposed into a set of basis signals, from which the bases corresponding to NBI are selected out. Then, the contribution of NBI is excised by filtering out the corresponding interference components. The performances of these advanced methods are compared with the conventional notch filtering method. The experimental results on real datasets show the effectiveness of the proposed methods.;Research on Methods for Narrow-Band Interference Suppression in Synthetic Aperture Radar Data;Not health related;Not health related;0
"T. Kubota; N. Yoshida; S. Urita; T. Iguchi; S. Seto; R. Meneghini; J. Awaka; H. Hanado; S. Kida; R. Oki";2014;"The Global Precipitation Measurement (GPM) Core Observatory will carry a Dual-frequency Precipitation Radar (DPR) consisting of a Ku-band precipitation radar (KuPR) and a Ka-band precipitation radar (KaPR). In this study, “at-launch” codes of DPR precipitation algorithms, which will be used in GPM ground systems at launch, were evaluated using synthetic data based upon the Tropical Rainfall Measuring Mission (TRMM) Precipitation Radar (PR) data. Results from the codes (Version 4.20131010) of the KuPR-only, KaPR-only, and DPR algorithms were compared with “true values” calculated based upon drop size distributions assumed in the synthetic data and standard results from the TRMM algorithms at an altitude of 2 km over the ocean. The results indicate that the total precipitation amounts during April 2011 from the KuPR and DPR algorithms are similar to the true values, whereas the estimates from the KaPR data are underestimated. Moreover, the DPR estimates yielded smaller precipitation rates for rates less than about 10 mm/h and greater precipitation rates above 10 mm/h. Underestimation of the KaPR estimates was analyzed in terms of measured radar reflectivity (Zm) of the KaPR at an altitude of 2 km. The underestimation of the KaPR data was most pronounced during strong precipitation events of Zm <; 18 dBZ (high attenuation cases) over heavy precipitation areas in the Tropics, whereas the underestimation was less pronounced when the Zm > 26 (moderate attenuation cases). The results suggest that the underestimation is caused by a problem in the attenuation correction method, which was verified by the improved codes.";Evaluation of Precipitation Estimates by at-Launch Codes of GPM/DPR Algorithms Using Synthetic Data from TRMM/PR Observations;Not health related;Not health related;0
"G. Daniel; F. Ceraudo; O. Limousin; D. Maier; A. Meuris";2020;Automatic and fast identification of gamma-ray-emitting radionuclides is a challenge in the field of nuclear safety, especially in case of emergency, since it requires complex calculations and often the knowledge of experts to interpret the data. We present a development of an automatic identification method based on convolutional neural networks (CNNs) as a new tool to analyze gamma-ray spectra in real time, which uses not only photoelectric peaks but also extracts all discriminant features in the spectrum, such as Compton structures, for instance. The original approach relies on the training of the CNN with a fully synthetic database, built by means of a Monte Carlo simulation with Geant4 combined with a detailed analytical detector response model. The algorithm and training method are evaluated to identify radionuclides in measurements of the mixtures of sources acquired with Caliste, a fine-pitch CdTe imaging spectrometer. The neural network is able to discriminate each element in an arbitrary mixture very quickly with high accuracy.;Automatic and Real-Time Identification of Radionuclides in Gamma-Ray Spectra: A New Method Based on Convolutional Neural Network Trained With Synthetic Data Set;Not health related;Not health related;0
"C. -L. Liu; P. -Y. Hsieh";2020;Imbalanced data is characterized by the severe difference in observation frequency between classes and has received a lot of attention in data mining research. The prediction performances usually deteriorate as classifiers learn from imbalanced data, as most classifiers assume the class distribution is balanced or the costs for different types of classification errors are equal. Although several methods have been devised to deal with imbalance problems, it is still difficult to generalize those methods to achieve stable improvement in most cases. In this study, we propose a novel framework called model-based synthetic sampling (MBS) to cope with imbalance problems, in which we integrate modeling and sampling techniques to generate synthetic data. The key idea behind the proposed method is to use regression models to capture the relationship between features and to consider data diversity in the process of data generation. We conduct experiments on 13 datasets and compare the proposed method with 10 methods. The experimental results indicate that the proposed method is not only comparative but also stable. We also provide detailed investigations and visualizations of the proposed method to empirically demonstrate why it could generate good data samples.;Model-Based Synthetic Sampling for Imbalanced Data;Not health related;Not health related;0
"I. Abbasnejad; S. Sridharan; D. Nguyen; S. Denman; C. Fookes; S. Lucey";2017;Over the past few years, neural networks have made a huge improvement in object recognition and event analysis. However, due to a lack of available data, neural networks were not efficiently applied in expression analysis. In this paper, we tackle the problem of facial expression analysis using deep neural network by generating a realistic large scale synthetic labeled dataset. We train a deep 3-dimensional convolutional network on the generated dataset and empirically show how the presented method can efficiently classify facial expressions. Our method addresses four fundamental issues: (i) generating a large scale facial expression dataset that is realistic and accurate, (ii) a rich spatial representation of expressions, (iii) better spatiotemporal feature learning compared to recent techniques and (iv) with a simple linear classifier our learned features outperform state-of-the-art methods.;Using Synthetic Data to Improve Facial Expression Analysis with 3D Convolutional Networks;Not health related;Not health related;0
"N. Inkawhich; M. J. Inkawhich; E. K. Davis; U. K. Majumder; E. Tripp; C. Capraro; Y. Chen";2021;Obtaining measured synthetic aperture radar (SAR) data for training automatic target recognition (ATR) models can be too expensive (in terms of time and money) and complex of a process in many situations. In response, researchers have developed methods for creating synthetic SAR data for targets using electro-magnetic prediction software, which is then used to enrich an existing measured training dataset. However, this approach relies on the availability of some amount of measured data. In this work, we focus on the case of having 100% synthetic training data, while testing on only measured data. We use the SAMPLE dataset public released by AFRL, and find significant challenges to learning generalizable representations from the synthetic data due to distributional differences between the two modalities and extremely limited training sample quantities. Using deep learning-based ATR models, we propose data augmentation, model construction, loss function choices, and ensembling techniques to enhance the representation learned from the synthetic data, and ultimately achieved over 95% accuracy on the SAMPLE dataset. We then analyze the functionality of our ATR models using saliency and feature-space investigations and find them to learn a more cohesive representation of the measured and synthetic data. Finally, we evaluate the out-of-library detection performance of our synthetic-only models and find that they are nearly 10% more effective than baseline methods at identifying measured test samples that do not belong to the training class set. Overall, our techniques and their compositions significantly enhance the feasibility of using ATR models trained exclusively on synthetic data.;Bridging a Gap in SAR-ATR: Training on Fully Synthetic and Testing on Measured Data;Not health related;Not health related;0
"Q. Chang; H. Qu; Y. Zhang; M. Sabuncu; C. Chen; T. Zhang; D. N. Metaxas";2020;In this paper, we propose a data privacy-preserving and communication efficient distributed GAN learning framework named Distributed Asynchronized Discriminator GAN (AsynDGAN). Our proposed framework aims to train a central generator learns from distributed discriminator, and use the generated synthetic image solely to train the segmentation model. We validate the proposed framework on the application of health entities learning problem which is known to be privacy sensitive. Our experiments show that our approach: 1) could learn the real image’s distribution from multiple datasets without sharing the patient’s raw data. 2) is more efficient and requires lower bandwidth than other distributed deep learning methods. 3) achieves higher performance compared to the model trained by one real dataset, and almost the same performance compared to the model trained by all real datasets. 4) has provable guarantees that the generator could learn the distributed distribution in an all important fashion thus is unbiased.We release our AsynDGAN source code at: https://github.com/tommy-qichang/AsynDGAN;Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN Without Sharing Medical Image Data;health related;health related;1
"V. R. N. Pauwels; A. Balenzano; G. Satalino; H. Skriver; N. E. C. Verhoest; F. Mattia";2009;It is widely recognized that synthetic aperture radar (SAR) data are a very valuable source of information for the modeling of the interactions between the land surface and the atmosphere. During the last couple of decades, most of the research on the use of SAR data in hydrologic applications has been focused on the retrieval of land and biogeophysical parameters (e.g., soil moisture contents). One relatively unexplored issue consists of the optimization of soil hydraulic model parameters, such as, for example, hydraulic conductivity values, through remote sensing. This is due to the fact that no direct relationships between the remote-sensing observations, more specifically radar backscatter values, and the parameter values can be derived. However, land surface models can provide these relationships. The objective of this paper is to retrieve a number of soil physical model parameters through a combination of remote sensing and land surface modeling. Spatially distributed and multitemporal SAR-based soil moisture maps are the basis of the study. The surface soil moisture values are used in a parameter estimation procedure based on the extended Kalman filter equations. In fact, the land surface model is, thus, used to determine the relationship between the soil physical parameters and the remote-sensing data. An analysis is then performed, relating the retrieved soil parameters to the soil texture data available over the study area. The results of the study show that there is a potential to retrieve soil physical model parameters through a combination of land surface modeling and remote sensing.;Optimization of Soil Hydraulic Model Parameters Using Synthetic Aperture Radar Data: An Integrated Multidisciplinary Approach;Not health related;Not health related;0
"J. Chen; J. J. Little";2019;Calibrating sports cameras is important for autonomous broadcasting and sports analysis. Here we propose a highly automatic method for calibrating sports cameras from a single image using synthetic data. First, we develop a novel camera pose engine that generates camera poses by randomly sampling camera parameters. The camera pose engine has only three significant free parameters so that it can effectively generate diverse camera poses and corresponding edge (i.e. field marking) images. Then, we learn compact feature descriptors via a siamese network from paired edge images and build a feature-pose database. After that, we use a novel GAN (generative adversarial network) model to detect field markings in real images. Finally, we query an initial camera pose from the feature-pose database and refine camera poses using truncated distance images. We evaluate our method on both synthetic and real data. Our method not only demonstrates the robustness on the synthetic data but also achieves state-of-the-art accuracy on a standard soccer dataset and very high performance on a volleyball dataset.;Sports Camera Calibration via Synthetic Data;Not health related;Not health related;0
"M. Nabati; H. Navidan; R. Shahbazian; S. A. Ghorashi; D. Windridge";2020;Human-centered data collection is typically costly and implicates issues of privacy. Various solutions have been proposed in the literature to reduce this cost, such as crowd-sourced data collection, or the use of semisupervised algorithms. However, semisupervised algorithms require a source of unlabeled data, and crowd-sourcing methods require numbers of active participants. An alternative passive data collection modality is fingerprint-based localization. Such methods use received signal strength or channel state information in wireless sensor networks to localize users in indoor/outdoor environments. In this letter, we introduce a novel approach to reduce training data collection costs in fingerprint-based localization by using synthetic data. Generative adversarial networks (GANs) are used to learn the distribution of a limited sample of collected data and, following this, to produce synthetic data that can be used to augment the real collected data in order to increase overall positioning accuracy. Experimental results on a benchmark dataset show that by applying the proposed method and using a combination of 10% collected data and 90% synthetic data, we can obtain essentially similar positioning accuracy to that which would be obtained by using the full set of collected data. This means that by employing GAN-generated synthetic data, we can use 90% less real data, thereby reducing data-collection costs while achieving acceptable accuracy.;Using Synthetic Data to Enhance the Accuracy of Fingerprint-Based Localization: A Deep Learning Approach;Not health related;Not health related;0
"M. S. Santos; R. C. Pereira; A. F. Costa; J. P. Soares; J. Santos; P. H. Abreu";2019;"The performance evaluation of imputation algorithms often involves the generation of missing values. Missing values can be inserted in only one feature (univariate configuration) or in several features (multivariate configuration) at different percentages (missing rates) and according to distinct missing mechanisms, namely, missing completely at random, missing at random, and missing not at random. Since the missing data generation process defines the basis for the imputation experiments (configuration, missing rate, and missing mechanism), it is essential that it is appropriately applied; otherwise, conclusions derived from ill-defined setups may be invalid. The goal of this paper is to review the different approaches to synthetic missing data generation found in the literature and discuss their practical details, elaborating on their strengths and weaknesses. Our analysis revealed that creating missing at random and missing not at random scenarios in datasets comprising qualitative features is the most challenging issue in the related work and, therefore, should be the focus of future work in the field.";Generating Synthetic Missing Data: A Review by Missing Mechanism;Not health related;Not health related;0
"I. Ali; S. Cao; V. Naeimi; C. Paulik; W. Wagner";2018;The Sentinel-1 GRD (ground range detected) Level-1 product generated by the Instrument Processing Facility of the European Space Agency has noise artifacts at the image borders, which are quite consistent at both left and right sides of the satellite's cross track and at the start and end of the data take along track. The Sentinel-1 border noise troubles the creation of clean and consistence time series of backscatter. Data quality control and management become very challenging tasks, when it comes to the large-scale data processing, both in terms of spatial coverage and data volume. In this paper, we evaluate three techniques for removing the Sentinel-1 border noise and compare the results with the existing “Sentinel-1 GRD Border Noise Removal” algorithm implemented in the Sentinel-1 toolbox of the Sentinel application platform.1 Validation and evaluation of the newly proposed algorithms was done using random samples containing 1500 Sentinel-1 scenes selected from a complete Sentinel-1 archive. The newly proposed approach has successfully achieved the required level of accuracy and solved the issue of time-series anomalies due to the border noise.;Methods to Remove the Border Noise From Sentinel-1 Synthetic Aperture Radar Data: Implications and Importance For Time-Series Analysis;Not health related;Not health related;0
"B. Planche; Z. Wu; K. Ma; S. Sun; S. Kluckner; O. Lehmann; T. Chen; A. Hutter; S. Zakharov; H. Kosch; J. Ernst";2017;"Recent progress in computer vision has been dominated by deep neural networks trained over larges amount of labeled data. Collecting such datasets is however a tedious, often impossible task; hence a surge in approaches relying solely on synthetic data for their training. For depth images however, discrepancies with real scans still noticeably affect the end performance. We thus propose an end-to-end framework which simulates the whole mechanism of these devices, generating realistic depth data from 3D models by comprehensively modeling vital factors e.g. sensor noise, material reflectance, surface geometry. Not only does our solution cover a wider range of sensors and achieve more realistic results than previous methods, assessed through extended evaluation, but we go further by measuring the impact on the training of neural networks for various recognition tasks; demonstrating how our pipeline seamlessly integrates such architectures and consistently enhances their performance.";DepthSynth: Real-Time Realistic Synthetic Data Generation from CAD Models for 2.5D Recognition;Not health related;Not health related;0
"J. C. Triyonoputro; W. Wan; K. Harada";2019;This paper explores the use of robots to autonomously assemble parts with variations in colors and textures. Specifically, we focus on peg-in-hole assembly with some initial position uncertainty and holes located on surfaces of different colors and textures. Two in-hand cameras and a force-torque sensor are used to account for the position uncertainty. A program sequence comprising learning-based visual servoing, spiral search, and impedance control is implemented to perform the peg-in-hole task with feedback from the above sensors. Contributions are mainly made in the learning-based visual servoing component of the sequence, where a deep neural network is trained with various sets of synthetic data generated using the concept of domain randomization to predict where a hole is. In the experiments and analysis section, the network is analyzed and compared, and a real-world robotic system to insert pegs to holes using the proposed method is implemented. The results show that the implemented peg-in-hole assembly system can perform successful peg-in-hole insertions on surfaces with various colors and textures. It can generally speed up the entire peg-in-hole process, especially when the initial position uncertainty is large.;Quickly Inserting Pegs into Uncertain Holes using Multi-view Images and Deep Network Trained on Synthetic Data;Not health related;Not health related;0
D. P. Williams;2009;A Bayesian data fusion approach for seabed classification using multiview synthetic aperture sonar (SAS) imagery is proposed. The principled approach exploits all available information and results in probabilistic predictions. Each data point, corresponding to a unique 10 m times10 m area of seabed, is represented by a vector of wavelet-based features. For each seabed type, the distribution of these features is then modeled by a unique Gaussian mixture model. When multiple views of the same data point (i.e., area of seabed) are available, the views are combined via a joint likelihood calculation. The end result of this Bayesian formulation is the posterior probability that a given data point belongs to each seabed type. It is also shown how these posterior probabilities can be exploited in a form of entropy-based active-learning to determine the most useful additional data to acquire. Experimental results of the proposed multiview classification framework are shown on a large data set of real, multiview SAS imagery spanning more than 2 km2 of seabed.;Bayesian Data Fusion of Multiview Synthetic Aperture Sonar Imagery for Seabed Classification;Not health related;Not health related;0
"H. M. Clever; Z. Erickson; A. Kapusta; G. Turk; C. K. Liu; C. C. Kemp";2020;People spend a substantial part of their lives at rest in bed. 3D human pose and shape estimation for this activity would have numerous beneficial applications, yet line-of-sight perception is complicated by occlusion from bedding. Pressure sensing mats are a promising alternative, but training data is challenging to collect at scale. We describe a physics-based method that simulates human bodies at rest in a bed with a pressure sensing mat, and present PressurePose, a synthetic dataset with 206K pressure images with 3D human poses and shapes. We also present PressureNet, a deep learning model that estimates human pose and shape given a pressure image and gender. PressureNet incorporates a pressure map reconstruction (PMR) network that models pressure image generation to promote consistency between estimated 3D body models and pressure image input. In our evaluations, PressureNet performed well with real data from participants in diverse poses, even though it had only been trained with synthetic data. When we ablated the PMR network, performance dropped substantially.;Bodies at Rest: 3D Human Pose and Shape Estimation From a Pressure Image Using Synthetic Data;Not health related;Not health related;0
"R. Madaan; D. Maturana; S. Scherer";2017;Wire detection is a key capability for safe navigation of autonomous aerial vehicles and is a challenging problem as wires are generally only a few pixels wide, can appear at any orientation and location, and are hard to distinguish from other similar looking lines and edges. We leverage the recent advances in deep learning by treating wire detection as a semantic segmentation task, and investigate the effectiveness of convolutional neural networks for the same. To find an optimal model in terms of detection accuracy and real time performance on a portable GPU, we perform a grid search over a finite space of architectures. Further, to combat the issue of unavailability of a large public dataset with annotations, we render synthetic wires using a ray tracing engine, and overlay them on 67K images from flight videos available on the internet. We use this synthetic dataset for pretraining our models before finetuning on real data, and show that synthetic data alone can lead to pretty accurate detections qualitatively as well. We also verify if providing explicit information about local evidence of wiry-ness in the form of edge and line detection results from a traditional computer vision method, as additional channels to the network input, makes the task easier or not. We evaluate our best models from the grid search on a publicly available dataset and show that they outperform previous work using traditional computer vision and various deep net baselines of FCNs, SegNet and E-Net, on both standard edge detection metrics and inference speed. Our top models run at more than 3Hz on the NVIDIA Jetson TX2 with input resolution of 480_640, with an Average Precision score of 0.73 on our test split of the USF dataset.;Wire detection using synthetic data and dilated convolutional networks for unmanned aerial vehicles;Not health related;Not health related;0
"H. Li; J. L. Wert; A. B. Birchfield; T. J. Overbye; T. G. S. Roman; C. M. Domingo; F. E. P. Marcos; P. D. Martinez; T. Elgindy; B. Palmintier";2020;This paper introduces a methodology for building synthetic electric grid data sets that represent fictitious, yet realistic, combined transmission and distribution (T&D) systems. Such data sets have important applications, such as in the study of the wide-area interactions of distributed energy resources, in the validation of advanced control schemes, and in network resilience to severe events. The data sets created here are geographically located on an actual North American footprint, with the end-user load information estimated from land parcel data. The grid created to serve these fictional but realistic loads is built starting with low-voltage and medium-voltage distribution systems in full detail, connected to distribution and transmission substations. Bulk generation is added, and a high-voltage transmission grid is created. This paper explains the overall process and challenges addressed in making the combined case. An example test case, syn-austin-TDgrid-v03, is shown for a 307236-customer case located in central Texas, with 140 substations, 448 feeders, and electric line data at voltages ranging from 120 V to 230 kV. Such new combined test cases help to promote high quality in the research on large-scale systems, particularly since much actual power system data are subject to data confidentiality. The highly detailed, combined T&D data set can also facilitate the modeling and analysis of coupled infrastructures.;Building Highly Detailed Synthetic Electric Grid Data Sets for Combined Transmission and Distribution Systems;Not health related;Not health related;0
"M. Alessandrini; B. Chakraborty; B. Heyde; O. Bernard; M. De Craene; M. Sermesant; J. D’Hooge";2018;"Two-dimensional (2-D) echocardiography is the modality of choice in the clinic for the diagnosis of cardiac disease. Hereto, speckle tracking (ST) packages complement visual assessment by the cardiologist by providing quantitative diagnostic markers of global and regional cardiac function (e.g., displacement, strain, and strain-rate). Yet, the reported high vendor-dependence between the outputs of different ST packages raises clinical concern and hampers the widespread dissemination of the ST technology. In part, this is due to the lack of a solid commonly accepted quality assurance pipeline for ST packages. Recently, we have developed a framework to benchmark ST algorithms for 3-D echocardiography by using realistic simulated volumetric echocardiographic recordings. Yet, 3-D echocardiography remains an emerging technology, whereas the compelling clinical concern is, so far, directed to the standardization of 2-D ST only. Therefore, by building upon our previous work, we present in this paper a pipeline to generate realistic synthetic sequences for 2-D ST algorithms. Hereto, the synthetic cardiac motion is obtained from a complex electromechanical heart model, whereas realistic vendor-specific texture is obtained by sampling a real clinical ultrasound recording. By modifying the parameters in our pipeline, we generated an open-access library of 105 synthetic sequences encompassing: 1) healthy and ischemic motion patterns; 2) the most common apical probe orientations; and 3) vendor-specific image quality from seven different systems. Ground truth deformation is also provided to allow performance analysis. The application of the provided data set is also demonstrated in the benchmarking of a recent academic ST algorithm.";Realistic Vendor-Specific Synthetic Ultrasound Data for Quality Assurance of 2-D Speckle Tracking Echocardiography: Simulation Pipeline and Open Access Database;Not health related;Not health related;0
"V. Krishnan; B. Bugbee; T. Elgindy; C. Mateo; P. Duenas; F. Postigo; J. -S. Lacroix; T. G. S. Roman; B. Palmintier";2020;"There is a strong need for synthetic yet realistic distribution system test data sets that are as diverse, large, and complex to solve as real systems. Such data sets can facilitate the development of advanced algorithms and the assessment of emerging distributed energy resources while avoiding the need to acquire proprietary critical infrastructure or private data. Such synthetic data sets, however, are useful only if they are realistic enough to look and behave similarly to actual systems. This paper presents a comprehensive framework for validating synthetic distribution data sets using a three-pronged statistical, operational, and expert validation approach. It also presents a set of statistical and operational metric targets for achieving realistic data sets based on detailed characterization of more than 10,000 real U.S. utility feeders. The paper demonstrates the use of the proposed validation approach to validate three large-scale synthetic data sets developed by the authors representing Santa Fe, New Mexico; Greensboro, North Carolina; and the San Francisco Bay Area, California.";Validation of Synthetic U.S. Electric Power Distribution System Data Sets;Not health related;Not health related;0
"J. W. Anderson; K. E. Kennedy; L. B. Ngo; A. Luckow; A. W. Apon";2014;The concept of Internet of Things (IoT) is rapidly moving from a vision to being pervasive in our everyday lives. This can be observed in the integration of connected sensors from a multitude of devices such as mobile phones, healthcare equipment, and vehicles. There is a need for the development of infrastructure support and analytical tools to handle IoT data, which are naturally big and complex. But, research on IoT data can be constrained by concerns about the release of privately owned data. In this paper, we present the design and implementation results of a synthetic IoT data generation framework. The framework enables research on synthetic data that exhibit the complex characteristics of original data without compromising proprietary information and personal privacy.;Synthetic data generation for the internet of things;Not health related;Not health related;0
"X. Zhang; H. Li; M. Dai; W. Ma; L. Quan";2014;In this paper, we develop a data-driven technique to model trees from a single laser scan. A multi-layer representation of the tree structure is proposed to guide the modeling process. In this process, a marching cylinder algorithm is first developed to construct visible branches from the laser scan data. Three levels of crown feature points are then extracted from the scan data to synthesize three layers of non-visible branches. Based on the hierarchical particle flow technique, the branch synthesis method has the advantage of producing visually convincing tree models that are consistent with scan data. User intervention is extremely limited. The robustness of this technique has been validated on both conifer and broadleaf trees.;Data-Driven Synthetic Modeling of Trees;Not health related;Not health related;0
"M. Hahner; D. Dai; C. Sakaridis; J. -N. Zaech; L. V. Gool";2019;This work addresses the problem of semantic scene understanding under foggy road conditions. Although marked progress has been made in semantic scene understanding over the recent years, it is mainly concentrated on clear weather outdoor scenes. Extending semantic segmentation methods to adverse weather conditions like fog is crucially important for outdoor applications such as self-driving cars. In this paper, we propose a novel method, which uses purely synthetic data to improve the performance on unseen real-world foggy scenes captured in the streets of Zurich and its surroundings. Our results highlight the potential and power of photo-realistic synthetic images for training and especially fine-tuning deep neural nets. Our contributions are threefold, 1) we created a purely synthetic, high-quality foggy dataset of 25,000 unique outdoor scenes, that we call Foggy Synscapes and plan to release publicly 2) we show that with this data we outperform previous approaches on real-world foggy test data 3) we show that a combination of our data and previously used data can even further improve the performance on real-world foggy data.;Semantic Understanding of Foggy Scenes with Purely Synthetic Data;Not health related;Not health related;0
"Z. Li; S. Papson; R. M. Narayanan";2008;Although techniques for resolution enhancement in single-aspect radar imaging have made rapid progress in recent years, it does not necessarily imply that such enhanced images will improve target identification or recognition. However, when multiple looks of the same target from different aspects are obtained, the available knowledge increases, allowing more useful target information to be extracted. Physics-based image fusion techniques can be developed by processing the raw data collected from multiple inverse synthetic aperture radar sensors, even if these individual images are at different resolutions. We derive an appropriate data fusion rule to generate a composite image containing enhanced target shape characteristics for improved target recognition. The rule maps multiple data sets collected by multiple radars with different system parameters on to the same spatial-frequency space. The composite image can be reconstructed using the inverse 2-D Fourier transform over the separated multiple integration areas. An algorithm called the Matrix Fourier Transform is proposed to realize such a complicated integral. This algorithm can be regarded as an exact interpolation such that there is no information loss caused by data fusion. The rotation centers need to be carefully selected to properly register the multiple images before performing the fusion. A comparison of the image attribute rating curve between the fused image and the spatially averaged images quantifies the improvement in the detected target features. The technique shows considerable improvement over a simple spatial averaging algorithm and thereby enhances target recognition.;Data-Level Fusion of Multilook Inverse Synthetic Aperture Radar Images;Not health related;Not health related;0
"A. Gilbert; M. Marciniak; C. Rodero; P. Lamata; E. Samset; K. Mcleod";2021;Deep learning can bring time savings and increased reproducibility to medical image analysis. However, acquiring training data is challenging due to the time-intensive nature of labeling and high inter-observer variability in annotations. Rather than labeling images, in this work we propose an alternative pipeline where images are generated from existing high-quality annotations using generative adversarial networks (GANs). Annotations are derived automatically from previously built anatomical models and are transformed into realistic synthetic ultrasound images with paired labels using a CycleGAN. We demonstrate the pipeline by generating synthetic 2D echocardiography images to compare with existing deep learning ultrasound segmentation datasets. A convolutional neural network is trained to segment the left ventricle and left atrium using only synthetic images. Networks trained with synthetic images were extensively tested on four different unseen datasets of real images with median Dice scores of 91, 90, 88, and 87 for left ventricle segmentation. These results match or are better than inter-observer results measured on real ultrasound datasets and are comparable to a network trained on a separate set of real images. Results demonstrate the images produced can effectively be used in place of real data for training. The proposed pipeline opens the door for automatic generation of training data for many tasks in medical imaging as the same process can be applied to other segmentation or landmark detection tasks in any modality. The source code and anatomical models are available to other researchers.11https://adgilbert.github.io/data-generation/;Generating Synthetic Labeled Data From Existing Anatomical Models: An Example With Echocardiography Segmentation;health related;Not health related;1
"J. J. Yackel; D. G. Barber";2007;In this paper, we examine the utility of synthetic aperture radar (SAR) backscatter data to detect a change in snow water equivalent (SWE) over landfast first-year sea ice during winter at relatively cold temperatures. We begin by reviewing the theoretical framework for linking microwave scattering from SAR to the thermodynamic and electrical properties of first-year sea ice. Previous research has demonstrated that for a given ice thickness and air-temperature change, a thick snow cover will result in a smaller change in the snow-ice interface temperature than will a thin snow cover. This small change in the interface temperature will result in a relatively small change in the brine volume at the interface and the resulting complex permittivity, thereby producing a relatively small change in scattering. A thin snow cover produces the opposite effect-a greater change in interface temperature, brine volume, permittivity, and scattering. This work is extended here to illustrate a variation of this effect over landfast first-year sea ice using in situ measurements of physical snow properties and RADARSAT-1 SAR imagery acquired during the winter of 1999 in the central Canadian Archipelago at cold (~-26degC) and moderately cold (~-14degC) snow-sea-ice interface temperatures. We utilize in situ data from five validation sites to demonstrate how the change in microwave scattering covaries and is inversely proportional with the change in the magnitude of SWE. These changes are shown to be detectable over both short (2 days) and longer (45 days) time durations;Observations of Snow Water Equivalent Change on Landfast First-Year Sea Ice in Winter Using Synthetic Aperture Radar Data;Not health related;Not health related;0
"K. Kim; H. Myung";2018;Image-based sensing of jellyfish is important as they can cause great damage to the fisheries and seaside facilities and need to be properly controlled. In this paper, we present a deep-learning-based technique to generate a synthetic image of the jellyfish easily with autoencoder-combined generative adversarial networks. The proposed system can easily generate simple images with a smaller number of data sets compared with other generative networks. The generated output showed high similarity with the real-image data set. The application using a fully convolutional network and regression network to estimate the size of the jellyfish swarm was also demonstrated, and showed high accuracy during the estimation test.;Autoencoder-Combined Generative Adversarial Networks for Synthetic Image Data Generation and Detection of Jellyfish Swarm;Not health related;Not health related;0
"Y. Cai; L. Ge; J. Cai; N. M. Thalmann; J. Yuan";2021;Compared with depth-based 3D hand pose estimation, it is more challenging to infer 3D hand pose from monocular RGB images, due to the substantial depth ambiguity and the difficulty of obtaining fully-annotated training data. Different from the existing learning-based monocular RGB-input approaches that require accurate 3D annotations for training, we propose to leverage the depth images that can be easily obtained from commodity RGB-D cameras during training, while during testing we take only RGB inputs for 3D joint predictions. In this way, we alleviate the burden of the costly 3D annotations in real-world dataset. Particularly, we propose a weakly-supervised method, adaptating from fully-annotated synthetic dataset to weakly-labeled real-world single RGB dataset with the aid of a depth regularizer, which serves as weak supervision for 3D pose prediction. To further exploit the physical structure of 3D hand pose, we present a novel CVAE-based statistical framework to embed the pose-specific subspace from RGB images, which can then be used to infer the 3D hand joint locations. Extensive experiments on benchmark datasets validate that our proposed approach outperforms baselines and state-of-the-art methods, which proves the effectiveness of the proposed depth regularizer and the CVAE-based framework.;3D Hand Pose Estimation Using Synthetic Data and Weakly Labeled RGB Images;Not health related;Not health related;0
"N. Damer; C. A. F. López; M. Fang; N. Spiller; M. V. Pham; F. Boutros";2022;"The main question this work aims at answering is: ""can morphing attack detection (MAD) solutions be successfully developed based on synthetic data?"". Towards that, this work introduces the first synthetic-based MAD development dataset, namely the Synthetic Morphing Attack Detection Development dataset (SMDD). This dataset is utilized successfully to train three MAD backbones where it proved to lead to high MAD performance, even on completely unknown attack types. Additionally, an essential aspect of this work is the detailed legal analyses of the challenges of using and sharing real biometric data, rendering our proposed SMDD dataset extremely essential. The SMDD dataset, consisting of 30,000 attack and 50,000 bona fide samples, is publicly available for research purposes 1.";Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors;Not health related;Not health related;0
"K. Davila; S. Ludi; R. Zanibbi";2014;We present an approach for on-line recognition of handwritten math symbols using adaptations of off-line features and synthetic data generation. We compare the performance of our approach using four different classification methods: AdaBoost. M1 with C4.5 decision trees, Random Forests and Support-Vector Machines with linear and Gaussian kernels. Despite the fact that timing information can be extracted from on-line data, our feature set is based on shape description for greater tolerance to variations of the drawing process. Our main datasets come from the Competition on Recognition of Online Handwritten Mathematical Expressions (CROHME) 2012 and 2013. Class representation bias in CROHME datasets is mitigated by generating samples for underrepresented classes using an elastic distortion model. Our results show that generation of synthetic data for underrepresented classes might lead to improvements of the average per-class accuracy. We also tested our system using the Math Brush dataset achieving a top-1 accuracy of 89.87% which is comparable with the best results of other recently published approaches on the same dataset.;Using Off-Line Features and Synthetic Data for On-Line Handwritten Math Symbol Recognition;Not health related;Not health related;0
"A. Wong; S. Cicek; S. Soatto";2021;We present a method for inferring dense depth maps from images and sparse depth measurements by leveraging synthetic data to learn the association of sparse point clouds with dense natural shapes, and using the image as evidence to validate the predicted depth map. Our learned prior for natural shapes uses only sparse depth as input, not images, so the method is not affected by the covariate shift when attempting to transfer learned models from synthetic data to real ones. This allows us to use abundant synthetic data with ground truth to learn the most difficult component of the reconstruction process, which is topology estimation, and use the image to refine the prediction based on photometric evidence. Our approach uses fewer parameters than previous methods, yet, achieves the state of the art on both indoor and outdoor benchmark datasets.;Learning Topology From Synthetic Data for Unsupervised Depth Completion;Not health related;Not health related;0
"Y. Lin; C. Tang; F. -J. Chu; P. A. Vela";2020;A segmentation-based architecture is proposed to decompose objects into multiple primitive shapes from monocular depth input for robotic manipulation. The backbone deep network is trained on synthetic data with 6 classes of primitive shapes generated by a simulation engine. Each primitive shape is designed with parametrized grasp families, permitting the pipeline to identify multiple grasp candidates per shape primitive region. The grasps are priority ordered via proposed ranking algorithm, with the first feasible one chosen for execution. On task-free grasping of individual objects, the method achieves a 94% success rate. On task-oriented grasping, it achieves a 76% success rate. Overall, the method supports the hypothesis that shape primitives can support task-free and task-relevant grasp prediction.;Using Synthetic Data and Deep Networks to Recognize Primitive Shapes for Object Grasping;Not health related;Not health related;0
"W. Xu; P. Huang; R. Wang; Y. Deng";2013;The inherent limitation between azimuth resolution and range swath width in conventional spaceborne synthetic aperture radar (SAR) systems can be overcome by introducing the displaced phase center antenna technique. In these SAR systems, echoes from all subapertures should be reconstructed and combined together before single-channel SAR processors. However, in the multichannel sliding spotlight and Terrain Observation by Progressive Scans (TOPS) modes, azimuth beam progressive sweeping during the whole acquisition interval leads to that the total Doppler bandwidth spans over several N ·PRF intervals, where N is the number of azimuth channels and PRF is the pulse repetition frequency. As a result, conventional azimuth multichannel reconstruction algorithms for the stripmap mode are not directly suitable for sliding spotlight and TOPS modes. This paper proposes a novel imaging processor for both modes according to their azimuth echo properties. The key point of the proposed focusing processor is the first processing step of multichannel azimuth data reconstruction, which extends the two-step focusing technique to process raw data of the azimuth multichannel case. In addition to azimuth data preprocessing and accurate range cell migration correction steps, the final postprocessing step is added to correct the possible back-folded SAR images. Imaging results on simulated raw data validate the proposed imaging approach.;Processing of Multichannel Sliding Spotlight and TOPS Synthetic Aperture Radar Data;Not health related;Not health related;0
"F. Boutros; M. Huber; P. Siebke; T. Rieber; N. Damer";2022;Recent deep face recognition models proposed in the literature utilized large-scale public datasets such as MS-Celeb-1M and VGGFace2 for training very deep neural networks, achieving state-of-the-art performance on mainstream benchmarks. Recently, many of these datasets, e.g., MS-Celeb-1M and VGGFace2, are retracted due to credible privacy and ethical concerns. This motivates this work to propose and investigate the feasibility of using a privacy-friendly synthetically generated face dataset to train face recognition models. Towards this end, we utilize a class-conditional generative adversarial network to generate class-labeled synthetic face images, namely SFace. To address the privacy aspect of using such data to train a face recognition model, we provide extensive evaluation experiments on the identity relation between the synthetic dataset and the original authentic dataset used to train the generative model. Our reported evaluation proved that associating an identity of the authentic dataset to one with the same class label in the synthetic dataset is hardly possible. We also propose to train face recognition on our privacy-friendly dataset, SFace, using three different learning strategies, multi-class classification, label-free knowledge transfer, and combined learning of multi-class classification and knowledge transfer. The reported evaluation results on five authentic face benchmarks demonstrated that the privacy-friendly synthetic dataset has a high potential to be used for training face recognition models, achieving, for example, a verification accuracy of 91.87% on LFW using multi-class classification and 99.13% using the combined learning strategy. The training code and the synthetic face image dataset are publicly released11https://github.com/fdbtrs/SFace-Privacy-friendly-and-Accurate-Face-Recognition-using-Synthetic-Data.;SFace: Privacy-friendly and Accurate Face Recognition using Synthetic Data;Not health related;Not health related;0
"J. J. Bird; M. Pritchard; A. Fratini; A. Ekárt; D. R. Faria";2021;Synthetic data augmentation is of paramount importance for machine learning classification, particularly for biological data, which tend to be high dimensional and with a scarcity of training samples. The applications of robotic control and augmentation in disabled and able-bodied subjects still rely mainly on subject-specific analyses. Those can rarely be generalised to the whole population and appear to over complicate simple action recognition such as grasp and release (standard actions in robotic prosthetics and manipulators). We show for the first time that multiple GPT-2 models can machine-generate synthetic biological signals (EMG and EEG) and improve real data classification. Models trained solely on GPT-2 generated EEG data can classify a real EEG dataset at 74.71% accuracy and models trained on GPT-2 EMG data can classify real EMG data at 78.24% accuracy. Synthetic and calibration data are then introduced within each cross validation fold when benchmarking EEG and EMG models. Results show algorithms are improved when either or both additional data are used. A Random Forest achieves a mean 95.81% (1.46) classification accuracy of EEG data, which increases to 96.69% (1.12) when synthetic GPT-2 EEG signals are introduced during training. Similarly, the Random Forest classifying EMG data increases from 93.62% (0.8) to 93.9% (0.59) when training data is augmented by synthetic EMG signals. Additionally, as predicted, augmentation with synthetic biological signals also increases the classification accuracy of data from new subjects that were not observed during training. A Robotiq 2F-85 Gripper was finally used for real-time gesture-based control, with synthetic EMG data augmentation remarkably improving gesture recognition accuracy, from 68.29% to 89.5%.;Synthetic Biological Signals Machine-Generated by GPT-2 Improve the Classification of EEG and EMG Through Data Augmentation;Not health related;Not health related;0
"G. Soltana; M. Sabetzadeh; L. C. Briand";2017;Usage-based statistical testing employs knowledge about the actual or anticipated usage profile of the system under test for estimating system reliability. For many systems, usage-based statistical testing involves generating synthetic test data. Such data must possess the same statistical characteristics as the actual data that the system will process during operation. Synthetic test data must further satisfy any logical validity constraints that the actual data is subject to. Targeting data-intensive systems, we propose an approach for generating synthetic test data that is both statistically representative and logically valid. The approach works by first generating a data sample that meets the desired statistical characteristics, without taking into account the logical constraints. Subsequently, the approach tweaks the generated sample to fix any logical constraint violations. The tweaking process is iterative and continuously guided toward achieving the desired statistical characteristics. We report on a realistic evaluation of the approach, where we generate a synthetic population of citizens' records for testing a public administration IT system. Results suggest that our approach is scalable and capable of simultaneously fulfilling the statistical representativeness and logical validity requirements.;Synthetic data generation for statistical testing;Not health related;Not health related;0
"A. Davari; E. Aptoula; B. Yanikoglu; A. Maier; C. Riess";2018;The amount of training data that is required to train a classifier scales with the dimensionality of the feature data. In hyperspectral remote sensing (HSRS), feature data can potentially become very high dimensional. However, the amount of training data is oftentimes limited. Thus, one of the core challenges in HSRS is how to perform multiclass classification using only relatively few training data points. In this letter, we address this issue by enriching the feature matrix with synthetically generated sample points. These synthetic data are sampled from a Gaussian mixture model (GMM) fitted to each class of the limited training data. Although the true distribution of features may not be perfectly modeled by the fitted GMM, we demonstrate that a moderate augmentation by these synthetic samples can effectively replace a part of the missing training samples. Doing so, the median gain in classification performance is 5% on two datasets. This performance gain is stable for variations in the number of added samples, which makes it easy to apply this method to real-world applications.;GMM-Based Synthetic Samples for Classification of Hyperspectral Images With Limited Training Data;Not health related;Not health related;0
"F. K. Dankar; M. K. Ibrahim; L. Ismail";2022;Synthetic datasets are gradually emerging as solutions for data sharing. Multiple synthetic data generators have been introduced in the last decade fueled by advancement in machine learning and by the increased demand for fast and inclusive data sharing, yet their utility is not well understood. Prior research tried to compare the utility of synthetic data generators using different evaluation metrics. These metrics have been found to generate conflicting conclusions making direct comparison of synthetic data generators very difficult. This paper identifies four criteria (or dimensions) for masked data evaluation by classifying available utility metrics into different categories based on the measure they attempt to preserve: attribute fidelity, bivariate fidelity, population fidelity, and application fidelity. A representative metric from each category is chosen based on popularity and consistency, and the four metrics are used to compare the overall utility of four recent data synthesizers across 19 datasets of different sizes and feature counts. The paper also examines correlations between the selected metrics in an attempt to streamline synthetic data utility.;A Multi-Dimensional Evaluation of Synthetic Data Generators;Not health related;Not health related;1
"S. Mirshekarian; H. Shen; R. Bunescu; C. Marling";2019;"We have shown in previous work that LSTM networks are effective at predicting blood glucose levels in patients with type I diabetes, outperforming human experts and an SVR model trained with features computed by manually engineered physiological models. In this paper we present the results of a much larger set of experiments on real and synthetic datasets in what-if, agnostic, and inertial scenarios. Experiments on a more recent real-patient dataset, which we are releasing to the research community, demonstrate that LSTMs are robust to noise and can easily incorporate additional features, such as skin temperature, heart rate and skin conductance, without any change in the architecture. A neural attention module that we designed specifically for time series prediction improves prediction performance on synthetic data; however, the improvements do not transfer to real data. Conversely, using time of day as an additional input feature consistently improves the LSTM performance on real data but not on synthetic data. These and other differences show that behavior on synthetic data cannot be assumed to always transfer to real data, highlighting the importance of evaluating physiological models on data from real patients.";LSTMs and Neural Attention Models for Blood Glucose Prediction: Comparative Experiments on Real and Synthetic Data;health related;Not health related;1
"Y. Huang; Z. Chen; C. Wen; J. Li; X. -G. Xia; W. Hong";2022;As a wideband radar system, a synthetic aperture radar (SAR) may conflict with several electromagnetic systems, such as frequency modulation (FM), TV, and other communication systems. These signals, termed as radio frequency interference (RFI), may severely interfere SAR systems from generating a high-resolution image. Numerous previous researches focused on the RFI suppression problem, among which the semiparametric methods have been verified to have the state-of-the-art performance. However, most of the semiparametric methods are computationally expensive and can hardly be used on wide-swath SAR imaging processing. In this article, an efficient semiparametric algorithm is proposed to suppress RFIs via alternating projections. It has comparable performance as the other methods but significantly improves the computational efficiency a lot. It is able to remove both narrowband and wideband RFIs and can be used directly on the Level-1 SAR data. Finally, multiple real SAR data are provided to demonstrate the effectiveness and efficiency of the proposed algorithm.;An Efficient Radio Frequency Interference Mitigation Algorithm in Real Synthetic Aperture Radar Data;Not health related;Not health related;0
"M. Pinheiro; M. Rodriguez-Cassola; P. Prats-Iraola; A. Reigber; G. Krieger; A. Moreira";2015;A number of synthetic aperture radar (SAR) systems might work in interrupted operation for different purposes. Examples are cooperative bistatic SAR systems with a synchronization link between the transmitter and receiver or multistatic systems operating in receive-only mode, among others. As a direct consequence, the acquired raw data contain missing echoes presented in a periodical or random pattern. Since the missing raw data introduce artifacts in the processed images, recovery methods have to be applied. Usually, spectral-estimation-based interpolators can be used to recover data. Although such algorithms show good performance for pointlike targets, their efficiency is decreased for distributed scatterers. In this paper, we propose, for a coherent pair of SAR images, the use of the common information in one image to reconstruct the other and vice versa. The conditions required for the proper use of the approach are discussed, and the method is verified using simulated data. One special case of study is the TanDEM-X mission, where the cooperative nature of the bistatic operation requires the periodic exchange of information between the satellites in order to gather information for calibration and synchronization, creating a periodic missing data pattern in the raw data. For this case of study, the reconstruction methods based on spectral estimation are analyzed, and the proposed reconstruction using cross-information is validated.;Reconstruction of Coherent Pairs of Synthetic Aperture Radar Data Acquired in Interrupted Mode;Not health related;Not health related;0
"N. Jaipuria; X. Zhang; R. Bhasin; M. Arafa; P. Chakravarty; S. Shrivastava; S. Manglani; V. N. Murali";2020;Deep Learning has seen an unprecedented increase in vision applications since the publication of large-scale object recognition datasets and introduction of scalable compute hardware. State-of-the-art methods for most vision tasks for Autonomous Vehicles (AVs) rely on supervised learning and often fail to generalize to domain shifts and/or outliers. Dataset diversity is thus key to successful real-world deployment. No matter how big the size of the dataset, capturing long tails of the distribution pertaining to task-specific environmental factors is impractical. The goal of this paper is to investigate the use of targeted synthetic data augmentation - combining the benefits of gaming engine simulations and sim2real style transfer techniques - for filling gaps in real datasets for vision tasks. Empirical studies on three different computer vision tasks of practical use to AVs -parking slot detection, lane detection and monocular depth estimation - consistently show that having synthetic data in the training mix provides a significant boost in cross-dataset generalization performance as compared to training on real data only, for the same size of the training set.;Deflating Dataset Bias Using Synthetic Data Augmentation;Not health related;Not health related;0
"I. . -I. Lin; W. Alpers; V. Khoo; H. Lim; T. K. Lim; D. Kasilingam";2001;"A radar image acquired by the C-band synthetic aperture radar (SAR) aboard the European Remote Sensing satellite ERS-2 over the coastal waters south of Singapore showing radar signatures of a strong tropical squall line (""Sumatra Squall"") is compared with coincident and collocated weather radar data. Squall line features such as the gust front, areas of updraft convergence, and rain areas are identified. Possible attenuation effects from the rain drops in the atmosphere under very heavy rain (rain rate >100 mm/h) is suggested. In addition, the possibility of extracting the associated geophysical parameters, i.e., rain rate and wind speed from SAR imagery is investigated. The rain rate is estimated from the attenuation signature in the SAR image. Comparison between the estimated rain rate and weather radar rain rate shows consistency. Wind speed associated with the squall line is estimated based on the CMOD4 wind scatterometer model. The estimated wind speed pattern appears to be in agreement with the observed squall line structure. Possible errors in the wind estimation due to effects of rain are suggested.";An ERS-1 synthetic aperture radar image of a tropical squall line compared with weather radar data;Not health related;Not health related;0
"Y. Zhou; W. Wang; Z. Chen; P. Wang; H. Zhang; J. Qiu; Q. Zhao; Y. Deng; Z. Zhang; W. Yu; R. Wang";2021;In the Earth observation mission of the synthetic aperture radar (SAR), wide swath can be used to complete global monitoring in a short time and high resolution can provide rich detailed information about the feature space and prominent structure and texture. However, the traditional single-channel classical SAR system cannot meet high-resolution and wide-swath (HRWS) imaging demand due to the constraint of minimum antenna area. Fortunately, this fundamental limitation can be overcome by using multiple receive subapertures in combination with advanced digital beamforming (DBF) technique. DBF in elevation can provide high gain and better system performance and has recently gained much attention in the field of SAR imaging. This article presents a 16-channel in elevation airborne X-band DBF-SAR system with 500-MHz bandwidth, characterized by high speed data acquisition and storage, as a test bed to provide the technical reserves and support for a future spaceborne DBF-SAR system in China. The hardware configuration of this system is designed according to a realistic flight mission. To verify the feasibility and operability of this advanced 16-channel DBF-SAR system, an outfield airborne flight experiment was successfully conducted in eastern Guangdong Province in November 2019. Meanwhile, considering the inevitable channel mismatch from airborne system, a precise strategy as well as the underlying signal processing is proposed to process the experiment data. In addition to the channel mismatch due to the topographic height, the Scan-On-Receive (SCORE) pattern loss (SPL) is also an inherent factor, which will deteriorate the output SNR in final SAR images. Therefore, this article also implements a quantitative assessment of SPL combined with the practical flight parameters and the real airborne data. Finally, the corresponding processing results are presented and analyzed in detail. The practical SNR improvement of 11.23 dB emphasize that DBF technology can significantly improve the quality of SAR images and will make an essential contribution to next generation of HRWS technology for environment monitoring.;Digital Beamforming Synthetic Aperture Radar (DBSAR): Experiments and Performance Analysis in Support of 16-Channel Airborne X-Band SAR Data;Not health related;Not health related;0
"T. Kobayashi; J. -H. Kim; S. R. Lee; A. Kumamoto; H. Nakagawa; S. Oshigami; H. Oya; Y. Yamaguchi; A. Yamaji; T. Ono";2012;Synthetic aperture radar (SAR) processing was applied to the observation data of Lunar Radar Sounder (LRS), which is an HF sounder which was installed onboard a Japanese lunar exploration orbiter, Kaguya, for the purpose of imaging lunar subsurface structure. A two-media model was introduced to the LRS SAR algorithm to define the reference function of the LRS SAR processing. The LRS SAR algorithm has two free parameters, i.e., dielectric constant of the subsurface medium and synthetic aperture. The effect of these free parameters on LRS SAR imaging was studied by simulation and was verified by actual LRS observation data. A practical guideline for LRS SAR processing was drawn. The dielectric constant of the subsurface medium may be ignored in practice so far as the synthetic aperture is smaller than 10 km. For a larger synthetic aperture case, assumption of a moderate dielectric constant (_ = 6 ~ 8) of the subsurface medium is effective in realizing good focusing of deep targets. Finally, taking full advantage of ground processing, advanced processing was attempted. Off-nadir focusing SAR processing proved to be effective in imaging oblique objects whose dominant scattering angle was not the angle toward zenith. Changing the dielectric constant of the two-media model proved to be effective in focusing/defocusing small objects, thus enabling us to localize the object's position as surface or subsurface.;Synthetic Aperture Radar Processing of Kaguya Lunar Radar Sounder Data for Lunar Subsurface Imaging;Not health related;Not health related;0
"T. Eltoft; S. N. Anfinsen; A. P. Doulgeris";2014;A statistical model for multilook polarimetric radar data is presented where the polarimetric channels are associated with individual texture variables having potentially different statistical properties. The feasibility of producing closed-form probability density functions under certain restrictions is outlined. Mellin kind statistics is derived under various assumptions on the texture variables, and the potential for model fit assessment and hypothesis testing in the Mellin domain is demonstrated. Application to real data proves the usefulness of the analytic approach.;A Multitexture Model for Multilook Polarimetric Synthetic Aperture Radar Data;Not health related;Not health related;0
"H. Xie; J. Hu; K. Duan; G. Wang";2020;P-band ultra-wideband bistatic synthetic aperture radar (UWB BSAR) has the well capability of the foliage penetrating, high-resolution imaging and adding the scattered information, which is potential of detecting the concealed target. However, the P-band UWB BSAR raw data is of the huge amount, big spatial-variance, significant range azimuth coupling and complicated motion error, which increases the difficulty of the efficient and precise reconstruction. In this paper, we propose a reconstruction strategy for the P-band UWB BSAR raw data including the motion errors, which can solve the above problems with the high-efficiency and high-precision. This method requires the local beamforming from the raw data as an intermediate processing in the slant range plane instead of ground plane, which can be exactly referenced to the tracks of the transmitter and receiver considering platform altitudes. And, it derives the requirement for selecting the subapertures and subimages by analyzing the bistatic range error considering the motion errors, as well as sampling requirement of the beam for the subimages, which offers a near-optimum tradeoff between the precision and efficiency. Simulated and measured results show that the proposed strategy is effective, and can achieve the near optimal performance with the low computational complexity.;High-Efficiency and High-Precision Reconstruction Strategy for P-Band Ultra-Wideband Bistatic Synthetic Aperture Radar Raw Data Including Motion Errors;Not health related;Not health related;0
"S. E. Kababji; P. Srikantha";2020;Today's electricity grid is rapidly evolving to become highly connected and automated. These advancements have been mainly attributed to the ubiquitous communication/computational capabilities in the grid and the Internet of Things paradigm that is steadily permeating modern society. Another trend is the recent resurgence of machine learning which is especially timely for smart grid applications. However, a major deterrent in effectively utilizing machine learning algorithms is the lack of labelled training data. We overcome this issue in the specific context of smart meter data by proposing a flexible framework for generating synthetic labelled load (e.g., appliance) patterns and usage habits via a non-intrusive novel data-driven approach. We leverage on recent developments in generative adversarial networks (GAN) and kernel density estimators (KDE) to eliminate model-based assumptions that otherwise result in biases. The ensuing synthetic datasets resemble real datasets and lend to rich and diverse training/testing platforms for developing effective machine learning algorithms pertaining to consumer-side energy applications. Theoretical and practical studies presented in this paper highlight the viability and superior performance of the proposed framework.;A Data-Driven Approach for Generating Synthetic Load Patterns and Usage Habits;Not health related;Not health related;0
"H. Hwang; C. Jang; G. Park; J. Cho; I. -J. Kim";2023;To train deep learning models for vision-based action recognition of elders’ daily activities, we need large-scale activity datasets acquired under various daily living environments and conditions. However, most public datasets used in human action recognition either differ from or have limited coverage of elders’ activities in many aspects, making it challenging to recognize elders’ daily activities well by only utilizing existing datasets. Recently, such limitations of available datasets have actively been compensated by generating synthetic data from realistic simulation environments and using those data to train deep learning models. In this paper, based on these ideas we develop ElderSim, an action simulation platform that can generate synthetic data on elders’ daily activities. For 55 kinds of frequent daily activities of the elders, ElderSim generates realistic motions of synthetic characters with various adjustable data-generating options and provides different output modalities including RGB videos, two- and three-dimensional skeleton trajectories. We then generate KIST SynADL, a large-scale synthetic dataset of elders’ activities of daily living, from ElderSim and use the data in addition to real datasets to train three state-of-the-art human action recognition models. From the experiments following several newly proposed scenarios that assume different real and synthetic dataset configurations for training, we observe a noticeable performance improvement by augmenting our synthetic data. We also offer guidance with insights for the effective utilization of synthetic data to help recognize elders’ daily activities.;ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications;Not health related;Not health related;0
"J. Josifovski; M. Kerzel; C. Pregizer; L. Posniak; S. Wermter";2018;Instance-based object detection and fine pose estimation is an active research problem in computer vision. While the traditional interest-point-based approaches for pose estimation are precise, their applicability in robotic tasks relies on controlled environments and rigid objects with detailed textures. CNN-based approaches, on the other hand, have shown impressive results in uncontrolled environments for more general object recognition tasks like category-based coarse pose estimation, but the need of large datasets of fully-annotated training images makes them unfavourable for tasks like instance-based pose estimation. We present a novel approach that combines the robustness of CNNs with a fine-resolution instance-based 3D pose estimation, where the model is trained with fully-annotated synthetic training data, generated automatically from the 3D models of the objects. We propose an experimental setup in which we can carefully examine how the model trained with synthetic data performs on real images of the objects. Results show that the proposed model can be trained only with synthetic renderings of the objects' 3D models and still be successfully applied on images of the real objects, with precision suitable for robotic tasks like object grasping. Based on the results, we present more general insights about training neural models with synthetic images for application on real-world images.;Object Detection and Pose Estimation Based on Convolutional Neural Networks Trained with Synthetic Data;Not health related;Not health related;0
"O. Cífka; U. _im_ekli; G. Richard";2020;Style transfer is the process of changing the style of an image, video, audio clip or musical piece so as to match the style of a given example. Even though the task has interesting practical applications within the music industry, it has so far received little attention from the audio and music processing community. In this article, we present Groove2Groove, a one-shot style transfer method for symbolic music, focusing on the case of accompaniment styles in popular music and jazz. We propose an encoder-decoder neural network for the task, along with a synthetic data generation scheme to supply it with parallel training examples. This synthetic parallel data allows us to tackle the style transfer problem using end-to-end supervised learning, employing powerful techniques used in natural language processing. We experimentally demonstrate the performance of the model on style transfer using existing and newly proposed metrics, and also explore the possibility of style interpolation.;Groove2Groove: One-Shot Music Style Transfer With Supervision From Synthetic Data;Not health related;Not health related;0
"J. -H. Kim; Y. Hwang";2022;Recently, convolutional neural networks (CNNs) have achieved state-of-the-art performance in infrared small target detection. However, the limited number of public training data restricts the performance improvement of CNN-based methods. To handle the scarcity of training data, we propose a method that can generate synthetic training data for infrared small target detection. We adopt the generative adversarial network framework where synthetic background images and infrared small targets are generated in two independent processes. In the first stage, we synthesize infrared images by transforming visible images into infrared ones. In the second stage, target masks are implanted on the transformed images. Then, the proposed intensity modulation network synthesizes realistic target objects that can be diversely generated from further image processing. Experimental results on the recent public dataset show that, when we train various detection networks using the dataset composed of both real and synthetic images, detection networks yield better performance than using real data only.;GAN-Based Synthetic Data Augmentation for Infrared Small Target Detection;Not health related;Not health related;0
"B. Kiefer; D. Ott; A. Zell";2022;Acquiring data to train deep learning-based object detectors on Unmanned Aerial Vehicles (UAVs) is expensive, time-consuming and may even be prohibited by law in specific environments. On the other hand, synthetic data is fast and cheap to access. In this work, we explore the potential use of synthetic data in object detection from UAVs across various application environments. For that, we extend the open-source framework DeepGTAV to work for UAV scenarios. We capture various large-scale high-resolution synthetic data sets in several domains to demonstrate their use in real-world object detection from UAVs by analyzing multiple training strategies across several models. Furthermore, we analyze several different data generation and sampling parameters to provide actionable engineering advice for further scientific research. The DeepGTAV framework is available at https://git.io/Jyf5j.;Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles;Not health related;Not health related;0
"X. Zheng; B. Wang; D. Kalathil; L. Xie";2021;A two-stage machine learning-based approach for creating synthetic phasor measurement unit (PMU) data is proposed in this article. This approach leverages generative adversarial networks (GAN) in data generation and incorporates neural ordinary differential equation (Neural ODE) to guarantee underlying physical meaning. We utilize this approach to synthetically create massive eventful PMU data, which would otherwise be difficult to obtain from the real world due to the critical energy infrastructure information (CEII) protection. To illustrate the utility of such synthetic data for subsequent data-driven methods, we specifically demonstrate the application of using synthetic PMU data for event classification by scaling up the real data set. The addition of the synthetic PMU data to a small set of real PMU data is shown to have improved the event classification accuracy by 2 to 5 percent.;Generative Adversarial Networks-Based Synthetic PMU Data Creation for Improved Event Classification;Not health related;Not health related;0
"F. Boutros; N. Damer; A. Kuijper";2022;Deep learning-based face recognition models follow the common trend in deep neural networks by utilizing full-precision floating-point networks with high computational costs. Deploying such networks in use-cases constrained by computational requirements is often infeasible due to the large memory required by the full-precision model. Previous compact face recognition approaches proposed to design special compact architectures and train them from scratch using real training data, which may not be available in a real-world scenario due to privacy concerns. We present in this work the QuantFace solution based on low-bit precision format model quantization. QuantFace reduces the required computational cost of the existing face recognition models without the need for designing a particular architecture or accessing real training data. QuantFace introduces privacy-friendly synthetic face data to the quantization process to mitigate potential privacy concerns and issues related to the accessibility to real training data. Through extensive evaluation experiments on seven benchmarks and four network architectures, we demonstrate that QuantFace can successfully reduce the model size up to 5x while maintaining, to a large degree, the verification performance of the full-precision model without accessing real training datasets. All training codes are publicly available 1.;QuantFace: Towards Lightweight Face Recognition by Synthetic Data Low-bit Quantization;Not health related;Not health related;0
"A. Bernardo; H. M. Gomes; J. Montiel; B. Pfahringer; A. Bifet; E. D. Valle";2020;Streaming Machine Learning (SML) studies single-pass learning algorithms that update their models one data item at a time given an unbounded and often non-stationary flow of data (a.k.a., in presence of concept drift). Online class imbalance learning is a branch of SML that combines the challenges of both class imbalance and concept drift. In this paper, we investigate the binary classification problem of rebalancing an imbalanced stream of data in the presence of concept drift, accessing one sample at a time. We propose Continuous Synthetic Minority Oversampling Technique (C-SMOTE), a novel rebalancing meta-strategy to pipeline with SML classification algorithms. C-SMOTE is inspired by the popular SMOTE algorithm but operates continuously. We benchmark C-SMOTE pipelines on ten different groups of data streams. We bring empirical evidence that models learnt with C-SMOTE pipelines outperform models trained on imbalanced data stream without losing the ability to deal with concept drifts. Moreover, we show that they outperform other stream balancing techniques from the literature.;C-SMOTE: Continuous Synthetic Minority Oversampling for Evolving Data Streams;Not health related;Not health related;0
"F. Skopik; G. Settanni; R. Fiedler; I. Friedberg";2014;Threats to modern ICT systems are rapidly changing these days. Organizations are not mainly concerned about virus infestation, but increasingly need to deal with targeted attacks. This kind of attacks are specifically designed to stay below the radar of standard ICT security systems. As a consequence, vendors have begun to ship self-learning intrusion detection systems with sophisticated heuristic detection engines. While these approaches are promising to relax the serious security situation, one of the main challenges is the proper evaluation of such systems under realistic conditions during development and before roll-out. Especially the wide variety of configuration settings makes it hard to find the optimal setup for a specific infrastructure. However, extensive testing in a live environment is not only cumbersome but usually also impacts daily business. In this paper, we therefore introduce an approach of an evaluation setup that consists of virtual components, which imitate real systems and human user interactions as close as possible to produce system events, network flows and logging data of complex ICT service environments. This data is a key prerequisite for the evaluation of modern intrusion detection and prevention systems. With these generated data sets, a system's detection performance can be accurately rated and tuned for very specific settings.;Semi-synthetic data set generation for security software evaluation;Not health related;Not health related;0
"W. Chapman; S. Ranka; S. Sahni; M. Schmalz; U. Majumder; L. Moore; B. Elton";2011;This paper presents a design for parallel processing of synthetic aperture radar (SAR) data using one or more Graphics Processing Units (GPUs). Our design supports real- time reconstruction of a two-dimensional image from a matrix of echo pulses and their corresponding response values. Key to our design is a dual partitioning scheme that divides the output image into tiles and divides the input matrix into sets of pulses. Pairs comprised of an image tile and a pulse set are distributed to thread blocks in a GPU, thus facilitating parallel computation. Memory access latency is masked by the GPU's low-latency thread scheduling. Our performance analysis quantifies latency as a function of the input and output parameters. Experimental results were generated with an nVidia Tesla C2050 GPU having maximum throughput of 1030 Gflop/s. Our design achieves peak throughput of 293 Gflop/s, which scales well for output image sizes from 2,048 _ 2,048 pixels to 4,096 _ 4,096 pixels. Higher throughput can be obtained by distributing the pulse matrix across multiple GPUs and combining the results at a host device.;Parallel processing techniques for the processing of synthetic aperture radar data on GPUs;Not health related;Not health related;0
"P. Imperatore; A. Pepe; R. Lanari";2016;This paper describes a general-purpose parallel scheme for efficiently focusing synthetic aperture radar (SAR) data on multicore-based shared-memory architectures. The rationale of the proposed tiling-based parallel focusing model is first discussed, and then, its implementation structure is illustrated. The adopted parallel solution, which is based on a canonical processing pattern, exploits a segmented-block-based approach and works successfully on data acquired by different spaceborne SAR platforms. Insofar as a significant portion of the focusing algorithm is amenable to tiling, our approach decomposes the problem into simpler subproblems of the same type, also providing a suitable mechanism to explicitly control the granularity of computation through the proper specification of the tiling at the different stages of the algorithm itself. Relevant implementation makes use of multithreading and high-performance libraries. Achievable performances are then experimentally investigated by quantifying the benefit of the parallelism incorporated into the prototype solution, thus demonstrating the validity of our approach. Accordingly, canonical performance metrics have been evaluated, and the pertinent scalability has been examined on different multicore architectures. Furthermore, in order to emphasize the practical ability of the proposed parallel model implementation to efficiently deal with data of different SAR sensors, a performance analysis has been carried out in different realistic scenarios including data acquired by the Envisat/ASAR, RADARSAT-1, and COSMO-SkyMed platforms.;Spaceborne Synthetic Aperture Radar Data Focusing on Multicore-Based Architectures;Not health related;Not health related;0
"M. B. Stuart; P. M. Jensen; J. T. R. Olsen; A. B. Kristensen; M. Schou; B. Dammann; H. H. B. Sørensen; J. A. Jensen";2021;Two delay-and-sum beamformers for 3-D synthetic aperture imaging with row-column addressed arrays are presented. Both beamformers are software implementations for graphics processing unit (GPU) execution with dynamic apodizations and third-order polynomial subsample interpolation. The first beamformer was written in the MATLAB programming language and the second was written in C/C++ with the compute unified device architecture (CUDA) extensions by NVIDIA. Performance was measured as volume rate and sample throughput on three different GPUs: a 1050 Ti, a 1080 Ti, and a TITAN V. The beamformers were evaluated across 112 combinations of output geometry, depth range, transducer array size, number of virtual sources, floating-point precision, and Nyquist rate or in-phase/quadrature beamforming using analytic signals. Real-time imaging defined as more than 30 volumes per second was attained by the CUDA beamformer on the three GPUs for 13, 27, and 43 setups, respectively. The MATLAB beamformer did not attain real-time imaging for any setup. The median, single-precision sample throughput of the CUDA beamformer was 4.9, 20.8, and 33.5 Gsamples/s on the three GPUs, respectively. The throughput of CUDA beamformer was an order of magnitude higher than that of the MATLAB beamformer.;Real-Time Volumetric Synthetic Aperture Software Beamforming of Row–Column Probe Data;Not health related;Not health related;0
"Y. Xu; S. Arai; F. Tokuda; K. Kosuge";2020;3D Instance segmentation is a fundamental task in computer vision. Effective segmentation plays an important role in robotic tasks, augmented reality, autonomous driving, etc. With the ascendancy of convolutional neural networks in 2D image processing, the use of deep learning methods to segment 3D point clouds receives much attention. A great convergence of training loss often requires a large amount of human-annotated data, while making such a 3D dataset is time-consuming. This paper proposes a method for training convolutional neural networks to predict instance segmentation results using synthetic data. The proposed method is based on the SGPN framework. We replaced the original feature extractor with “dynamic graph convolutional neural networks” that learned how to extract local geometric features and proposed a simple and effective loss function, making the network more focused on hard examples. We experimentally proved that the proposed method significantly outperforms the state-of-the-art method in both Stanford 3D Indoor Semantics Dataset and our datasets.;A Convolutional Neural Network for Point Cloud Instance Segmentation in Cluttered Scene Trained by Synthetic Data Without Color;Not health related;Not health related;0
"S. -b. Kim; M. Arii; T. Jackson";2017;L-band airborne synthetic aperture radar observations were made over California shrublands to better understand the effects of soil and vegetation parameters on backscattering coefficient (_0). Temporal changes in _0 of up to 3 dB were highly correlated to surface soil moisture but not to vegetation, even though vegetation water content (VWC) varied seasonally by a factor of two. HH was always greater than VV, suggesting the importance of double-bounce scattering by the woody parts. However, the geometric and dielectric properties of the woody parts did not vary significantly over time. Instead the changes in VWC occurred primarily in thin leaves that may not meaningfully influence absorption and scattering. A physically based model for single scattering by discrete elements of plants successfully simulated the magnitude of the temporal variations in HH, VV, and HH/VV with a difference of less than 0.9 dB for both the mean and standard deviation when compared with the airborne data. In order to simulate the observations, the VWC input of the plant to the model was formulated as a function of plant's dielectric property (water fraction) while the plant geometry remains static in time. In comparison, when the VWC input was characterized by the geometry of a growing plant, the model performed poorly in describing the observed patterns in the _0 changes. The modeling results offer explanation of the observation that soil moisture correlated highly with _0: the dominant mechanisms for HH and VV are double-bounce scattering by trunk, and soil surface scattering, respectively. The time-series inversion of the physical model was able to retrieve soil moisture with the difference of -0.037 m3/m3 (mean), 0.025 m3/m3 (standard deviation), and 0.89 (correlation), which demonstrates the efficacy of the model-based time-series soil moisture retrieval for shrublands.;Modeling L-Band Synthetic Aperture Radar Data Through Dielectric Changes in Soil Moisture and Vegetation Over Shrublands;Not health related;Not health related;0
"M. Hittmeir; A. Ekelhart; R. Mayer";2019;With ever increasing capacity for collecting, storing, and processing of data, there is also a high demand for intelligent data analysis methods. While there have been impressive advances in machine learning and similar domains in recent years, this also gives rise to concerns regarding the protection of personal and otherwise sensitive data, especially if it is to be analysed by third parties. Besides anonymisation, which becomes challenging with high dimensional data, one approach for privacy-preserving data mining lies in the usage of synthetic data, which comes with the promise of protecting the users’ data and producing analysis results close to those achieved by using real data. In this paper, we analyse a number of different approaches for creating synthetic data, and study the utility of the created datasets for regression tasks, i.e. the prediction of a numeric value. We further investigate the similarity of real and synthetic data samples. Finally, we contribute to privacy assessments and measurements of the risk of attribute disclosure on synthetic data by extending an approach developed for categorical data.;Utility and Privacy Assessments of Synthetic Data for Regression Tasks;Not health related;Not health related;1
"B. Thamsen; P. Yevtushenko; L. Gundelwein; A. A. A. Setio; H. Lamecker; M. Kelm; M. Schafstedde; T. Heimann; T. Kuehne; L. Goubergrits";2021;Modeling of hemodynamics and artificial intelligence have great potential to support clinical diagnosis and decision making. While hemodynamics modeling is extremely time- and resource-consuming, machine learning (ML) typically requires large training data that are often unavailable. The aim of this study was to develop and evaluate a novel methodology generating a large database of synthetic cases with characteristics similar to clinical cohorts of patients with coarctation of the aorta (CoA), a congenital heart disease associated with abnormal hemodynamics. Synthetic data allows use of ML approaches to investigate aortic morphometric pathology and its influence on hemodynamics. Magnetic resonance imaging data (154 patients as well as of healthy subjects) of aortic shape and flow were used to statistically characterize the clinical cohort. The methodology generating the synthetic cohort combined statistical shape modeling of aortic morphometry and aorta inlet flow fields and numerical flow simulations. Hierarchical clustering and non-linear regression analysis were successfully used to investigate the relationship between morphometry and hemodynamics and to demonstrate credibility of the synthetic cohort by comparison with a clinical cohort. A database of 2652 synthetic cases with realistic shape and hemodynamic properties was generated. Three shape clusters and respective differences in hemodynamics were identified. The novel model predicts the CoA pressure gradient with a root mean square error of 4.6 mmHg. In conclusion, synthetic data for anatomy and hemodynamics is a suitable means to address the lack of large datasets and provide a powerful basis for ML to gain new insights into cardiovascular diseases.;Synthetic Database of Aortic Morphometry and Hemodynamics: Overcoming Medical Imaging Data Availability;health related;health related;1
"A. K. Bharati; V. Ajjarapu";2022;The power grid is transforming with large amounts of distributed energy resource (DER) integration that is impacting the bulk power system planning and operations. Grid regulators have identified that traditional power system analysis techniques are challenged under the changing grid conditions. Transmission and distribution (T&D) co-simulation is an effective methodology to accurately account for the changing power-grid. A scalable multitimescale T&D (SMTD) co-simulation framework is developed with commercial transmission system solver that can handle large-scale system models, and distribution system solver that can model the distribution systems in detail with dynamic models of load and DER. In this article, PSS/E and GridLAB-D are combined using a Python interface with Hierarchical Engine for Large-scale Infrastructure Co-Simulation software to enable T&D co-simulation. This framework is scalable and parallel computing compatible that enables large-scale system simulation and adoption of T&D co-simulation by the industry and utilities. The SMTD cosimulator is designed to integrate with existing system models and new ones. The article also highlights functional specifications for the T&D co-simulation solvers and models along with providing the algorithms for the steady state, quasi-steady state, and dynamic T&D co-simulation. In context of changing power grid, future power grid planning, validation of controls under critical contingencies and validation of various wide area monitoring and controls can effectively utilize the proposed framework that can capture the T&D interactions accurately. The SMTD cosimulator can also be used to generate realistic synthetic measurement-data with noise and packet drop for uncommon grid events enabling development and validation of power system data analytics and controls.;SMTD Co-Simulation Framework With HELICS for Future-Grid Analysis and Synthetic Measurement-Data Generation;Not health related;Not health related;0
"L. De Laurentiis; C. E. Jones; B. Holt; G. Schiavon; F. Del Frate";2021;Studies of oil slicks in the ocean environment with synthetic aperture radar (SAR) have found that one of the most complex challenges to oil spill detection is the separation of mineral oil spills from slicks that are biogenic in origin. The possible occurrence of multiple scattering mechanisms beyond Bragg scattering for the sea surface, with or without biogenic or mineral oil slicks, and even under low to moderate wind conditions, has also been a subject of debate because the measured signals from these radar-dark surfaces can be contaminated easily by noise. Therefore, the use of noise-uncontaminated data is required for oil spill study in order to avoid significant alteration in the measured radar backscatter, which can lead to misinterpretation and misclassification of the scattering mechanisms involved. To this end, this study uses uninhabited aerial vehicle SAR data, with a noise-equivalent sigma zero as low as _53 dB, to investigate slick classification within a deep learning framework in order to assess deep architectures’ capabilities for providing a reliable and accurate three-state classifier capable of separating mineral oil films from biogenic slicks and from the clean sea. The study exploits parameters with sensitivity to the dielectric constant and ocean wave damping properties, and convolutional neural networks’ (CNNs’) capability for learning nonlinear features, shapes, and textural and statistical patterns, in order to obtain significant classification accuracy. Very high accuracy results have been achieved, with values up to 0.91, 0.94, 0.98, and 0.99 under the most probable real-world spill acquisition conditions.;Deep Learning for Mineral and Biogenic Oil Slick Classification With Airborne Synthetic Aperture Radar Data;Not health related;Not health related;0
"F. M. Carlucci; P. Russo; B. Caputo";2017;Convolutional Neural Networks (CNNs) trained on large scale RGB databases have become the secret sauce in the majority of recent approaches for object categorization from RGB-D data. Thanks to colorization techniques, these methods exploit the filters learned from 2D images to extract meaningful representations in 2.5D. Still, the perceptual signature of these two kind of images is very different, with the first usually strongly characterized by textures, and the second mostly by silhouettes of objects. Ideally, one would like to have two CNNs, one for RGB and one for depth, each trained on a suitable data collection, able to capture the perceptual properties of each channel for the task at hand. This has not been possible so far, due to the lack of a suitable depth database. This paper addresses this issue, proposing to opt for synthetically generated images rather than collecting by hand a 2.5D large scale database. While being clearly a proxy for real data, synthetic images allow to trade quality for quantity, making it possible to generate a virtually infinite amount of data. We show that the filters learned from such data collection, using the very same architecture typically used on visual data, learns very different filters, resulting in depth features (a) able to better characterize the different facets of depth images, and (b) complementary with respect to those derived from CNNs pre-trained on 2D datasets. Experiments on two publicly available databases show the power of our approach.;A deep representation for depth images from synthetic data;Not health related;Not health related;0
"D. L. Schuler; J. Lee; T. L. Ainsworth; M. R. Grunes";2000;A method has been investigated for the measurement of topography using airborne fully polarimetric synthetic aperture radar (SAR) data. Terrain slopes in both the range and azimuthal directions have been estimated using multipass flight geometries. Using these slope values, the Poisson equation was then solved to create a Digital Elevation Model (DEM) of the terrain topography. The method measures polarimetric orientation angles which are then converted into terrain slopes in the azimuthal direction. The conversion of these orientation angles into terrain slopes requires additional knowledge of the radar look angle and the range direction terrain slopes. The solution for slopes is, therefore, a problem coupled between the range and azimuthal directions. For specialized multipass flight geometries these orthogonal terrain slopes are solved for, and maps of terrain slopes are produced. In particular, the processing of two-pass orthogonal and two-pass antiparallel (headings _ and _+ 180°) NASA - Jet Propulsion Laboratory airborne SAR data sets has been carried out for an area in central California. When orthogonal slopes are derived using either of these data sets, a digital elevation model may be generated. The L band, polarimetric SAR (POLSAR) DEM created by this solution is compared to a coregistered C band, interferometric SAR (IFSAR) DEM. Similar comparisons are made for terrain slopes in the azimuthal - range directions which are generated by the POLSAR and IFSAR elevation data. The polarimetric SAR, operating from an aircraft, or satellite, in a strip-mapping mode, is capable of measuring terrain topography for large areas provided that phase-preserving fully polarimetric data are taken. Polarimetric SAR data are also widely used for studies of crop classification, surface roughness, biomass density, and soil moisture content. All of these studies are adversely affected by scattering changes attributable to topography. The technique investigated here potentially offers a means to correct for these effects by making simultaneous, coregistered estimates of local orthogonal terrain slopes.;Terrain topography measurement using multipass polarimetric synthetic aperture radar data;Not health related;Not health related;0
"J. Lambrecht; L. Kästner";2019;Pose estimation is a necessity for many applications in robotics incorporating interaction between the robot and external camera-equipped devices, e.g. mobile robots or Augmented Reality devices. In the practice of monocular cameras, one mostly takes advantage of pose estimation through fiducial marker detection. We propose a novel approach for marker-less robot pose estimation through monocular cameras utilizing 2D keypoint detection and 3D keypoint determination through readings from the encoders and forward kinematics. In particular, 2D-3D point correspondences enable the pose estimation through solving the Perspective-n-Point problem for calibrated cameras. The method does not rely on any depth data or initializations. The robust 2D keypoint detection is implemented by modern Convolutional Neural Networks trained on different dataset configurations of real and synthetic data in order to quantitatively evaluate robustness, precision and data efficiency. We demonstrate that the method provides robust pose estimation for random joint poses and benchmark the performance of different (synthetic) dataset configurations. Furthermore, we compare the accuracies to marker pose estimation and give an outlook towards enhancements and realtime capability.;Towards the Usage of Synthetic Data for Marker-Less Pose Estimation of Articulated Robots in RGB Images;Not health related;Not health related;0
M. Koziarski;2021;In this paper we propose a novel data-level algorithm for handling data imbalance in the classification task, Synthetic Majority Undersampling Technique (SMUTE). SMUTE leverages the concept of interpolation of nearby instances, previously introduced in the oversampling setting in SMOTE. Furthermore, we combine both in the Combined Synthetic Oversampling and Undersampling Technique (CSMOUTE), which integrates SMOTE oversampling with SMUTE undersampling. The results of the conducted experimental study demonstrate the usefulness of both the SMUTE and the CSMOUTE algorithms, especially when combined with more complex classifiers, namely MLP and SVM, and when applied on datasets consisting of a large number of outliers. This leads us to a conclusion that the proposed approach shows promise for further extensions accommodating local data characteristics, a direction discussed in more detail in the paper.;CSMOUTE: Combined Synthetic Oversampling and Undersampling Technique for Imbalanced Data Classification;Not health related;Not health related;0
"N. Li; Z. Lv; Z. Guo";2022;"In complex electromagnetic environments, synthetic aperture radar (SAR) is severely affected by radio frequency interference (RFI) from other systems, such as ground-based radar, cellular networks, and global positioning systems, and this interference cannot be neglected. Pulse RFI (PRFI), a common form of RFI, can hinder SAR signal processing and image interpretation to varying degrees. The time-domain notch filtering method designed for mitigating PRFI can locate and mitigate the evident PRFI covered in SAR echo data, but it is helpless against PRFI hidden in a strong echo signal. In this article, a three-step approach is proposed to tackle the PRFI problem. In the proposed approach, the first step is to detect and locate PRFI; this is based on eigenvalue decomposition (EVD) and the short-time Fourier transform (STFT). The second step is to notch PRFI, and this is based on a time-domain notch filter. The third step is to recover the notched signal using a novel matrix completion strategy, which integrates with a robust low-rank matrix completion (LRMC) technique—i.e.,the singular value thresholding (SVT) algorithm—and a well-known Lagrange interpolation technique. Experimental results via simulated SAR data, Sentinel-1 level-0 raw data, and L-band airborne SAR raw data demonstrate the performance of the proposed approach.";Pulse RFI Mitigation in Synthetic Aperture Radar Data via a Three-Step Approach: Location, Notch, and Recovery;Not health related;Not health related;0
"L. Ren; J. Yang; A. A. Mouche; H. Wang; G. Zheng; J. Wang; H. Zhang; X. Lou; P. Chen";2019;This paper assesses different retrieval schemes used for the Chinese Gaofen-3 Synthetic Aperture Radar (GF-3 SAR) co-polarized data. The data consist of 4186 GF-3 data points and collocated wind information from sources including the ASCAT scatterometer, HY2A-SCAT scatterometer, and National Data Buoy Center (NDBC) buoy wind data set. The VV-polarized geophysical model function (GMF) is a CMOD7 model while the HH-polarized GMF is a hybrid of the CMOD7 and PR model. Assessments involve comparisons between SAR-derived and collocated winds in terms of the root-mean-square difference (RMSD) and bias. First, a comparison between the two retrieval schemes for the VV-polarized data clearly shows that the optimal scheme performs better than the classical scheme for wind speed retrieval. Comparisons for HH-polarized data show similar results. These experiments indicate that the wind speed RMSDs for the GF-3 co-polarized data are within 2 m/s when using the optimal scheme. Moreover, the wind direction RMSDs from the two schemes have no significant difference, with values near 20°. Overall, these assessments indicate that the GF-3 co-polarized data are sufficient for operational wind speed retrieval using the optimal scheme. However, wind direction retrieval requires further improvement.;Assessments of Ocean Wind Retrieval Schemes Used for Chinese Gaofen-3 Synthetic Aperture Radar Co-Polarized Data;Not health related;Not health related;0
"H. Zhang; M. Grimmer; R. Ramachandra; K. Raja; C. Busch";2021;Face verification has come into increasing focus in various applications including the European Entry/Exit System, which integrates face recognition mechanisms. At the same time, the rapid advancement of biometric authentication requires extensive performance tests in order to inhibit the discriminatory treatment of travellers due to their demographic background. However, the use of face images collected as part of border controls is restricted by the European General Data Protection Law to be processed for no other reason than its original purpose. Therefore, this paper investigates the suitability of synthetic face images generated with StyleGAN and StyleGAN2 to compensate for the urgent lack of publicly available largescale test data. Specifically, two deep learning-based (SER-FIQ, FaceQnet v1) and one standard-based (ISO/IEC TR 29794-5) face image quality assessment algorithm is utilized to compare the applicability of synthetic face images compared to real face images extracted from the FRGC dataset. Finally, based on the analysis of impostor score distributions and utility score distributions, our experiments reveal negligible differences between StyleGAN vs. StyleGAN2, and further also minor discrepancies compared to real face images.;On the Applicability of Synthetic Data for Face Recognition;Not health related;Not health related;0
"T. Björklund; A. Fiandrotti; M. Annarumma; G. Francini; E. Magli";2017;We present an Automatic License Plate Recognition system designed around Convolutional Neural Networks (CNNs) and trained over synthetic plate images. We first design CNNs suitable for plate and character detection, sharing a common architecture and training procedure. Then, we generate synthetic images that account for the varying illumination and pose conditions encountered with real plate images and we use exclusively such synthetic images to train our CNNs. Experiments with real vehicle images captured in natural light with commodity imaging systems show precision and recall in excess of 93% despite our networks are trained exclusively on synthetic images.;Automatic license plate recognition with convolutional neural networks trained on synthetic data;Not health related;Not health related;1
"D. Park; J. Lee; J. Lee; K. Lee";2021;In the process of intelligently segmenting foods in images using deep neural networks for diet management, data collection and labeling for network training are very important but labor-intensive tasks. In order to solve the difficulties of data collection and annotations, this paper proposes a food segmentation method applicable to real-world through synthetic data. To perform food segmentation on healthcare robot systems, such as meal assistance robot arm, we generate synthetic data using the open-source 3D graphics software Blender placing multiple objects on meal plate and train Mask R-CNN for instance segmentation. Also, we build a data collection system and verify our segmentation model on real-world food data. As a result, on our real-world dataset, the model trained only synthetic data is available to segment food instances that are not trained with 52.2% mask AP@all, and improve performance by +6.4%p after fine-tuning comparing to the model trained from scratch. In addition, we also confirm the possibility and performance improvement on the public dataset for fair analysis.;Deep Learning based Food Instance Segmentation using Synthetic Data;Not health related;Not health related;0
"S. Thalhammer; T. Patten; M. Vincze";2019;Object pose estimation is an important problem in robotics because it supports scene understanding and enables subsequent grasping and manipulation. Many methods, including modern deep learning approaches, exploit known object models, however, in industry these are difficult and expensive to obtain. 3D CAD models, on the other hand, are often readily available. Consequently, training a deep architecture for pose estimation exclusively from CAD models leads to a considerable decrease of the data creation effort. While this has been shown to work well for feature-and template-based approaches, real-world data is still required for pose estimation in clutter using deep learning. We use synthetically created depth data with domain-relevant background randomized noise heuristics to train an end-to-end, multi-task network, for pose estimation. We simultaneously detect, classify and estimate the poses of texture-less objects in cluttered real-world depth images of an arbitrary amount of objects. We present the results of our experiments with the LineMOD and the Occlusion dataset.;SyDPose: Object Detection and Pose Estimation in Cluttered Real-World Depth Images Trained using Only Synthetic Data;Not health related;Not health related;0
"R. Maciejewski; R. Hafen; S. Rudolph; G. Tebbetts; W. S. Cleveland; S. J. Grannis; D. S. Ebert";2009;This system generates synthetic syndromic-surveillance data for evaluating visualization and visual-analytics techniques. Modeling data from emergency room departments, the system generates two years of patient data, into which system users can inject spatiotemporal disease outbreak signals. The result is a data set with known seasonal trends and irregular outbreak patterns.;Generating Synthetic Syndromic-Surveillance Data for Evaluating Visual-Analytics Techniques;health related;Not health related;1
"Q. Yang; Y. Lin; J. Wang; J. Bao; X. Wang; L. Ma; Z. Zhou; Q. Yang; S. Cai; H. He; C. Cai; J. Dong; J. Cheng; Z. Chen; J. Zhong";2022;Use of synthetic data has provided a potential solution for addressing unavailable or insufficient training samples in deep learning-based magnetic resonance imaging (MRI). However, the challenge brought by domain gap between synthetic and real data is usually encountered, especially under complex experimental conditions. In this study, by combining Bloch simulation and general MRI models, we propose a framework for addressing the lack of training data in supervised learning scenarios, termed MOST-DL. A challenging application is demonstrated to verify the proposed framework and achieve motion-robust  $\text{T}_{{2}}$  mapping using single-shot overlapping-echo acquisition. We decompose the process into two main steps: (1) calibrationless parallel reconstruction for ultra-fast pulse sequence and (2) intra-shot motion correction for  $\text{T}_{{2}}$  mapping. To bridge the domain gap, realistic textures from a public database and various imperfection simulations were explored. The neural network was first trained with pure synthetic data and then evaluated with in vivo human brain. Both simulation and in vivo experiments show that the MOST-DL method significantly reduces ghosting and motion artifacts in  $\text{T}_{{2}}$  maps in the presence of unpredictable subject movement and has the potential to be applied to motion-prone patients in the clinic. Our code is available at https://github.com/qinqinyang/MOST-DL.;MOdel-Based SyntheTic Data-Driven Learning (MOST-DL): Application in Single-Shot T2 Mapping With Severe Head Motion Using Overlapping-Echo Acquisition;Not health related;Not health related;0
"T. Linder; K. Y. Pfeiffer; N. Vaskevicius; R. Schirmer; K. O. Arras";2020;While 2D object detection has made significant progress, robustly localizing objects in 3D space under presence of occlusion is still an unresolved issue. Our focus in this work is on real-time detection of human 3D centroids in RGB-D data. We propose an image-based detection approach which extends the YOLO v3 architecture with a 3D centroid loss and mid-level feature fusion to exploit complementary information from both modalities. We employ a transfer learning scheme which can benefit from existing large-scale 2D object detection datasets, while at the same time learning end-to-end 3D localization from our highly randomized, diverse synthetic RGB-D dataset with precise 3D groundtruth. We further propose a geometrically more accurate depth-aware crop augmentation for training on RGB-D data, which helps to improve 3D localization accuracy. In experiments on our challenging intralogistics dataset, we achieve state-of-the-art performance even when learning 3D localization just from synthetic data.;Accurate detection and 3D localization of humans using a novel YOLO-based RGB-D fusion approach and synthetic training data;Not health related;Not health related;0
"M. Leibovich; G. Papanicolaou; C. Tsogka";2020;We analyze synthetic aperture radar (SAR) imaging of complex ground scenes that contain both stationary and moving targets. In the usual SAR acquisition scheme, we consider ways to preprocess the data so as to separate the contributions of the moving targets from those due to stationary background reflectors. Both components of the data, that is, reflections from stationary and moving targets, are considered as signal and are needed for target imaging and tracking, respectively. The approach we use is to decompose the data matrix into a low rank and a sparse part. This decomposition enables us to capture the reflections from moving targets into the sparse part and those from stationary targets into the low rank part of the data. The computational tool for this is robust principal component analysis (RPCA) applied to the SAR data matrix. We also introduce a lossless baseband transformation of the data, which simplifies the analysis and improves the performance of the RPCA algorithm. A modified version of RPCA, the stable principal component pursuit (PCP), is robust to additive noise. Our main contribution is a theoretical analysis that determines an optimal choice of parameters for the RPCA algorithm so as to have an effective and stable separation of SAR data coming from moving and stationary targets. This analysis also gives a lower bound for detectable target velocities. We show in particular that the rank of the sparse matrix is proportional to the square root of the target's speed in the direction that connects the SAR platform trajectory to the imaging region. The robustness of the approach is illustrated with numerical simulations in the X-band SAR regime.;Low Rank Plus Sparse Decomposition of Synthetic Aperture Radar Data for Target Imaging;Not health related;Not health related;0
"F. Alharbi; L. Ouarbya; J. A. Ward";2020;Human activity recognition (HAR) based on wearable sensors has emerged as an active topic of research in machine learning and human behavior analysis because of its applications in several fields, including health, security and surveillance, and remote monitoring. Machine learning algorithms are frequently applied in HAR systems to learn from labeled sensor data. The effectiveness of these algorithms generally relies on having access to lots of accurately labeled training data. But labeled data for HAR is hard to come by and is often heavily imbalanced in favor of one or other dominant classes, which in turn leads to poor recognition performance. In this study we introduce a generative adversarial network (GAN)-based approach for HAR that we use to automatically synthesize balanced and realistic sensor data. GANs are robust generative networks, typically used to create synthetic images that cannot be distinguished from real images. Here we explore and construct a model for generating several types of human activity sensor data using a Wasserstein GAN (WGAN). We assess the synthetic data using two commonly-used classifier models, Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM). We evaluate the quality and diversity of the synthetic data by training on synthetic data and testing on real sensor data, and vice versa. We then use synthetic sensor data to oversample the imbalanced training set. We demonstrate the efficacy of the proposed method on two publicly available human activity datasets, the Sussex-Huawei Locomotion (SHL) and Smoking Activity Dataset (SAD). We achieve improvements of using WGAN augmented training data over the imbalanced case, for both SHL (0.85 to 0.95 F1-score), and for SAD (0.70 to 0.77 F1-score) when using a CNN activity classifier.;Synthetic Sensor Data for Human Activity Recognition;health related;Not health related;1
"B. Hurl; R. Cohen; K. Czarnecki; S. Waslander";2020;Inter-vehicle communication for autonomous vehicles (AVs) stands to provide significant benefits in terms of perception robustness. We propose a novel approach for AVs to communicate perceptual observations, tempered by trust modelling of peers providing reports. Based on the accuracy of reported object detections as verified locally, communicated messages can be fused to augment perception performance beyond line of sight and at great distance from the ego vehicle. Also presented is a new synthetic dataset which can be used to test cooperative perception. The TruPercept dataset includes unreliable and malicious behaviour scenarios to experiment with some challenges cooperative perception introduces. The TruPercept runtime and evaluation framework allows modular component replacement to facilitate ablation studies as well as the creation of new trust scenarios we are able to show.;TruPercept: Trust Modelling for Autonomous Vehicle Cooperative Perception from Synthetic Data;Not health related;Not health related;0
Y. Morishita;2019;Many studies have used Advanced Land Observing Satellite 2 (ALOS-2) synthetic aperture radar (SAR) interferograms to make remarkable advances toward understanding and assessing seismic hazards. Next-generation satellites will make abundant L-band SAR data available in the near future, enabling even more progress in earthquake research and disaster response. Because a deep understanding of the performance capabilities and limitations of the only existing L-band satellite, ALOS-2, is crucial for planning future L-band SAR missions, this study produced SAR interferograms using ALOS-2 data for large global earthquakes that occurred between August 2014 and December 2016. All of the interferograms produced (49 from 30 seismic events) exhibited adequate coherence even in densely vegetated areas, where C-band interferograms tend to be unreliable because of severe decorrelation. Interferograms for 23 of the 30 seismic events successfully captured significant coseismic deformation signals. These results indicated that ALOS-2 can be leveraged to detect deformations with a high spatial resolution and a high precision unavailable from other instruments, particularly in tropical areas. In addition to the high coherence and high spatial resolution, ALOS-2 features such as left-looking and rapid emergency observations are also advantageous for earthquake research and disaster response. Although current baseline conditions are not always desirable for ALOS-2 interferograms because of limited observation resources, planned L-band missions (e.g., ALOS-4 and NASA-ISRO SAR), which will offer much wider coverage and higher observation frequency, are expected to improve this situation.;A Systematic Study of Synthetic Aperture Radar Interferograms Produced From ALOS-2 Data for Large Global Earthquakes From 2014 to 2016;Not health related;Not health related;0
"T. M. Marston; J. L. Kennedy; P. L. Marston";2011;Circular synthetic aperture sonar (CSAS) traditionally involves the coherent processing of 360 degree scattering information from acoustic targets. To obtain 360 degree scattering information, a source may circle around a central target field and constantly illuminate targets from multiple aspects. Another method of obtaining CSAS data is to fix the source location and spin a target on a rotating mount. Following data reception, a variety of methods in the Fourier or time-domain may be used to construct images. For certain targets, resonances and elastic effects can interfere with the specular portions of backscattered echoes. The time-delay associated with elastic or resonant responses destroys the uniqueness of the location to which the signal is mapped, and occasionally these resonant features can be mapped directly on top of target specular features, causing destructive interference and reduced image clarity. Destructive interference can be reduced and image clarity enhanced by incoherently summing separate images generated from sub-apertures of CSAS data. Additionally, limiting the aperture and frequency band of the pre-processed data before applying an imaging algorithm is an effective method for understanding and localizing various elastic and non-elastic target responses. In a solid 3-to-1 cylinder, for example, effects such as meridional and face-crossing rays cause well defined image features that are prominently visible when limiting the aperture to the angular portions in which these rays are the dominant elastic effects. Further analysis may be obtained by masking portions of these sub-aperture images and reversing the imaging process. This can be used to directly relate target image features to the angular frequency response (colorplot) of the target. An added benefit of this reversal process is that signals from surrounding objects, and the random noise spread throughout the image scene can be rejected by an image masking process, and the resulting time-domain information has an enhanced signal-to-noise ratio. This effect has been successfully demonstrated on data acquired in field-tests, and in controlled laboratory experiments with real and replicated Unexploded Ordnance (UXO) objects. A “Projection-Slice” based CSAS script has been tested on data acquired at sea by an unmanned vehicle, as well as in laboratory experiments from UXO objects placed on a rotational mount. The laboratory based full scale UXO datasets were acquired in a controlled environment for full 360 degree aperture in a free-field configuration. The measurements were conducted at the Naval Surface Warfare Center, Panama City Division (NSWC PCD), facility T-2069 Barge Acoustic Test Facility, which has a 31.5 feet wide, 62 feet long, and 28 ft deep vinyl linear encapsulating 423,000 gallons of isothermal freshwater. The full scale targets examined included an inert 100 mm UXO target, a machined 100 mm aluminum facsimile UXO target, and a cylinder with a notch. The targets were suspended from a rotation stage, with 360 degree rotation capability, in the water column and insonified by a broadband acoustic projector. Preliminary results have also demonstrated the ability to image high-resolution circular synthetic aperture data in which the sonar platform, Remote Environmental Monitoring Units (REMUS) 600 unmanned underwater vehicle, was programmed to circle sunken objects. [Research supported by Office of Naval Research and The Strategic Environmental Research and Development Program (SERDP) under projects MM-1665 and MM-1666.].;Coherent and semi-coherent processing of limited-aperture circular synthetic aperture (CSAS) data;Not health related;Not health related;0
"I. Idehen; W. Jang; T. J. Overbye";2020;In spite of the challenges associated with obtaining actual PMU measurements for research purposes and analytic methods testing, it remains crucial that experimental input data exhibits similar quality features of real measurements for proper grid assessment and planning. The objective of this paper is to generate and validate large sets of synthetic, but realistic, PMU datasets obtained from complex grid models. A study of different variability components in PMU measurements is first presented followed by the proposed steps in generating synthetic datasets. Random variations of resource inputs are used in a simulation platform to generate prior voltage data from a synthetic 2,000-bus system, followed by a data modification process to infuse further realism into the dataset. The validation process used to assess the accuracy of the generated voltage dataset utilizes a variability metric to determine the level of inherent variations in individual measurements, and further applies a dimension reduction technique to identify the extent of electrical dynamics retained in the overall synthetic dataset.;Large-Scale Generation and Validation of Synthetic PMU Data;Not health related;Not health related;0
"B. Liang; L. Zheng";2017;This paper presents a novel approach to action recognition using synthetic multi-view data from depth maps. Specifically, multiple views are first generated by rotating 3D point clouds from depth maps. A pyramid multi-view depth motion template is then adopted for multi-view action representation, characterizing the multi-scale motion and shape patterns in 3D. Empirically, despite the view-specific information, the latent information between multiple views often provides important cues for action recognition. Concentrating on this observation and motivated by the success of the dictionary learning framework, this paper proposes to explicitly learn a view-specific dictionary (called specificity) for each view, and simultaneously learn a latent dictionary (called latent correlation) across multiple views. Thus, a novel method, specificity and latent correlation learning, is put forward to learn the specificity that captures the most discriminative features of each view, and learn the latent correlation that contributes the inherent 3D information to multiple views. In this way, a compact and discriminative dictionary is constructed by specificity and latent correlation for feature representation of actions. The proposed method is evaluated on the MSR Action3D, the MSR Gesture3D, the MSR Action Pairs, and the ChaLearn multi-modal data sets, consistently achieving promising results compared with the state-of-the-art methods based on depth data.;Specificity and Latent Correlation Learning for Action Recognition Using Synthetic Multi-View Data From Depth Maps;Not health related;Not health related;0
"M. Huber; F. Boutros; A. T. Luu; K. Raja; R. Ramachandra; N. Damer; P. C. Neto; T. Gonçalves; A. F. Sequeira; J. S. Cardoso; J. Tremoço; M. Lourenço; S. Serra; E. Cermeño; M. Ivanovska; B. Batagelj; A. Kronov_ek; P. Peer; V. _truc";2022;This paper presents a summary of the Competition on Face Morphing Attack Detection Based on Privacy-aware Synthetic Training Data (SYN-MAD) held at the 2022 In-ternational Joint Conference on Biometrics (IJCB 2022). The competition attracted a total of 12 participating teams, both from academia and industry and present in 11 differ-ent countries. In the end, seven valid submissions were submitted by the participating teams and evaluated by the organizers. The competition was held to present and at-tract solutions that deal with detecting face morphing at-tacks while protecting people's privacy for ethical and le-gal reasons. To ensure this, the training data was limited to synthetic data provided by the organizers. The submitted solutions presented innovations that led to out-performing the considered baseline in many experimental settings. The evaluation benchmark is now available at: https://github.com/marcohuber/SYN-MAD-2022.;SYN-MAD 2022: Competition on Face Morphing Attack Detection Based on Privacy-aware Synthetic Training Data;Not health related;Not health related;0
S. Wu;1984;The results of Synthetic Aperture Radar (SAR) measurements over Kershaw County, Sooth Carolina, using HH, HV, and VV polarization and two-incidence angle X-band airborne SAR system and over Baldwin County, Alabama, using HH polarization L-band Shuttle Imaging Radar (SIR-A) are presented. The X-band data indicate higher HH than VV radar return for cypress forest with standing water. Muttipolarization (HH, HV, and VV) data help delineate several land-cover types that are difficult to delineate by the single polarization (HH) data. The L-band data indicate that radar return signal strength is highly correlated with tree height or age for three types of pine forest. It is found that delineation of urban/residential from deciduous forest is significantly improved by the inclusion of Landsat multispectral scanner data.;Analysis of synthetic aperture radar data acquired over a variety of land cover;Not health related;Not health related;0
"F. Shang; N. Kishi; A. Hirose";2019;This paper proposes a novel data filtering algorithm for fully polarimetric synthetic aperture radar (PolSAR) based on the degree of polarization (DoP) information. First, we define the homogeneity degree and polarization independence degree using the DoP information, and propose a feature plane to characterize the target feature. Second, employing the feature plane, we categorize the targets into three types and assign specific filtering policy for each type to estimate the optimal filtering window sizes. Finally, the $T$ -matrices of fully PolSAR data are filtered using the windows with estimated optimal sizes. Compared with boxcar filter, refined Lee filter, scattering model-based filter, and improved sigma filter in processing ALOS2-PALSAR2 data, the proposed DoP-based algorithm presents the best filtering performance.;Degree of Polarization-Based Data Filter for Fully Polarimetric Synthetic Aperture Radar;Not health related;Not health related;0
"C. Wu; B. Barkan; W. J. Karplus; D. Caswell";1982;This paper presents a digital signal processing system that produces the SEASAT synthetic-aperture radar (SAR) imagery. The system consists of a SEL 32/77 host minicomputer and three AP-120B array processors. The partitioning of the SAR processing functions and the design of softwae modules is described. The rationale for selecting the parallel array processor architecture and the methodology for developing the parallel processing scheme on this system is described. This system attains a SEASAT SAR data reduction speed of 2.5 h per 25-m resolution 4-look and 100 km X 100 km image frame. A prelininary performance evaluation of this parallel processing system and potential future applications for remote sensing data reduction are described.;Seasat Synthetic-Aperture Radar Data Reduction Using Parallel Programmable Array Processors;Not health related;Not health related;0
"H. Liu; X. -M. Li; H. Guo";2016;"In early January 2014, the Chinese icebreaker XueLong was stopped by thick ice on its passage to rescue the trapped Russian vessel Akademik Shokalskiy along the east coast of Antarctica, between Commonwealth Bay and the Mertz Glacier. During the event, X-band spaceborne synthetic aperture radar (SAR) data were continuously acquired from the TerraSAR-X (TS-X) satellite to monitor the state of the sea ice to assist both vessels. The emphasis of this case study is not to understand how the vessels escaped danger but rather to investigate the sea ice breaks in the region, i.e., the dynamic processes of the sea ice during the event through an analysis of the TS-X data, sea ice classification, and the reanalysis of surface wind and ocean current modeling results. Therefore, we present six images of TS-X ScanSAR and newly operational wide ScanSAR imaging modes; these images provide abundant information about the state of the sea ice. Both the ScanSAR and wide ScanSAR images in the HH-polarization are used for classifying sea ice in the region; the resulting classification illustrates how the sea ice varied and finally broke during the event. A further analysis of the sea ice drift derived from the sequential TS-X images and surface wind and ocean current model data partially explains why the sea ice broke under such weather conditions. This case study contributes to a better understanding of the sea ice dynamics on both regional and local scales.";The Dynamic Processes of Sea Ice on the East Coast of Antarctica—A Case Study Based on Spaceborne Synthetic Aperture Radar Data from TerraSAR-X;Not health related;Not health related;0
"M. Diaz-Cabrera; M. Gomez-Barrero; A. Morales; M. A. Ferrer; J. Galbally";2014;One of the main challenges of off-line signature verification is the absence of large databases. A possible alternative to overcome this problem is the generation of fully synthetic signature databases, not subject to legal or privacy concerns. In this paper we propose several approaches to the synthesis of off-line enhanced signatures from real dynamic information. These synthetic samples show a performance very similar to the one offered by real signatures, even increasing their discriminative power under the skilled forgeries scenario, one of the biggest challenges of handwriting recognition. Furthermore, the feasibility of synthetically increasing the enrolment sets is analysed, showing promising results.;Generation of Enhanced Synthetic Off-Line Signatures Based on Real On-Line Data;Not health related;Not health related;0
"S. Juraev; A. Ghimire; J. Alikhanov; V. Kakani; H. Kim";2022;The world’s elderly population continues to grow at an unprecedented rate, creating a need to monitor the safety of an aging population. One of the current problems is accurately classifying elderly physical activities, especially falling down, and delivering prompt assistance to someone in need. Owing to the advancements in deep learning research, vision based solutions are employed for action recognition. One such popular approach is human pose estimation based action recognition or fall detection. Nevertheless, due to a lack of large-scale elderly fall datasets and the continuation of numerous challenges such as varying camera angles, illumination, and occlusion accurately classifying falls has been a problematic. To address these problems, this research first carried out a comprehensive study of the AI Hub dataset collected from real lives of elderly people in order to benchmark the performance of state-of-the-art human pose estimation methods. Secondly, owing to the limited number of real datasets, augmentation with synthetic data was applied and performance improvement was validated based on changes in the degree of accuracy. Third, this study shows that a Transformer network applied to elderly action recognition outperforms LSTM-based networks by a noticeable margin. Lastly, by observing the quantitative and qualitative performances of different networks, this paper proposes an efficient solution for elderly activity recognition and fall detection in the context of surveillance cameras.;Exploring Human Pose Estimation and the Usage of Synthetic Data for Elderly Fall Detection in Real-World Surveillance;health related;Not health related;0
"F. Boutros; M. Klemt; M. Fang; A. Kuijper; N. Damer";2023;Over the past years, the main research innovations in face recognition focused on training deep neural networks on large-scale identity-labeled datasets using variations of multi-class classification losses. However, many of these datasets are retreated by their creators due to increased privacy and ethical concerns. Very recently, privacy-friendly synthetic data has been proposed as an alternative to privacy-sensitive authentic data to comply with privacy regulations and to ensure the continuity of face recognition research. In this paper, we propose an unsupervised face recognition model based on unlabeled synthetic data (USynthFace). Our proposed USynthFace learns to maximize the similarity between two augmented images of the same synthetic instance. We enable this by a large set of geometric and color transformations in addition to GAN-based augmentation that contributes to the USynthFace model training. We also conduct numerous empirical studies on different components of our USynthFace. With the proposed set of augmentation operations, we proved the effectiveness of our USynthFace in achieving relatively high recognition accuracies using unlabeled synthetic data. The training code and pretrained model are publicly available under https://github.com/fdbtrs/Unsupervised-Face-Recognition-using-Unlabeled-Synthetic-Data.;Unsupervised Face Recognition using Unlabeled Synthetic Data;Not health related;Not health related;0
"M. Abufadda; K. Mansour";2021;Data is the fuel of machine learning algorithms, therefore data generation in machine learning is becoming an important topic. The problem is that finding enough data for machine learning algorithms in some domains or situations is difficult. For example, some data may invade the privacy of people or some other datasets can be related to national security and difficult to be unveiled. This paper reviews the related work in synthetic data generation in terms of available methods for data generation (augmentation) and how the generated data helps in improving the performance of machine learning algorithms. The main focus of this paper is data synthetic methods in the healthcare domain.;A Survey of Synthetic Data Generation for Machine Learning;Not health related;Not health related;0
"S. Lang; X. Liu; B. Zhao; X. Chen; G. Fang";2015;In this paper, we propose a new algorithm to address the speckle noise problem in imaging of ice sheets. It is a wave-equation-based ice-sounding imaging method using curvelets as building blocks of ice-sounding data, which successfully images the topography of ice sheets. First, theory analysis has been carried on to the proposed algorithm. Then, we give the specific steps to implement this algorithm. Finally, we apply this algorithm to the simulation point targets and High-Resolution Ice-Sounding Radar data to prove its validity of imaging of ice sheets. Furthermore, compared with five previous methods in two major aspects—the power of clutter reduction and the equivalent number of looks—the proposed algorithm reduces the speckle noise during the imaging processing without degrading the ability in clutter reduction.;Focused Synthetic Aperture Radar Processing of Ice-Sounding Data Collected Over the East Antarctic Ice Sheet via the Modified Range Migration Algorithm Using Curvelets;Not health related;Not health related;0
"P. de Heering; K. U. Simmer; E. Ochieng-Ogolla; A. Wasiljeff";1994;Conventional processing of synthetic aperture sonar (SAS) data is equivalent to a two-dimensional matched filter operation. In principle, two-dimensional deconvolution improves the resolution of the processed image. However, its direct implementation is generally impractical, due to numerical problems. The paper discusses the development of iterative algorithms that efficiently perform the deconvolution of broadband synthetic aperture data and gives examples of their application. It is concluded that, in many cases, the proposed approach is preferable to more classical solutions.<>;A deconvolution algorithm for broadband synthetic aperture data processing;Not health related;Not health related;0
"C. M. de Melo; B. Rothrock; P. Gurram; O. Ulutan; B. S. Manjunath";2020;"Building successful collaboration between humans and robots requires efficient, effective, and natural communication. Here we study a RGB-based deep learning approach for controlling robots through gestures (e.g., ""follow me""). To address the challenge of collecting high-quality annotated data from human subjects, synthetic data is considered for this domain. We contribute a dataset of gestures that includes real videos with human subjects and synthetic videos from our custom simulator. A solution is presented for gesture recognition based on the state-of-the-art I3D model. Comprehensive testing was conducted to optimize the parameters for this model. Finally, to gather insight on the value of synthetic data, several experiments are described that systematically study the properties of synthetic data (e.g., gesture variations, character variety, generalization to new gestures). We discuss practical implications for the design of effective human-robot collaboration and the usefulness of synthetic data for deep learning.";Vision-Based Gesture Recognition in Human-Robot Teams Using Synthetic Data;Not health related;Not health related;0
"G. Jia; W. Chang; Q. Zhang; X. Luan";2016;Regarding the two-dimensional (2-D) imaging processing of circular synthetic aperture radar (CSAR), the motion compensation (MOCO) in the frequency domain is not fully developed. To perform the MOCO for CSAR data in the frequency domain, a novel three-step strategy is presented in this paper. The motion error model is established first, and it is found that a difficulty of MOCO for CSAR lies in the space variance of the motion errors. Specifically, according to the proposed MOCO strategy, this first step is space-invariant MOCO, the second step is space-variant MOCO in the range-compressed angular-wavenumber domain with respect to subaperture data, and the last step is the autofocus operation in the 2-D wavenumber domain after the polar format resampling. Eventually, the complete MOCO flowchart is obtained. Simulation and experimental tests verify that the proposed motion error model is valid and the three-step MOCO strategy can remove motion errors accurately and figure out well-focused CSAR images.;The Analysis and Realization of Motion Compensation for Circular Synthetic Aperture Radar Data;Not health related;Not health related;0
"Z. Tang; T. Miyazaki; Y. Sugaya; S. Omachi";2021;"Scene text erasing, which replaces text regions with reasonable content in natural images, has drawn significant attention in the computer vision community in recent years. There are two potential subtasks in scene text erasing: text detection and image inpainting. Both subtasks require considerable data to achieve better performance; however, the lack of a large-scale real-world scene-text removal dataset does not allow existing methods to realize their potential. To compensate for the lack of pairwise real-world data, we made considerable use of synthetic text after additional enhancement and subsequently trained our model only on the dataset generated by the improved synthetic text engine. Our proposed network contains a stroke mask prediction module and background inpainting module that can extract the text stroke as a relatively small hole from the cropped text image to maintain more background content for better inpainting results. This model can partially erase text instances in a scene image with a bounding box or work with an existing scene-text detector for automatic scene text erasing. The experimental results from the qualitative and quantitative evaluation on the SCUT-Syn, ICDAR2013, and SCUT-EnsText datasets demonstrate that our method significantly outperforms existing state-of-the-art methods even when they are trained on real-world data.";Stroke-Based Scene Text Erasing Using Synthetic Data for Training;Not health related;Not health related;0
"M. Razghandi; H. Zhou; M. Erol-Kantarci; D. Turgut";2022;Data is the fuel of data science and machine learning techniques for smart grid applications, similar to many other fields. However, the availability of data can be an issue due to privacy concerns, data size, data quality, and so on. To this end, in this paper, we propose a Variational AutoEncoder Generative Adversarial Network (VAE-GAN) as a smart grid data generative model which is capable of learning various types of data distributions and generating plausible samples from the same distribution without performing any prior analysis on the data before the training phase. We compared the Kullback–Leibler (KL) divergence, maximum mean discrepancy (MMD), and Wasserstein distance between the synthetic data (electrical load and PV production) distribution generated by the proposed model, vanilla GAN network, and the real data distribution, to evaluate the performance of our model. Furthermore, we used five key statistical parameters to describe the smart grid data distribution and compared them between synthetic data generated by both models and real data. Experiments indicate that the proposed synthetic data generative model outperforms the vanilla GAN network. The distribution of VAE-GAN synthetic data is the most comparable to that of real data.;Variational Autoencoder Generative Adversarial Network for Synthetic Data Generation in Smart Home;Not health related;Not health related;0
"B. -F. Wu; L. -W. Chiu; Y. -C. Wu; C. -C. Lai; P. -H. Chu";2022;Deriving blood pressure in a non-invasive way via photoplethysmography (PPG) signals has become a familiar topic. With the knowledge of the relation between PPG and blood pressure, we expect to further make the measurement contactless for convenience reasons. An alternative signal source is remote photoplethysmography (rPPG) signals. There are mainly two kinds of approaches for exploiting blood pressure through PPG signals, one is by calculating the pulse transit time of the arterial pulse wave at two consecutive sites and the other is based on waveform feature analysis from a single signal. The calibration procedure is necessary for the former way, which leads to some limitations in general use. On the other hand, the properties of the rPPG waveform are far from PPG signals. Hence, the known waveform features in PPG signals are hard to be leveraged in the case of rPPG signals. Recently, convolutional neural networks are also applied for solving this problem. However, the lack of data is an obstacle to the training procedure and evaluation. In this study, a multichannel rPPG-based blood pressure estimator is proposed. To ease the data scarcity issue, the generative adversarial network is adopted to augment synthetic waveform data. Besides, as we know that some physiological states like age and BMI are dominant factors in blood pressure. InfoGAN is chosen in this work to generate the synthetic data with the blood pressure value fluctuating correspondingly to the controlled age and BMI combination. The proposed model outperforms the state-of-the-art methods on MIMIC III and Cuffless datasets. With the synthetic data generation, the mean absolute error (MAE) is reduced to 6.72 and 5.95 mmHg in MAP and DBP respectively. The standard deviations of the MAEs are also reduced. In the rPPG case, the MAE of SBP is 9.13 and 8.76 mmHg for DBP.;Contactless Blood Pressure Measurement via Remote Photoplethysmography with Synthetic Data Generation Using Generative Adversarial Network;Not health related;Not health related;0
"A. Davari; H. C. Özkan; A. Maier; C. Riess";2019;In hyperspectral remote sensing (HSRS), feature data can potentially become very high dimensional. At the same time, manual labeling of that data is an expensive task. As a consequence of these two factors, one of the core challenges is to perform multi-class classification using only relatively few training data points. In this work, we investigate the classification performance with limited training data. First, we revisit the optimization of the internal parameters of a classifier in the context of limited training data. Second, we report an interesting alternative to parameter optimization: classification performance can also be considerably increased by adding synthetic GMM data to the feature space while using a classifier with unoptimized parameters. Third, we show that using variational expectation maximization, we can achieve a much faster convergence in fitting the GMM on the data. In our experiments, we show that the addition of synthetic samples leads to comparable, and, in some cases, even higher classification performance than that for a properly tuned classifier on limited training data. One advantage of the proposed framework is that the reported performance improvements are achieved by a quite simple model. Another advantage is that this approach is computationally much more efficient than classifier parameter optimization and conventional expectation maximization.;Fast and Efficient Limited Data Hyperspectral Remote Sensing Image Classification via GMM-Based Synthetic Samples;Not health related;Not health related;0
"K. Binici; N. Trung Pham; T. Mitra; K. Leman";2022;With the increasing popularity of deep learning on edge devices, compressing large neural networks to meet the hardware requirements of resource-constrained devices became a significant research direction. Numerous compression methodologies are currently being used to reduce the memory sizes and energy consumption of neural networks. Knowledge distillation (KD) is among such methodologies and it functions by using data samples to transfer the knowledge captured by a large model (teacher) to a smaller one (student). However, due to various reasons, the original training data might not be accessible at the compression stage. Therefore, data-free model compression is an ongoing research problem that has been addressed by various works. In this paper, we point out that catastrophic forgetting is a problem that can potentially be observed in existing data-free distillation methods. Moreover, the sample generation strategies in some of these methods could result in a mismatch between the synthetic and real data distributions. To prevent such problems, we propose a data-free KD framework that maintains a dynamic collection of generated samples over time. Additionally, we add the constraint of matching the real data distribution in sample generation strategies that target maximum information gain. Our experiments demonstrate that we can improve the accuracy of the student models obtained via KD when compared with state-of-the-art approaches on the SVHN, Fashion MNIST and CIFAR100 datasets.;Preventing Catastrophic Forgetting and Distribution Mismatch in Knowledge Distillation via Synthetic Data;Not health related;Not health related;0
"S. Sojoudi; J. Doyle";2014;The brain functional connectivity is usually assessed with the correlation coefficients of certain signals. The partial correlation matrix can reveal direct interactions between brain regions. However, computing this matrix is usually challenging due to the availability of only a limited number of samples. As an alternative, thresholding the sample correlation matrix is a common technique for the identification of the direct interactions. In this work, we investigate the performance of this method in addition to some other well-known techniques, namely graphical lasso and Chow-Liu algorithm. Our analysis is performed on some synthetic data produced by an electrical circuit model with certain structural properties. We show that the simple method of thresholding the correlation matrix and the graphical lasso algorithm would both create false positives and negatives that wrongly imply some network properties such as small-worldness. We also apply these techniques to some resting-state functional MRI (fMRI) data and show that similar observations can be made.;Study of the brain functional network using synthetic data;Not health related;Not health related;0
"Z. Xu; D. Shen; Y. Kou; T. Nie";2024;Data imbalance is a common phenomenon in machine learning. In the imbalanced data classification, minority samples are far less than majority samples, which makes it difficult for minority to be effectively learned by classifiers. A synthetic minority oversampling technique (SMOTE) improves the sensitivity of classifiers to minority by synthesizing minority samples without repetition. However, the process of synthesizing new samples in the SMOTE algorithm may lead to problems such as “noisy samples” and “boundary samples.” Based on the above description, we propose a synthetic minority oversampling technique based on Gaussian mixture model filtering (GMF-SMOTE). GMF-SMOTE uses the expected maximum algorithm based on the Gaussian mixture model to group the imbalanced data. Then, the expected maximum filtering algorithm is used to filter out the “noisy samples” and “boundary samples” in the subclasses after grouping. Finally, to synthesize majority and minority samples, we design two dynamic oversampling ratios. Experimental results show that the GMF-SMOTE performs better than the traditional oversampling algorithms on 20 UCI datasets. The population averages of sensitivity and specificity indexes of random forest (RF) on the UCI datasets synthesized by GMF-SMOTE are 97.49% and 97.02%, respectively. In addition, we also record the G-mean and MCC indexes of the RF, which are 97.32% and 94.80%, respectively, significantly better than the traditional oversampling algorithms. More importantly, the two statistical tests show that GMF-SMOTE is significantly better than the traditional oversampling algorithms.;A Synthetic Minority Oversampling Technique Based on Gaussian Mixture Model Filtering for Imbalanced Data Classification;Not health related;Not health related;0
"A. Kishore; T. E. Choe; J. Kwon; M. Park; P. Hao; A. Mittel";2021;"We propose a strategic approach to generate synthetic data in order to improve machine learning algorithms such as Deep Neural Networks (DNN). Utilization of synthetic data has shown promising results yet there are no specific rules or recipes on how to generate and cook synthetic data. We propose imitation training as a guideline of synthetic data generation to add more underrepresented entities and balance the data distribution for DNN to handle corner cases and resolve long tail problems. The proposed imitation training has a circular process with three main steps: First, the existing system is evaluated and failure cases such as false positive and false negative detections are sorted out; Secondly, synthetic data imitating such failure cases is created with domain randomization; Thirdly, we train a net-work with the existing data and the newly added synthetic data; We repeat these three steps until the evaluation metric converges. We validated the approach by experimenting on object detection in autonomous driving.";Synthetic Data Generation using Imitation Training;Not health related;Not health related;0
"I. Cano; V. Torra";2009;Problems related to data privacy are studied in the areas of privacy preserving data mining (PPDM) and statistical disclosure control (SDC). Their goal is to avoid the disclosure of sensitive or proprietary information to third parties. In this paper a new synthetic data generation method is proposed and the information loss and disclosure risk are measured. The method is based on fuzzy techniques. Informally, a fuzzy c-regression method is applied to the original data set and synthetic data is released with an appropriate information loss and disclosure risk depending on c. As other data protection methods do, our synthetic data generation procedure allows third parties to do some statistical computations with a limited risk of disclosure. The trade-off between data utility and data safety of our proposed method will be assessed.;Generation of synthetic data by means of fuzzy c-Regression;Not health related;Not health related;0
"Z. Li; Y. Zhao; J. Fu";2020;A synthetic dataset is a data object that is generated programmatically, and it may be valuable to creating a single dataset from multiple sources when direct collection is difficult or costly. Although it is a fundamental step for many data science tasks, an efficient and standard framework is absent. In this paper, we study a specific synthetic data generation task called downscaling, a procedure to infer high-resolution, harder-to-collect information (e.g., individual level records) from many low-resolution, easy-to-collect sources, and propose a multi-stage framework called SynC (Synthetic Data Generation via Gaussian Copula). For given low-resolution datasets, the central idea of SynC is to fit Gaussian copula models to each of the low-resolution datasets in order to correctly capture dependencies and marginal distributions, and then sample from the fitted models to obtain the desired high-resolution subsets. Predictive models are then used to merge sampled subsets into one, and finally, sampled datasets are scaled according to low-resolution marginal constraints. We make four key contributions in this work: 1) propose a novel framework for generating individual level data from aggregated data sources by combining state-of-the-art machine learning and statistical techniques, 2) perform simulation studies to validate SynC's performance as a synthetic data generation algorithm, 3) demonstrate its value as a feature engineering tool, as well as an alternative to data collection in situations where gathering is difficult through two real-world datasets, 4) release an easy-to-use framework implementation for reproducibility and scalability at the production level that easily incorporates new data.;SynC: A Copula based Framework for Generating Synthetic Data from Aggregated Sources;Not health related;Not health related;0
"L. W. Mdakane; W. Kleynhans";2022;"Oil spills are often caused by vessels when dumping oily bilge wastewater at sea (also referred to as bilge dumping). In an synthetic aperture radar (SAR) image, oil spills dampen the radar energy return and appear as linearly shaped dark regions. However, naturally occurring phenomena (e.g., natural seepage) known as oil spill look-alikes can also dampen energy return and occur more often compared to a real oil spill. The primary goal of the study is to develop a monitoring system dedicated to automatically detect oil spill events caused by ships (bilge dumping) in African Oceans. To achieve this goal, the knowledge of features that has a high probability of separating oil spills from look-alikes is of great importance. The study aimed to accomplish three things, 1) to improve the lack of oil spill studies in Africa; 2) to determine the critical features that yield the highest discrimination accuracy of oil spills caused by moving vessels from look-alikes; and 3) to use these features to automatically detect and classify oil spill events. The study investigated the most common features used in literature for discriminating oil spills from look-alikes from SAR imagery. Multiple feature selection methods and the gradient boosting decision tree (GBT) classifier were used to select, classify, and determine the significant features for discriminating oil spills caused by moving vessels. The results showed that while some features vary, there are features that consistently have high and low significance across all methods.";Feature Selection and Classification of Oil Spill From Vessels Using Sentinel-1 Wide–Swath Synthetic Aperture Radar Data;Not health related;Not health related;0
"R. Shamsuddin; B. M. Maweu; M. Li; B. Prabhakaran";2018;In this paper, we address the problem of research data availability and access in the healthcare sector, by proposing Virtual Patient Model (VPM) a process that combines the use of an optimization algorithm, statistical analysis and machine learning techniques to generate synthetic time series data and report their effectiveness in predictive models. We validate the proposed model by implementing a genetic algorithm that captures important features of a real-world patient time-series data (original) and in applying constraints in the generative process, outputs a best fit synthetic candidate solution of the original time series data. Experimental results using statistical verification tests on both the synthetic and original datasets showed that the synthetic dataset preserved features from the original dataset. We used machine learning prediction models that integrated the synthetic dataset into classification learners and compare their outcomes with those from learners trained with the original dataset. We found promising results in machine learner's ability to discriminate between the different classes when synthetic data is used for training.;Virtual Patient Model: An Approach for Generating Synthetic Healthcare Time Series Data;health related;health related;1
"X. Tu; M. S. Zhdanov";2020;The synthetic aperture (SA) method has recently found applications in the analysis of the low-frequency marine controlled-source electromagnetic (MCSEM) data. It has been shown that this method can enhance the response from an anomalous target. However, in a SA method, anomalous EM fields and the noise will be equally steered and focused, leading to amplifying the noise and introducing artifacts into the images. In addition, the current realizations of the SA method are very sensitive to the noise in the data and the parameters of the SA. In this article, we address these difficulties by introducing a robust SA (RSA) method. The RSA method consists of three steps, namely, robust smoothing of the background field, robust interpolation of EM fields from the real receiver positions to the virtual receiver positions, and estimating the SA weights with a robust optimization scheme. The synthetic model studies show that this method is stable to noise and has a relatively high spatial resolution. We have also applied this method to the towed streamer data collected in the Barents Sea. The generated pseudo-3-D images accurately reveal the locations of the salt domes and fault structures known from the seismic data.;Robust Synthetic Aperture Imaging of Marine Controlled-Source Electromagnetic Data;Not health related;Not health related;0
"Y. Park; J. Ghosh; M. Shankar";2013;This paper introduces a non-parametric data synthesizing algorithm to generate privacy-safe ``realistic but not real'' synthetic health data. Our goal is to provide a systematic mechanism that guarantees an adequate and controllable level of privacy while substantially improving on the utility of public use data, compared to current practices by CMS, OSHPD and other agencies. The proposed algorithm synthesizes artificial records while preserving the statistical characteristics of the original data to the extent possible. The risk from ``database linking attack'' is quantified by either an l-diversified or an _-differentially perturbed data generation process. Moreover its algorithmic performance is optimized using Locality-Sensitive Hashing and parallel computation techniques to yield a linear-time algorithm that is suitable for Big Data Health applications. We synthesize a public Medicare claim dataset using the proposed algorithm, and demonstrate multiple data mining applications and statistical analyses using the data. The synthetic dataset delivers results that are substantially identical to those obtained from the original dataset, without revealing the actual records.;Perturbed Gibbs Samplers for Generating Large-Scale Privacy-Safe Synthetic Health Data;health related;health related;1
"M. S. Zhdanov; D. Yoon; J. Mattsson";2017;The mainstream approach to the interpretation of towed streamer electromagnetic (EM) data is based on 2.5-D and/or 3-D inversions of the observed data into the resistivity models of the subsurface formations. However, the rigorous 3-D and even 2.5-D inversions require large amounts of computational power and time. The synthetic aperture (SA) method is one of the key techniques in remote sensing using radio frequency signals. During recent years, this method was also applied to low-frequency EM fields used for geophysical exploration. This letter demonstrates that the concept of the SA EM method can be extended for rapid imaging of the large volumes of towed streamer EM data. We introduce a notion of virtual receivers, which complement the actual receivers in the construction of the SA for the towed streamer data. A numerical study demonstrates that this method increases the EM response from potential subsurface targets and opens a possibility for on-board real-time imaging of EM data during a survey. The method is illustrated by the imaging of towed streamer EM data acquired over the Troll oil and gas fields in the North Sea. Remarkably, the imaging of the entire towed streamer EM survey requires just a few seconds of computation time on a desktop PC. This result is significant, because it opens a possibility for real-time imaging of the towed streamer EM survey data.;Rapid Imaging of Towed Streamer EM Data Using the Optimal Synthetic Aperture Method;Not health related;Not health related;0
"D. Libes; D. Lechevalier; S. Jain";2017;To have any chance of application in real world, advanced manufacturing research in data analytics needs to explore and prove itself with real-world manufacturing data. Limited access to real-world data largely contrasts with the need for data of varied types and larger quantity for research. Use of virtual data is a promising approach to make up for the lack of access. This paper explores the issues, identifies challenges, and suggests requirements and desirable features in the generation of virtual data. These issues, requirements, and features can be used by researchers to build virtual data generators and gain experience that will provide data to data scientists while avoiding known or potential problems. This, in turn, will lead to better requirements and features in future virtual data generators.;Issues in synthetic data generation for advanced manufacturing;Not health related;Not health related;0
"D. Websdale; S. Taylor; B. Milner";2022;"We propose a real-time speaker-independent speech-to-facial animation system that predicts lip and jaw movements on a reference face for audio speech taken from any speaker. Our approach is motivated by two key observations; 1) Speakerindependent facial animation can be generated from phoneme labels, but to perform this automatically a speech recogniser is needed which, due to contextual look-ahead, introduces too much time lag. 2) Audio-driven speech animation can be performed in real-time but requires large, multi-speaker audio-visual speech datasets of which there are few. We adopt a novel threestage training procedure that leverages the advantages of each approach. First we train a phoneme-to-visual speech model from a large single-speaker audio-visual dataset. Next, we use this model to generate the synthetic visual component of a large multi-speaker audio dataset of which the video is not available. Finally, we learn an audio-to-visual speech mapping using the synthetic visual features as the target. Furthermore, we increase the realism of the predicted facial animation by introducing two perceptually-based loss functions that aim to improve mouth closures and openings. The proposed method and loss functions are evaluated objectively using mean square error, global variance and a new metric that measures the extent of mouth opening. Subjective tests show that our approach produces facial animation comparable to those produced from phoneme sequences and that improved mouth closures, particularly for bilabial closures, are achieved.";Speaker-Independent Speech Animation Using Perceptual Loss Functions and Synthetic Data;Not health related;Not health related;0
"M. Jegorova; A. I. Karjalainen; J. Vazquez; T. Hospedales";2020;Deployment and operation of autonomous underwater vehicles is expensive and time-consuming. High-quality realistic sonar data simulation could be of benefit to multiple applications, including training of human operators for post-mission analysis, as well as tuning and validation of autonomous target recognition (ATR) systems for underwater vehicles. Producing realistic synthetic sonar imagery is a challenging problem as the model has to account for specific artefacts of real acoustic sensors, vehicle attitude, and a variety of environmental factors. We propose a novel method for generating realistic-looking sonar side-scans of full-length missions, called Markov Conditional pix2pix (MC-pix2pix). Quantitative assessment results confirm that the quality of the produced data is almost indistinguishable from real. Furthermore, we show that bootstrapping ATR systems with MC-pix2pix data can improve the performance. Synthetic data is generated 18 times faster than real acquisition speed, with full user control over the topography of the generated data.;Full-Scale Continuous Synthetic Sonar Data Generation with Markov Conditional Generative Adversarial Networks;Not health related;Not health related;0
"Z. Wang; H. Wang";2021;Imbalanced learning is a common problem in data mining. There is a different distribution of data samples among other classes in the imbalanced datasets. It’s a challenge for standard algorithms designed for balanced class distributions. Although there are various strategies to solve this problem, generating artificial data to achieve a relatively balanced class distribution is universal rather than directly modifying specific classification algorithms. The oversampled data can be combined with any user-specified algorithm without any restrictions. In this paper, we present a novel oversampling method, Global Data Distribution Weighted Synthetic Oversampling Technique (GDDSYN). By applying clustering, optimizing the selection criteria of the minority class samples that are used to generate synthetic samples, avoiding generating more noise samples. GDDSYN assigns weights for the number of synthetic samples to tackle the within-class imbalance and between-class imbalance simultaneously, according to the informative level of the sample and the sparsity of the cluster to which the sample belongs. The use of scores with Silhouette Coefficient and Mutual Information helps the k-means algorithm set a reasonable number of clusters for the minority and majority classes respectively so that the clustering effect can be guaranteed. Next, by using clustering information, synthetic samples’ generation path is improved to avoid class overlap. Additionally, GDDSYN has been evaluated extensively on 10 artificial and 10 real-world data sets. The empirical results show that our method is outperforms or comparable with some other existing methods in terms of assessment metrics when artificial data generated by GDDSYN are used.;Global Data Distribution Weighted Synthetic Oversampling Technique for Imbalanced Learning;Not health related;Not health related;0
"A. D. Nicholson; D. E. Peplow; J. M. Ghawaly; M. J. Willis; D. E. Archer";2020;"This work details the generation of synthetic radiation data using large-scale Monte Carlo transport models to evaluate radiation search detection algorithms. Modular 3-D Monte Carlo models spanning multiple city blocks were constructed, loosely based on downtown Knoxville, TN, containing buildings composed of multiple materials (brick, granite, and concrete), sidewalks, a four-lane road, side streets, parking lots, and grassy fields. Background and simulated source detector response calculations from these models were used to create synthetic list mode data sets for a 2"" _ 4"" _ 16"" NaI(Tl) detector moving through a city street at a constant speed. For the background simulations, major isotopes were computed individually so that background composition and variability could be computed efficiently outside Monte Carlo. The source detector response included six simulated sources placed at 15 source locations. Detector response was developed to be periodic through the city street so that a detector path could begin at one end of the model and wrap around to the other. This framework allowed for the creation of diverse data sets, each with its own unique background and simulated source detector response. Synthetic data allows for high-quality labels, which are useful in developing data-driven radiation detection algorithms. This methodology was used to create synthetic data sets which were released as part of a public data competition to spur the development of new radiation detection algorithms for radiological search applications.";Generation of Synthetic Data for a Radiation Detection Algorithm Competition;Not health related;Not health related;0
"C. Quigley; C. Brekke; T. Eltoft";2022;In this study, we compare the retrieval results for the dielectric properties of verified oil slick, acquired using airborne multifrequency synthetic aperture radar. A polarimetric two-scale model was used to invert the radar imagery by first employing solely the co-polarization channels, and then by employing the co-polarization channels in conjunction with the cross-polarization channels, and thereby employing the full suite of polarization information available. The goal is to show that the inversion results obtained from both methods are consistent. Given that the ocean surface is a highly nondepolarizing surface scatter, the signal return within the cross-polarization channels is usually negligible and of no practical use when trying to invert the returned backscatter into useful quantities such as the dielectric constant. In this article, we employ F-SAR data, which was acquired in X-, S-, and L-bands and has an extremely low noise floor, implying that the cross-polarization ratio can be employed. A signal-to-noise analysis showed that only the L-band acquisitions were suitable for analysis in this article. The retrieval results are comparable for the two methods in the case of low dielectric values.;Comparison Between Dielectric Inversion Results From Synthetic Aperture Radar Co- and Quad-Polarimetric Data via a Polarimetric Two-Scale Model;Not health related;Not health related;0
"P. Pincus; M. Preiss; A. S. Goh; D. Gray";2017;"Two novel aspects of polarimetric calibration for fully polarimetric imaging radar systems are addressed. First, the radar system model is formulated in the context of two generic transmitter designs, either a single amplifier followed by a high-power switch or a low-power switch followed by two amplifiers. In the latter case, it is shown that a particular factorization of the polarimetric distortion matrix leads to a significant simplification of the cross-talk representation, from the standard four parameters to two reciprocal parameters, one for each of the antennas. Various system models from the literature are thus placed in a unified framework. Second, calibration techniques for circularly polarized antennas are derived, using either corner reflectors or clutter. However, where standard linear-basis algorithms estimate the cross-talk by its first-order distortion of reflection-symmetric clutter, no equivalent algorithm has been found for the circular basis; indeed, it is shown that the distortion caused, to first-order, by circular-basis cross-talk does not permit the individual cross-talk parameters to be identified. The calibration techniques are applied to fully polarimetric data acquired by the Ingara L-band radar using left- and right-polarized helical antennas.";Polarimetric calibration of circularly polarized synthetic aperture radar data;Not health related;Not health related;0
"W. Chapman; S. Ranka; S. Sahni; M. Schmalz; U. K. Majumder";2010;This paper presents a design for the parallel processing of synthetic aperture radar data using one or more Field Programmable Gate Arrays (FPGAs). Our design supports real-time computation of a two-dimensional image from a matrix of echo pulses and their corresponding response values. Components of this design include: (a) central processing pipeline to perform back projection calculations, (b) pre-fetch cache to minimize external memory access latency, (c) memory bridge that serves as the primary on-chip storage for pulse data, and (d) a pixel queue to direct image data in and out of the pipeline. Design parameters may be adjusted to achieve optimum performance, and multiple instances of this design may be replicated on-chip to achieve prespecified performance objectives. We provide a complexity analysis as a function of the input and output parameters. Simulation results based on an implementation of this design show that our design achieves 160 GFLOPs per instance on a simulated Altera Stratix III EP3SL150 FPGA, and scales well for output image size ranging from 500 _ 500 pixels to 5,000 _ 5,000 pixels.;Parallel processing techniques for the processing of synthetic aperture radar data on FPGAs;Not health related;Not health related;0
"P. N. V. R. Koutilya; H. Zhou; D. Jacobs";2020;We propose a novel method for combining synthetic and real images when training networks to determine geometric information from a single image. We suggest a method for mapping both image types into a single, shared domain. This is connected to a primary network for end-to-end training. Ideally, this results in images from two domains that present shared information to the primary network. Our experiments demonstrate significant improvements over the state-of-the-art in two important domains, surface normal estimation of human faces and monocular depth estimation for outdoor scenes, both in an unsupervised setting.;SharinGAN: Combining Synthetic and Real Data for Unsupervised Geometry Estimation;Not health related;Not health related;0
"S. Mishra; R. Panda; C. P. Phoo; C. -F. R. Chen; L. Karlinsky; K. Saenko; V. Saligrama; R. S. Feris";2022;Pre-training models on Imagenet or other massive datasets of real images has led to major advances in Computer vision, albeit accompanied with shortcomings related to curation cost, privacy, usage rights, and ethical issues. In this paper, for the first time, we study the transferability of pre-trained models based on synthetic data generated by graphics simulators to downstream tasks from very different domains. In using such synthetic data for pre-training, we find that downstream performance on different tasks are fa-vored by different configurations of simulation parameters (e.g. lighting, object pose, backgrounds, etc.), and that there is no one-size-fits-all solution. It is thus better to tailor syn-thetic pre-training data to a specific downstream task, for best performance. We introduce Task2Sim, a unified model mapping downstream task representations to optimal sim-ulation parameters to generate synthetic pre-training data for them. Task2Sim learns this mapping by training to find the set of best parameters on a set of “seen” tasks. Once trained, it can then be used to predict best simulation pa-rameters for novel “unseen” tasks in one shot, without re-quiring additional training. Given a budget in number of images per class, our extensive experiments with 20 di-verse downstream tasks show Task2Sim's task-adaptive pre-training data results in significantly better downstream per-formance than non-adaptively choosing simulation param-eters on both seen and unseen tasks. It is even competitive with pre-training on real images from Imagenet.;Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data;Not health related;Not health related;0
"F. Faisal; N. Mohammed; C. K. Leung; Y. Wang";2022;Due to the recent development in the deep learning community and the availability of state-of-the-art models, medical practitioners are getting more interested in computer vision and deep learning for diagnosis tasks. Moreover, those medical diagnostic models can also increase the reliability of conventional findings. As radiology images can convey a lot of information for a patient’s diagnosis task, the problem is that such medical data may contain sensitive private information in their content header. De-anonymization (i.e., removal of sensitive header information) does not work well due to the re-identification risk, which may link those images to essential details (e.g., birth date, SSN, institution name, etc.), and such an approach can also reduce utility. In the medical domain, utility is significant because a less accurate diagnosis may lead to the wrong course of treatment and/or loss of life. In this paper, we developed a differentially private approach that can generate high-quality and high dimensional synthetic medical image data with guaranteed differential privacy. It can be used to create sufficient quality data to train a deep model. Moreover, we used W-GAN for bounded gradient guarantee, which eliminates the need for an extensive clipping hyperparameter search. We also added noise selectively to the generator to maintain the privacy-utility trade-off. Due to a noise-free discriminator and such selective noise addition to the generator, high-quality and reliable generated radiology images can be utilized for diagnosis tasks. Moreover, our approach can work in a distributed system where different hospitals can contain their private images in the local server and use a central server to generate synthetic radiology images without storing patient data.;Generating Privacy Preserving Synthetic Medical Data;health related;health related;1
"F. Dittrich; H. Woern; V. Sharma; S. Yayilgan";2014;In this paper we present an approach for low-level body part segmentation based on RGB-D data. The RGB-D sensor is thereby placed at the ceiling and observes a shared workspace for human-robot collaboration in the industrial domain. The pixelwise information about certain body parts of the human worker is used by a cognitive system for the optimization of interaction and collaboration processes. In this context, for rational decision making and planning, the pixelwise predictions must be reliable despite the high variability of the appearance of the human worker. In our approach we treat the problem as a pixelwise classification task, where we train a random decision forest classifier on the information contained in depth frames produced by a synthetic representation of the human body and the ceiling sensor, in a virtual environment. As shown in similar approaches, the samples used for training need to cover a broad spectrum of the geometrical characteristics of the human, and possible transformations of the body in the scene. In order to reduce the number of training samples and the complexity of the classifier training, we therefore apply an elaborated and coupled strategy for randomized training data sampling and feature extraction. This allows us to reduce the training set size and training time, by decreasing the dimensionality of the sampling parameter space. In order to keep the creation of synthetic training samples and real-world ground truth data simple, we use a highly reduced virtual representation of the human body, in combination with KINECT skeleton tracking data from a calibrated multi-sensor setup. The optimized training and simplified sample creation allows us to deploy standard hardware for the realization of the presented approach, while yielding a reliable segmentation in real-time, and high performance scores in the evaluation.;Pixelwise object class segmentation based on synthetic data using an optimized training strategy;Not health related;Not health related;0
"X. Yang; X. Fan; J. Wang; K. Lee";2022;Deep learning-based methods have shown excellent potential on object detection and pose estimation with vast amounts of training data to achieve good performance. Obtaining enough comprehensive manual labeling training data is a time-consuming and error-prone task in industrial scenes, and most current time-saving synthetic data generation methods require detailed high-quality CAD models. Instead, we introduce an image-to-image translation based synthetic data generation approach, which requires only untextured CAD models and a small number of real images. The proposed approach starts with edge maps extracted by mesh model projection. And each edge map is provided with realistic appearance learned from real images through image-to-image translation to achieve patch-level realism, and then superimposed on randomly selected backgrounds with a simple Cut-and-Paste strategy. Experiments on our custom dataset and T-LESS dataset show that our synthetic data is competitive with uncomprehensive real data and gives a considerable improvement when compared to the same degree of model rendering images. In addition, the proposed approach allows for extensibility to accommodate new parts with similar texture and structure. And it shows the potential to train image translation model on only one single object and synthesize data for multiples in one take.;Image Translation Based Synthetic Data Generation for Industrial Object Detection and Pose Estimation;Not health related;Not health related;0
"P. Li; X. Liu; K. J. Palmer; E. Fleishman; D. Gillespie; E. -M. Nosal; Y. Shiu; H. Klinck; D. Cholewiak; T. Helble; M. A. Roch";2020;We present a learning-based method for extracting whistles of toothed whales (Odontoceti) in hydrophone recordings. Our method represents audio signals as time-frequency spectrograms and decomposes each spectrogram into a set of time-frequency patches. A deep neural network learns archetypical patterns (e.g., crossings, frequency modulated sweeps) from the spectrogram patches and predicts time-frequency peaks that are associated with whistles. We also developed a comprehensive method to synthesize training samples from background environments and train the network with minimal human annotation effort. We applied the proposed learn-from-synthesis method to a subset of the public Detection, Classification, Localization, and Density Estimation (DCLDE) 2011 workshop data to extract whistle confidence maps, which we then processed with an existing contour extractor to produce whistle annotations. The F1-score of our best synthesis method was 0.158 greater than our baseline whistle extraction algorithm (~25% improvement) when applied to common dolphin (Delphinus spp.) and bottlenose dolphin (Tursiops truncatus) whistles.;Learning Deep Models from Synthetic Data for Extracting Dolphin Whistle Contours;Not health related;Not health related;0
"L. Lindner; D. Narnhofer; M. Weber; C. Gsaxner; M. Kolodziej; J. Egger";2019;In this work, fully automatic binary segmentation of GBMs (glioblastoma multiforme) in 2D magnetic resonance images is presented using a convolutional neural network trained exclusively on synthetic data. The precise segmentation of brain tumors is one of the most complex and challenging tasks in clinical practice and is usually done manually by radiologists or physicians. However, manual delineations are time-consuming, subjective and in general not reproducible. Hence, more advanced automated segmentation techniques are in great demand. After deep learning methods already successfully demonstrated their practical usefulness in other domains, they are now also attracting increasing interest in the field of medical image processing. Using fully convolutional neural networks for medical image segmentation provides considerable advantages, as it is a reliable, fast and objective technique. In the medical domain, however, only a very limited amount of data is available in the majority of cases, due to privacy issues among other things. Nevertheless, a sufficiently large training data set with ground truth annotations is required to successfully train a deep segmentation network. Therefore, a semi-automatic method for generating synthetic GBM data and the corresponding ground truth was utilized in this work. A U-Net-based segmentation network was then trained solely on this synthetically generated data set. Finally, the segmentation performance of the model was evaluated using real magnetic resonance images of GBMs.;Using Synthetic Training Data for Deep Learning-Based GBM Segmentation;health related;Not health related;1
"M. T. Silva; R. Shahidi; E. W. Gill; W. Huang";2020;This paper proposes the use of a nonlinear inversion technique for the extraction of the directional ocean wave spectrum from bistatic high-frequency surface wave radar (HFSWR) Doppler data. The extraction method is combined with nonlinear regression expressions, solely based on the Doppler data, to retrieve wind speed and direction. Once the initialization parameters have been defined, a blind iterative algorithm based on Tikhonov regularization in Hilbert Scales is used to extract the nondirectional spectrum. The extracted spectrum is then used to determine the directional factor, which is assumed to be described by a cosine-power model. The proposed method yields fairly good results with synthetic noise-contaminated HFSWR data with a priori regularization parameters.;Nonlinear Extraction of Directional Ocean Wave Spectrum From Synthetic Bistatic High-Frequency Surface Wave Radar Data;Not health related;Not health related;0
"M. Fang; M. Huber; N. Damer";2023;Recently, significant progress has been made in face presentation attack detection (PAD), which aims to secure face recognition systems against presentation attacks, owing to the availability of several face PAD datasets. However, all available datasets are based on privacy and legally-sensitive authentic biometric data with a limited number of subjects. To target these legal and technical challenges, this work presents the first synthetic-based face PAD dataset, named SynthASpoof, as a large-scale PAD development dataset. The bona fide samples in SynthASpoof are synthetically generated and the attack samples are collected by presenting such synthetic data to capture systems in a real attack scenario. The experimental results demonstrate the feasibility of using SynthASpoof for the development of face PAD. Moreover, we boost the performance of such a solution by incorporating the domain generalization tool MixStyle into the PAD solutions. Additionally, we showed the viability of using synthetic data as a supplement to enrich the diversity of limited authentic training data and consistently enhance PAD performances. The SynthASpoof dataset, containing 25,000 bona fide and 78,800 attack samples, the implementation, and the pre-trained weights are made publicly available1.;SynthASpoof: Developing Face Presentation Attack Detection Based on Privacy-friendly Synthetic Data;Not health related;Not health related;0
"A. Patil; G. Singh; C. Rüdiger; S. Mohanty; S. Kumar; Snehmani";2021;In this article, an algorithm for snow depth (SD) and snow water equivalent (SWE) retrieval is proposed based on a polarimetric synthetic aperture radar (SAR) decomposition model and field measured snow data. The field campaigns were conducted at the Dhundi observatory (in the Indian Himalaya) in January 2016 and 2018. The field-measured data are used here to build a linear regression between wetness ( w) and the imaginary part of the snow permittivity ( _''), and the validation of retrieved SD and SWE. The snow density ( _s) and w are calculated with a generalized volume parameter derived using a theoretical model and SAR data (coherency matrix). These snow parameters and the field-based regression relating w and _'' are eventually used for the SD and SWE retrieval. Three TerraSAR-X scenes of the quad-polarization X-band data acquired in January 2016 are used to study the effect of the snow conditions on the accuracy of the proposed algorithm. The mean absolute error (MAE), root-mean-square error (RMSE), and index of agreement (IOA) for SD are 4.84 cm, 5.12 cm, and 0.71, respectively. On the other hand, for SWE, it is 1.42 cm, 1.53 cm, and 0.71, respectively.;A Novel Approach for the Snow Water Equivalent Retrieval Using X-Band Polarimetric Synthetic Aperture Radar Data;Not health related;Not health related;0
"M. M. Rahman; E. A. Malaia; A. C. Gurbuz; D. J. Griffin; C. Crawford; S. Z. Gurbuz";2022;RF sensors have been recently proposed as a new modality for sign language processing technology. They are noncontact, effective in the dark, and acquire a direct measurement of signing kinematic via exploitation of the micro-Doppler effect. First, this work provides an in depth comparative examination of the kinematic properties of signing as measured by RF sensors for both fluent ASL users and hearing imitation signers. Second, as ASL recognition techniques utilizing deep learning requires a large amount of training data, this work examines the effect of signing kinematics and subject fluency on adversarial learning techniques for data synthesis. The following two different approaches for the synthetic training data generation are proposed: 1) adversarial domain adaptation to minimize the differences between imitation signing and fluent signing data and 2) kinematically-constrained generative adversarial networks for accurate synthesis of RF signing signatures. The results show that the kinematic discrepancies between imitation signing and fluent signing are so significant that training on data directly synthesized from fluent RF signers offers greater performance (93% top-5 accuracy) than that produced by adaptation of imitation signing (88% top-5 accuracy) when classifying 100 ASL signs.;Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition With RF Sensors;Not health related;Not health related;0
P. J. Martin;1981;This paper describes an algorithm which computes two-dimensional Fourier transform of a synthetic aperture radar (SAR) image. This method is intended to replace the whole SAR processing and the two-dimensional Fourier transformation by a specialized processing scheme which, in terms of computational speed and complexity, will be of the order of a single two-dimensional autocorrelation. This processing approach is applied to the estimation of the wave directional spectrum from the raw data of the SAR sensor of Seasat-1.;Direct Determination of the Two-Dimensional Image Spectrum from Raw Synthetic Aperture Radar Data;Not health related;Not health related;0
"J. Hu; G. Choe; Z. Nadir; O. Nabil; S. -J. Lee; H. Sheikh; Y. Yoo; M. Polley";2020;Deep learning-based mobile imaging applications are often limited by the lack of training data. To this end, researchers have resorted to using synthetic training data. However, pure synthetic data does not accurately mimic the distribution of the real data. To improve the utility of synthetic data, we present a systematic pipeline that takes synthetic data coming purely from a game engine and then produces synthetic data with real sensor characteristics such as noise and color gamut. We validate the utility of our sensor-realistic synthetic data for multi-frame high dynamic range (HDR) photography using a Samsung Galaxy S10 Plus smartphone. The result of training two baseline neural networks using our sensor realistic synthetic data modeled for the S10 Plus show that our sensor realistic synthetic data improves the quality of HDR photography on the modeled device. The synthetic dataset is publicly available at https://github.com/nadir-zeeshan/sensor-realistic-synthetic-data.;Sensor-realistic Synthetic Data Engine for Multi-frame High Dynamic Range Photography;Not health related;Not health related;1
"R. Alkurd; I. AbuAlhaol; H. Yanikomeroglu";2019;It is envisioned that wireless networks of the future will support personalized, fine-grained services and decisions by predicting user satisfaction in real-time using machine learning and big data analytics. Data-driven personalization will empower wireless networks to further optimize resources while maintaining user expectations of networks. In order to design, test, and validate research ideas related to wireless network personalization, acquiring data is essential. However, datasets that comprise user behavior and corresponding user satisfaction information are generally not published due to privacy and confidentiality concerns. To account for this, in this paper, we propose a synthetic dataset design methodology to generate labeled user behavior data with ground truth satisfaction values which mimic the real characteristics of real datasets. Finally, we conduct sample user satisfaction prediction experiments using several machine learning algorithms.;A Synthetic User Behavior Dataset Design for Data-Driven AI-Based Personalized Wireless Networks;Not health related;Not health related;0
"A. Magaña; H. Wu; P. Bauer; G. Reinhart";2020;The latest developments and research of convolutional neuronal networks (CNNs) have proven the feasibility of their use in industrial applications that require object detection and pose estimation in unknown environments. Nevertheless, the end-users have neither the required resources for model-training nor the expertise to efficiently implement such applications. On the one hand, our work proposes a pipeline that focuses on the automated generation of training data by using synthetic images. On the other hand, we introduce a deep neural network to estimate the orientation of a reference object by using a one-shot image. We demonstrate the use of PoseNetwork by detecting and estimating the 5D-Pose of a workpiece in a robot-based inspection cell.;PoseNetwork: Pipeline for the Automated Generation of Synthetic Training Data and CNN for Object Detection, Segmentation, and Orientation Estimation;Not health related;Not health related;0
"K. Vyas; L. Jiang; S. Liu; S. Ostadabbas";2021;3D modeling of articulated bodies of humans or animals and using these models for synthetic 2D and 3D pose data generation can mitigate the small data challenges faced by many critical applications such as healthcare. In this paper, we present our efficient 3D synthetic model generation (3D-SMG) pipeline used for body pose data augmentation. 3D-SMG pipeline starts with scanning point clouds from various angles around the subject using an off-the-shelf RGBD camera. We then implement a dual objective iterative closest point (ICP) algorithm that uses both color (if available) as well as geometric information from point cloud and apply a pose graph node optimization to form one single rigid body mesh. 3D-SMG also includes a series of post processing steps to obtain a smooth mesh at the end of the pipeline. The approach allows it to be applied to any articulated object such as a human body or an animal. Our experiments also show high level of accuracy in dimensions of obtained 3D meshes, when compared to the original subject. As the final step towards developing augmented pose dataset, we perform model rigging to articulate the 3D model of the subject and generate dynamic avatars within variety of context-feasible poses1.;An Efficient 3D Synthetic Model Generation Pipeline for Human Pose Data Augmentation;Not health related;Not health related;0
"M. Pätzold; M. Kahl; T. Klinkert; A. Keil; T. Löffler; P. H. Bolívar; A. Kolb";2013;We present a simulation and data-processing framework that incorporates full 3-D configuration, simulation, and reconstruction functionalities for hybrid synthetic aperture THz systems, which combine mechanically scanned real imaging projection (1-D) and synthetic imaging reconstruction (2-D). The simulation is based on a ray-casting approach including models for reflections at rough surfaces and angular antenna sensitivity. A near real-time GPU reconstruction is introduced to process large amounts of data in parallel. Techniques for fast fusion of reconstructed data from several acquisition directions are presented. They serve as basis to visualize a fully consistent 3-D object. The modular framework allows a fast, easy, and efficient evaluation and prediction of system properties including scattering behavior for these types of scanner setups.;Simulation and Data-Processing Framework for Hybrid Synthetic Aperture THz Systems Including THz-Scattering;Not health related;Not health related;0
"A. J. Rodriguez-Almeida; H. Fabelo; S. Ortega; A. Deniz; F. J. Balea-Fernandez; E. Quevedo; C. Soguero-Ruiz; A. M. Wägner; G. M. Callico";2023;The increasing prevalence of chronic non-communicable diseases makes it a priority to develop tools for enhancing their management. On this matter, Artificial Intelligence algorithms have proven to be successful in early diagnosis, prediction and analysis in the medical field. Nonetheless, two main issues arise when dealing with medical data: lack of high-fidelity datasets and maintenance of patient's privacy. To face these problems, different techniques of synthetic data generation have emerged as a possible solution. In this work, a framework based on synthetic data generation algorithms was developed. Eight medical datasets containing tabular data were used to test this framework. Three different statistical metrics were used to analyze the preservation of synthetic data integrity and six different synthetic data generation sizes were tested. Besides, the generated synthetic datasets were used to train four different supervised Machine Learning classifiers alone, and also combined with the real data. F1-score was used to evaluate classification performance. The main goal of this work is to assess the feasibility of the use of synthetic data generation in medical data in two ways: preservation of data integrity and maintenance of classification performance.;Synthetic Patient Data Generation and Evaluation in Disease Prediction Using Small and Imbalanced Datasets;health related;health related;1
"Y. Li; G. Liu; T. Li; L. Jiao; G. Lu; N. Marturi";2020;Data-driven optimization is an efficient global optimization algorithm for expensive black-box functions. In this paper, we apply data-driven optimization algorithm to the task of change detection with synthetic aperture radar (SAR) images for the first time. We first propose an easy-to-implement threshold algorithm for change detection in SAR images based on data-driven optimization. Its performance has been compared with commonly used methods like generalized Kittler and Illingworth threshold algorithms (GKIT). Next, we demonstrate how to tune the hyper-parameter of a (previously available) deep belief network (DBN) for change detection using data-driven optimization. Extensive evaluations are carried out using publicly available benchmark datasets. The obtained results suggest comparatively strong performance of our optimized DBN-based change detection algorithm.;Application of Data Driven Optimization for Change Detection in Synthetic Aperture Radar Images;Not health related;Not health related;0
"S. Basak; P. Corcoran; F. Khan; R. Mcdonnell; M. Schukat";2021;Accurate head pose estimation from 2D image data is an essential component of applications such as driver monitoring systems, virtual reality technology, and human-computer interaction. It enables a better determination of user engagement and attentiveness. The most accurate head pose estimators are based on Deep Neural Networks that are trained with the supervised approach and rely primarily on the accuracy of training data. The acquisition of real head pose data with a wide variation of yaw, pitch and roll is a challenging task. Publicly available head pose datasets have limitations with respect to size, resolution, annotation accuracy and diversity. In this work, a methodology is proposed to generate pixel-perfect synthetic 2D headshot images rendered from high-quality 3D synthetic facial models with accurate head pose annotations. A diverse range of variations in age, race, and gender are also provided. The resulting dataset includes more than 300k pairs of RGB images with corresponding head pose annotations. A wide range of variations in pose, illumination and background are included. The dataset is evaluated by training a state-of-the-art head pose estimation model and testing against the popular evaluation-dataset Biwi. The results show that training with purely synthetic data generated using the proposed methodology achieves close to state-of-the-art results on head pose estimation which are originally trained on real human facial datasets. As there is a domain gap between the synthetic images and real-world images in the feature space, initial experimental results fall short of the current state-of-the-art. To reduce the domain gap, a semi-supervised visual domain adaptation approach is proposed, which simultaneously trains with the labelled synthetic data and the unlabeled real data. When domain adaptation is applied, a significant improvement in model performance is achieved. Additionally, by applying a data fusion-based transfer learning approach, better results are achieved than previously published work on this topic.;Learning 3D Head Pose From Synthetic Data: A Semi-Supervised Approach;Not health related;Not health related;0
"M. F. Khan; R. K. Gazara; M. M. Nofal; S. Chakrabarty; E. M. A. Dannoun; R. Al-Hmouz; M. Mursaleen";2021;Congestive heart failure is among leading genesis of concern that requires an immediate medical attention. Among various cardiac disorders, left ventricular systolic dysfunction is one of the well known cardiovascular disease which causes sudden congestive heart failure. The irregular functioning of a heart can be diagnosed through some of the clinical attributes, such as ejection fraction, serum creatinine etcetera. However, due to availability of a limited data related to the death events of patients suffering from left ventricular systolic dysfunction, a critical level of thresholds of clinical attributes cannot be estimated with higher precision. Hence, this paper proposes a novel pseudo reinforcement learning algorithm which overcomes a problem of majority class skewness in a limited dataset by appending a synthetic dataset across minority data space. The proposed pseudo agent in the algorithm continuously senses the state of the dataset (pseudo environment) and takes an appropriate action to populate the dataset resulting into higher reward. In addition, the paper also investigates the role of statistically significant clinical attributes such as age, ejection fraction, serum creatinine etc., which tends to efficiently predict the association of death events of the patients suffering from left ventricular systolic dysfunction.;Reinforcing Synthetic Data for Meticulous Survival Prediction of Patients Suffering From Left Ventricular Systolic Dysfunction;health related;Not health related;1
"B. Thai; R. Jimerson; D. Arcoraci; E. Prud'hommeaux; R. Ptucha";2019;Although the application of deep learning to automatic speech recognition (ASR) has resulted in dramatic reductions in word error rate for languages with abundant training data, ASR for languages with few resources has yet to benefit from deep learning to the same extent. In this paper, we investigate various methods of acoustic modeling and data augmentation with the goal of improving the accuracy of a deep learning ASR framework for a low-resource language with a high baseline word error rate. We compare several methods of generating synthetic acoustic training data via voice transformation and signal distortion, and we explore several strategies for integrating this data into the acoustic training pipeline. We evaluate our methods on an indigenous language of North America with minimal training resources. We show that training initially via transfer learning from an existing high-resource language acoustic model, refining weights using a heavily concentrated synthetic dataset, and finally fine-tuning to the target language using limited synthetic data reduces WER by 15% over just transfer learning using deep recurrent methods. Further, we show improvements over traditional frameworks by 19% using a similar multistage training with deep convolutional approaches.;Synthetic Data Augmentation for Improving Low-Resource ASR;Not health related;Not health related;0
"P. Martinez-Gonzalez; S. Oprea; J. A. Castro-Vargas; A. Garcia-Garcia; S. Orts-Escolano; J. Garcia-Rodriguez; M. Vincze";2021;Synthetic data generation has become essential in last years for feeding data-driven algorithms, which surpassed traditional techniques performance in almost every computer vision problem. Gathering and labelling the amount of data needed for these data-hungry models in the real world may become unfeasible and error-prone, while synthetic data give us the possibility of generating huge amounts of data with pixel-perfect annotations. However, most synthetic datasets lack from enough realism in their rendered images. In that context UnrealROX generation tool was presented in 2019, allowing to generate highly realistic data, at high resolutions and framerates, with an efficient pipeline based on Unreal Engine, a cutting-edge videogame engine. UnrealROX enabled robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation. Nevertheless, its workflow was very tied to generate image sequences from a robotic on-board camera, making hard to generate data for other purposes. In this work, we present UnrealROX+, an improved version of UnrealROX where its decoupled and easy-to-use data acquisition system allows to quickly design and generate data in a much more flexible and customizable way. Moreover, it is packaged as an Unreal plug-in, which makes it more comfortable to use with already existing Unreal projects, and it also includes new features such as generating albedo or a Python API for interacting with the virtual environment from Deep Learning frameworks.;UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments;Not health related;Not health related;0
"S. Chauhan; H. S. Srivastava; P. Patel";2019;The objective of this paper was to explore the potential of hybrid-polarized (RH and RV) RISAT-1 SAR data to retrieve the height of wheat crop-an important winter crop in South Asian countries including India. The images acquired over north-west India in 2015 covered critical growth stages of wheat. The field campaigns were carried out in synchronous with the SAR passes. Considering the dominant role of underlying soil cover in the total backscatter (_total0) response from a target, we propose that refining the _total0 by reducing the effect of underlying soil can significantly improve the retrieval accuracy of crop height (CH). To achieve this, we modified the existing water cloud model (WCM) to estimate soil-corrected vegetation backscatter (_veg0). Leaf area index and interaction factor showed great potential as the vegetation descriptors in modeling _total0 using WCM. A comparative analysis between the CH retrieved from _total0 and _veg0 using multilayer perceptron neural networks revealed the response of C-band backscatter to CH. CH was moderately correlated to _total0, but the results improved considerably with the substitution of _total0 with _veg0. This holds true particularly in the early growth stages of crop growth when the vegetation cover is scarce and there is a substantial effect of soil background on the remote sensing signal. Thus, the results suggest suitability of C-band hybrid-polarized data for the assessment of CH.;Crop Height Estimation Using RISAT-1 Hybrid-Polarized Synthetic Aperture Radar Data;Not health related;Not health related;0
"V. Varkarakis; S. Bazrafkan; G. Costache; P. Corcoran";2020;This work explores the identity attribute of synthetic face samples derived from Generative Adversarial Networks. The goal is to determine if individual samples are unique in terms of identity, firstly with respect to the seed dataset that trains the GAN model and secondly with respect to other synthetic face samples. Two approaches are introduced to enable the comparative analysis of large sets of synthetic face samples. The first of these uses ROC curves to determine identity uniqueness using a number of large publicly available datasets of real facial samples to provide reference ROCs as a baseline. The second approach uses a thresholding technique utilizing again large publicly available datasets as a reference. For this approach, new metrics are introduced, and a technique is provided to remove the most connected data samples within a large synthetic dataset. The remaining synthetic samples can be considered as unique as data samples gathered from different real individuals. Several StyleGAN models are used to create the synthetic datasets, and variations in key model parameters are explored. It is concluded that the resulting synthetic data samples exhibit excellent uniqueness when compared with the original training dataset, but significantly less uniqueness when comparisons are made within the synthetic dataset. Nevertheless, it is possible to remove the most highly connected synthetic data samples. Thus, in some cases, up to 92% of the data samples in a 20k synthetic dataset can be shown to exhibit similar uniqueness to data samples taken from real public datasets.;Validating Seed Data Samples for Synthetic Identities – Methodology and Uniqueness Metrics;Not health related;Not health related;0
"S. Yanushkevich; A. Stoica; P. Shmerko; W. Howells; K. Crockett; R. Guest";2020;Synthetic, or artificial data is used in security applications such as protection of sensitive information, prediction of rare events, and training neural networks. Risk and trust are assessed specifically for a given kind of synthetic data and particular application. In this paper, we consider a more complicated scenario, - biometric-enabled cognitive cognitive biometric-enabled identity management, in which multiple kinds of synthetic data are used in addition to authentic data. For example, authentic biometric traits can be used to train the intelligent tools to identify humans, while synthetic, algorithmically generated data can be used to expand the training set or to model extreme situations. This paper is dedicated to understanding the potential impact of synthetic data on the cognitive checkpoint performance, and risk and trust prediction.;Cognitive Identity Management: Synthetic Data, Risk and Trust;Not health related;Not health related;0
"E. Maneas; A. Hauptmann; E. J. Alles; W. Xia; T. Vercauteren; S. Ourselin; A. L. David; S. Arridge; A. E. Desjardins";2022;Instrumented ultrasonic tracking is used to improve needle localization during ultrasound guidance of minimally invasive percutaneous procedures. Here, it is implemented with transmitted ultrasound pulses from a clinical ultrasound imaging probe, which is detected by a fiber-optic hydrophone integrated into a needle. The detected transmissions are then reconstructed to form the tracking image. Two challenges are considered with the current implementation of ultrasonic tracking. First, tracking transmissions are interleaved with the acquisition of B-mode images, and thus, the effective B-mode frame rate is reduced. Second, it is challenging to achieve an accurate localization of the needle tip when the signal-to-noise ratio is low. To address these challenges, we present a framework based on a convolutional neural network (CNN) to maintain spatial resolution with fewer tracking transmissions and enhance signal quality. A major component of the framework included the generation of realistic synthetic training data. The trained network was applied to unseen synthetic data and experimental <italic>in vivo</italic> tracking data. The performance of needle localization was investigated when reconstruction was performed with fewer (up to eightfold) tracking transmissions. CNN-based processing of conventional reconstructions showed that the axial and lateral spatial resolutions could be improved even with an eightfold reduction in tracking transmissions. The framework presented in this study will significantly improve the performance of ultrasonic tracking, leading to faster image acquisition rates and increased localization accuracy.;Deep Learning for Instrumented Ultrasonic Tracking: From Synthetic Training Data to <italic>In Vivo</italic> Application;Not health related;Not health related;0
"S. Imtiaz; M. Arsalan; V. Vlassov; R. Sadre";2021;With the rapid advancements in machine learning, the health care paradigm is shifting from treatment towards prevention. The smart health care industry relies on the availability of large-scale health datasets in order to benefit from machine learning-based services. As a consequence, preserving the individuals’ privacy becomes vital for sharing sensitive personal information. Synthetic datasets with generative models are considered to be one of the most promising solutions for privacy-preserving data sharing. Among the generative models, generative adversarial networks (GANs) have emerged as the most impressive models for synthetic data generation in recent times. However, smart health care data is attributed with unique challenges such as volume, velocity, and various data types and distributions. We propose a GAN coupled with differential privacy mechanisms for generating a realistic and private smart health care dataset. The proposed approach is not only able to generate realistic synthetic data samples but also the differentially private data samples under different settings: learning from a noisy distribution or noising the learned distribution. We tested and evaluated our proposed approach using a real-world Fitbit dataset. Our results indicate that our proposed approach is able to generate quality synthetic and differentially private dataset that preserves the statistical properties of the original dataset.;Synthetic and Private Smart Health Care Data Generation using GANs;health related;health related;1
"D. Winkelbauer; M. Denninger; R. Triebel";2021;Most existing approaches for visual localization either need a detailed 3D model of the environment or, in the case of learning-based methods, must be retrained for each new scene. This can either be very expensive or simply impossible for large, unknown environments, for example in search-and-rescue scenarios. Although there are learning-based approaches that operate scene-agnostically, the generalization capability of these methods is still outperformed by classical approaches. In this paper, we present an approach that can generalize to new scenes by applying specific changes to the model architecture, including an extended regression part, the use of hierarchical correlation layers, and the exploitation of scale and uncertainty information. Our approach outperforms the 5-point algorithm using SIFT features on equally big images and additionally surpasses all previous learning-based approaches that were trained on different data. It is also superior to most of the approaches that were specifically trained on the respective scenes. We also evaluate our approach in a scenario with only very few reference images, showing that under such more realistic conditions our learning-based approach considerably exceeds both existing learning-based and classical methods.;Learning to Localize in New Environments from Synthetic Training Data;Not health related;Not health related;0
"S. Moonen; B. Vanherle; J. de Hoog; T. Bourgana; A. Bey-Temsamani; N. Michiels";2023;The use of computer vision for product and assembly quality control is becoming ubiquitous in the manufacturing industry. Lately, it is apparent that machine learning based solutions are outperforming classical computer vision algorithms in terms of performance and robustness. However, a main drawback is that they require sufficiently large and labeled training datasets, which are often not available or too tedious and too time consuming to acquire. This is especially true for low-volume and high-variance manufacturing. Fortunately, in this industry, CAD models of the manufactured or assembled products are available. This paper introduces CAD2Render, a GPU-accelerated synthetic data generator based on the Unity High Definition Render Pipeline (HDRP). CAD2Render is designed to add variations in a modular fashion, making it possible for high customizable data generation, tailored to the needs of the industrial use case at hand. Although CAD2Render is specifically designed for manufacturing use cases, it can be used for other domains as well. We validate CAD2Render by demonstrating state of the art performance in two industrial relevant setups. We demonstrate that the data generated by our approach can be used to train object detection and pose estimation models with a high enough accuracy to direct a robot. The code for CAD2Render is available at https://github.com/EDM-Research/CAD2Render.;CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry;Not health related;Not health related;0
"B. Shen; B. Li; W. J. Scheirer";2021;Computer vision has achieved superior results with the rapid development of new techniques in deep neural networks. Object detection in the wild is a core task in computer vision, and already has many successful applications in the real world. However, deep neural networks for object detection usually consist of hundreds, and sometimes even thousands, of layers. Training such networks is challenging, and training data has a fundamental impact on model performance. Because data collection and annotation are expensive and labor-intensive, lots of data augmentation methods have been proposed to generate synthetic data for neural network training. Most of those methods focus on manipulating 2D images. In contrast to that, in this paper, we leverage the realistic visual effects of 3D environments and propose a new way of generating synthetic data for computer vision tasks related to city scenes. Specifically, we describe a pipeline that can generate a 3D city model from an input of a 2D image that portrays the layout design of a city. This pipeline also takes optional parameters to further customize the output 3D city model. Using our pipeline, a virtual 3D city model with high-quality textures can be generated within seconds, and the output is an object ready to render. The model generated will assist people with limited 3D development knowledge to create high quality city scenes for different needs. As examples, we show the use of generated 3D city models as the synthetic data source for a scene text detection task and a traffic sign detection task. Both qualitative and quantitative results show that the generated virtual city is a good match to real-world data and potentially can benefit other computer vision tasks with similar contexts.;Automatic Virtual 3D City Generation for Synthetic Data Collection;Not health related;Not health related;0
"S. A. Dorado-Rojas; M. de Castro Fernandes; L. Vanfretti";2020;This article presents a simulation-based massive data generation procedure with applications in training machine learning (ML) solutions to automatically assess the small-signal stability condition of a power system subjected to contingencies. This method of scenario generation for employs a Monte Carlo two-stage sampling procedure to set up a contingency condition while considering the likelihood of a given combination of line outages. The generated data is pre-processed and then used to train several ML models (logistic and softmax regression, support vector machines, k-nearest Neighbors, Naïve Bayes and decision trees), and a deep learning neural network. The performance of the ML algorithms shows the potential to be deployed in efficient real-time solutions to assist power system operators.;Synthetic Training Data Generation for ML-based Small-Signal Stability Assessment;Not health related;Not health related;0
"C. Tiago; A. Gilbert; A. S. Beela; S. A. Aase; S. R. Snare; J. _prem; K. McLeod";2022;Due to privacy issues and limited amount of publicly available labeled datasets in the domain of medical imaging, we propose an image generation pipeline to synthesize 3D echocardiographic images with corresponding ground truth labels, to alleviate the need for data collection and for laborious and error-prone human labeling of images for subsequent Deep Learning (DL) tasks. The proposed method utilizes detailed anatomical segmentations of the heart as ground truth label sources. This initial dataset is combined with a second dataset made up of real 3D echocardiographic images to train a Generative Adversarial Network (GAN) to synthesize realistic 3D cardiovascular Ultrasound images paired with ground truth labels. To generate the synthetic 3D dataset, the trained GAN uses high resolution anatomical models from Computed Tomography (CT) as input. A qualitative analysis of the synthesized images showed that the main structures of the heart are well delineated and closely follow the labels obtained from the anatomical models. To assess the usability of these synthetic images for DL tasks, segmentation algorithms were trained to delineate the left ventricle, left atrium, and myocardium. A quantitative analysis of the 3D segmentations given by the models trained with the synthetic images indicated the potential use of this GAN approach to generate 3D synthetic data, use the data to train DL models for different clinical tasks, and therefore tackle the problem of scarcity of 3D labeled echocardiography datasets.;A Data Augmentation Pipeline to Generate Synthetic Labeled Datasets of 3D Echocardiography Images Using a GAN;health related;Not health related;1
"Y. E. Sagduyu; A. Grushin; Y. Shi";2018;This paper presents a novel system, synthetic high-fidelity social media data generator (SHIELD), for generating the synthetic social media data. SHIELD jointly generates time-varying, directed and weighted interaction graph structures and topic-driven text features similar to the input social media data. A synthetic interaction graph is generated by a social network model to minimize the distance to real graph and is enhanced by adding various patterns, such as anomalies and information cascades, interaction types, and temporal dynamics. A synthetic text generator based on the  $n$ -gram Markov model is trained under each topic identified by topic modeling. Synthetic text and graph structures are combined through the assignment of synthetic social media entities. Extensive performance evaluation via a graph and text analysis is provided to demonstrate the statistical fidelity of large-scale synthetic data generated by SHIELD. A data evaluation exercise with human participants is executed to identify how difficult it is for a human to distinguish between tweets that were generated by SHIELD and tweets that were posted by real users. Experimental results followed by a statistical significance analysis showed that human participants cannot reliably distinguish between real and synthetic tweets.;Synthetic Social Media Data Generation;Not health related;Not health related;0
"R. Sun; Y. Tian; H. Zhang; R. Yue; B. Lv; J. Chen";2019;One of the key tasks for improving vehicle operating costs estimation is to develop representative driving cycles. A driving cycle is a vehicle speed-time profile. The cycles are a critical input for simulating VOCs in different road scenarios. The traditional methods could not generate driving cycles representing the speed pattern of the sample snippets. A new driving cycle development method, synthetic optimization, was developed and applied to generate synthetic driving cycles by applying the speed-acceleration frequency distribution matrix, speed-acceleration status transition matrix, and simulated annealing optimization algorithm. The data used for driving cycle development come from the new Strategic Highway Research Program (SHRP 2) naturalistic driving study (NDS) data and truck GPS trajectory data from American Transportation Research Institute (ATRI). Driving cycles of full-access-control facilities were developed as an example to show the performance of the proposed method. Compared to the conventional methods, the synthetic optimization approach provides, along with other advantages, driving cycles that better represent the speed patterns observed in the different scenarios.;Data-Driven Synthetic Optimization Method for Driving Cycle Development;Not health related;Not health related;0
"D. M. Tralli; R. G. Blom; E. J. Fielding; A. Donnellan; D. L. Evans";2007;The study of the Earth as a system is being adopted widely by geoscientists. Numerical models and simulations are providing the capability to rapidly test hypotheses and make forecasts of complex geophysical behavior. International efforts are seeking to integrate existing and emerging Earth observation systems into a global network, with enhanced data distribution, models, and decision support tools. Remote sensing is poised to fulfil the increasing need for a synoptic framework. However, the desire to improve the connection between scientific research and societal benefits has not been matched with resources and tools required to bridge the gap between research and applications. Natural hazards research and disaster management are a prime example. Here, we present a conceptual case for how interferometric synthetic aperture radar (InSAR) data could make a definitive contribution to understanding earthquake processes while simultaneously supporting policy- and decision-making. InSAR measurements derived from time series of radar observations from Earth orbit uniquely can provide geographically comprehensive maps of surface deformation. Observing system simulations are suggested to evaluate the potential contributions of a future system. Simulations would adopt an open seismic hazard analysis (SHA) framework, OpenSHA, recognizing the need for more physics-based modeling and computational infrastructure. SHA is employed by the HAZUS-MH earthquake module to estimate losses. InSAR measurements of strain accumulation would provide event magnitude recurrence bounds for probabilistic SHA, while coseismic InSAR measurements would add constraints on fault rupture models for deterministic approaches. Moreover, interferograms would be incorporated graphically as proxy seismic risk maps for planning and mitigation;Conceptual Case for Assimilating Interferometric Synthetic Aperture Radar Data Into the HAZUS-MH Earthquake Module;Not health related;Not health related;0
"Y. Yue; Y. Li; K. Yi; Z. Wu";2018;The goal of this paper is to automatically generate synthetic data to enable data analyzers to cope with the problem of insufficient data. Taking the most typical machine learning tasks, classification and regression, as an example, limited and insufficient samples cause low generalization of machine learning models, which cannot provide reasonable predictions. Data are insufficient either because of sample rarity or because data are impeded to be accessed for privacy concerns or confidential protection. To overcome this, we present a Synthetic Data Approach for Classification and Regression, adopting probability distribution and k-nearest neighbor model to generate synthetic data. We first estimate the probability distribution of each feature and construct a k-nearest neighbor model for all original data samples. Then we generate random samples based on probability distributions, adopt the k-nearest neighbor model to validate these random samples, and output the synthetic samples. We use proposed synthetic approaches to generate synthetic data of five publicly available datasets for classification and regression, respectively, and evaluate the performance of machine learning models to evaluate the resemblance between synthetic data and original data. The experimental results show that the synthetic data can resemble the original data, which indicates it is an effective approach for data analyzer to overcome the problem of insufficient data.;Synthetic Data Approach for Classification and Regression;Not health related;Not health related;0
"K. Dietz; N. Gray; M. Seufert; T. Hossfeld";2022;With increasing digitization and the emergence of the Internet of Things, more and more devices communicate with each other, resulting in a drastic growth of communication networks. Consequently, managing these networks, too, becomes harder and harder. Thus, Software-defined Networking (SDN) is employed, simplifying the management and configuration of networks by introducing a central controlling entity, which makes the network programmable via software and ultimately more flexible. As the SDN controller may impose scalability and elasticity issues, distributed controller architectures are utilized to combat this potential performance bottleneck. However, these distributed architectures introduce the need for constant synchronization to keep a centralized network view, and controller instances need to be placed in appropriate locations. As a result, thoroughly designing SDN-enabled networks with respect to a multitude of performance metrics, e. g., latency and induced traffic, is a challenging task. To assist in this process, we train a performance prediction model based on properties which are available during the network planning phase. We utilize a simulation-based approach for data collection to cover a large parameter space, simulating a variety of networks and controller placements for two opposing SDN architectures. On basis of this dataset, we apply Machine Learning (ML) to solve the performance prediction as a regression problem.;ML-based Performance Prediction of SDN using Simulated Data from Real and Synthetic Networks;Not health related;Not health related;0
"K. Mahapatra; D. J. Sebastian-Cardenas; S. N. Gupta Gourisetti; J. G. O'Brien; J. P. Ogle";2021;Sensors play a critical role in supporting day-to-day grid operations and they are essential to operator's decision-making process. Furthermore, sensors and sensor behaviors need to be emulated with grid simulations to perform modeling studies and to design cutting edge power systems applications. Ensuring the accurate behavior of these applications requires accurate emulation of sensors and pertinent signals. However, most grid simulators and modeling tools assume either zero error scenarios or simplistic noise models that may not always correlate to realworld sensors. To address the above issue, this work presents an initial study on the noise characteristics of phasor measurement units (PMUs), along with models for recreating their unique noise signatures. The proposed methods (both analytical and machine-learning-based) provide a substantial increase in a sensor's model fidelity, a feature that can be leveraged by an end-user application to yield more accurate system representations. The proposed methods were then applied to micro PMU data from the EPFL microgrid campus to extract sensor noise profiles. This data was used to train a deep learning model, which was tested to emulate the noise characteristics present in actual signals. Based on the observed results and the employed data-driven methodology, the proposed methods may be adapted to replicate the behavior of other grid sensors and power new applications capable of detecting sensor degradation and eventual device failures in near real-time.;Novel Data Driven Noise Emulation Framework using Deep Neural Network for Generating Synthetic PMU Measurements;Not health related;Not health related;0
"D. Talwar; S. Guruswamy; N. Ravipati; M. Eirinaki";2020;Autonomous vehicles have the potential to completely upend the way we transport today, however deploying them safely at scale is not an easy task. Any autonomous driving system relies on multiple layers of software to function safely. Among these layers, the Perception layer is the most data intensive and also the most complex layer to get right. Companies need to collect and annotate lots of data to properly train deep learning perception models. Simulation systems have come up as an alternative to the expensive task of data collection and annotation. However, whether simulated data can be used as a proxy for real-world data is an ongoing debate. In this work, we attempt to address the question of whether models trained on simulated data can generalize well to the real-world. We collect datasets based on two different simulators with varying levels of graphics fidelity and use the KITTI dataset as an example of real- world data. We train three separate deep learning based object detection models on each of these datasets, and compare their performance on test sets collected from the same sources. We also add the recently released Waymo Open Dataset as a challenging test set. Performance is evaluated based on the mean average precision (mAP) metric for object detection. We find that training on simulation in general does not translate to generalizability on real-world data and that diversity in the training set is much more important than visual graphics' fidelity.;Evaluating Validity of Synthetic Data in Perception Tasks for Autonomous Vehicles;Not health related;Not health related;0
"E. Theunissen; F. D. Roefs; G. J. M. Koeners; R. M. Rademaker; T. J. Etherington";2004;To make the operation of aircraft less dependent on visibility conditions, concepts using imaging sensors (enhanced vision) and concepts using databases (synthetic vision) have been and are being investigated. Operations that use the capability to 'see' using a database in order to go beyond limits of current operations will require a level of safety equivalent to that of current operations. A capability for the timely detection of hazardous discrepancies between the real world and the depiction of the world that is generated from the database needs to be provided. One potential approach is the use of imaging sensors. Previous research has already addressed the combination of data from imaging sensors with a computer-generated depiction of the environment. The general approach has been to perform spatial and temporal multiresolution image fusion. In the resulting image, none of the original sources is clearly distinguishable. Although such an approach may be desirable to compensate for some of the sensor deficiencies, it may be better if certain specific elements or features from the synthetic world are clearly identifiable as such. This paper discusses research that aims to provide an integration of EVS and SVS in which the EVS data is intended to support the integrity monitoring of the SVS.;Integration of imaging sensor data into a synthetic vision display;Not health related;Not health related;0
"F. Condrea; V. -A. Ivan; M. Leordeanu";2020;Automatically detecting vital signs in videos, such as the estimation of heart and respiration rates, is a challenging research problem in computer vision with important applications in the medical field. One of the key difficulties in tackling this task is the lack of sufficient supervised training data, which severely limits the use of powerful deep neural networks. In this paper we address this limitation through a novel deep learning approach, in which a recurrent deep neural network is trained to detect vital signs in the infrared thermal domain from purely synthetic data. What is most surprising is that our novel method for synthetic training data generation is general, relatively simple and uses almost no prior medical domain knowledge. Moreover, our system, which is trained in a purely automatic manner and needs no human annotation, also learns to predict the respiration or heart intensity signal for each moment in time and to detect the region of interest that is most relevant for the given task, e.g. the nose area in the case of respiration. We test the effectiveness of our proposed system on the recent LCAS dataset and obtain state-of-the-art results.;In Search of Life: Learning from Synthetic Data to Detect Vital Signs in Videos;Not health related;Not health related;0
"P. Dansena; S. Bag; R. Pal";2021;Fraudsters often alter handwritten contents in a document in order to achieve illicit purposes. At times, this may result in financial and mental loss to an individual or an organization. Hence, ink analysis is necessary to identify such an alteration. Convolution Neural Network (CNN) can be used to identify such cases of alteration, as CNN has emerged as a monumental success in the field of computer vision for varieties of classification tasks. But, CNN requires large amount of labeled data for training. Hence, there is a need to generate a large dataset for the experiments relating to handwritten word alteration detection. Collection, digitization, and cropping of a large number of altered and unaltered handwritten words are tedious and time consuming. To overcome such an issue, an approach for synthetic word data generation is presented in this paper for handwritten word alteration detection experiments. This scheme is designed in such a way that the synthetically generated words are very similar to the original ones. In order to achieve this, handwritten character data set is prepared using 10 blue and 10 black pens. These handwritten characters are used for creating synthetic word alteration data set. The presented approach uses relatively less number of handwritten character images to create a huge word alteration data set. Further, deep learning models are trained on the synthetically generated data set for word alteration detection.;Generation of Synthetic Data for Handwritten Word Alteration Detection;Not health related;Not health related;0
"R. Behjati; E. Arisholm; M. Bedregal; C. Tan";2019;Testing in production-like test environments is an essential part of quality assurance processes in many industries. Provisioning of such test environments, for information-intensive services, involves setting up databases that are rich-enough to enable simulating a wide variety of user scenarios. While production data is perhaps the gold-standard here, many organizations, particularly within the public sectors, are not allowed to use production data for testing purposes due to privacy concerns. The alternatives are to use anonymized data, or synthetically generated data. In this paper, we elaborate on these alternatives and compare them in an industrial context. Further we focus on synthetic data generation and investigate the use of recurrent neural networks for this purpose. In our preliminary experiments, we were able to generate representative and highly accurate data using a recurrent neural network. These results open new research questions that we discuss here, and plan to investigate in our future research.;Synthetic Test Data Generation Using Recurrent Neural Networks: A Position Paper;Not health related;Not health related;0
"D. Kornish; S. Ezekiel; M. Cornacchia";2018;Deep convolutional neural networks have recently demonstrated incredible capabilities in areas such as image classification and object detection, but they require large datasets of quality pre-labeled data to achieve high levels of performance. Almost all data is not properly labeled when it is captured, and the process of manually labeling large enough datasets for effective learning is impractical in many real-world applications. New studies have shown that synthetic data, generated from a simulated environment, can be effective training data for DCNNs. However, synthetic data is only as effective as the simulation from which it is gathered, and there is often a significant trade-off between designing a simulation that properly models real-world conditions and simply gathering better real-world data. Using generative network architectures, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), it is possible to produce new synthetic samples based on the features of real-world data. This data can be used to augment small datasets to increase DCNN performance, similar to traditional augmentation methods such as scaling, translation, rotation, and adding noise. In this paper, we compare the advantages of synthetic data from GANs and VAEs to traditional data augmentation techniques. Initial results are promising, indicating that using synthetic data for augmentation can improve the accuracy of DCNN classifiers.;DCNN Augmentation via Synthetic Data from Variational Autoencoders and Generative Adversarial Networks;Not health related;Not health related;0
"A. H. Martinez; J. L. Díaz; I. G. Daza; D. F. Llorca";2021;Despite all the challenges and limitations, vision-based vehicle speed detection is gaining research interest due to its great potential benefits such as cost reduction, and enhanced additional functions. As stated in a recent survey [1], the use of learning-based approaches to address this problem is still in its infancy. One of the main difficulties is the need for a large amount of data, which must contain the input sequences and, more importantly, the output values corresponding to the actual speed of the vehicles. Data collection in this context requires a complex and costly setup to capture the images from the camera synchronized with a high precision speed sensor to generate the ground truth speed values. In this paper we explore the use of synthetic images generated from a driving simulator (e.g., CARLA) to address vehicle speed detection using a learning-based approach. We simulate a virtual camera placed over a stretch of road, and generate thousands of images with variability corresponding to multiple speeds, different vehicle types and colors, and lighting and weather conditions. Two different approaches to map the sequence of images to an output speed (regression) are studied, including CNN-GRU and 3D-CNN. We present preliminary results that support the high potential of this approach to address vehicle speed detection.;Data-driven vehicle speed detection from synthetic driving simulator images;Not health related;Not health related;0
"Y. Sei; J. A. Onesimu; A. Ohsuga";2022;With the development of IoT technology, personal data are being collected in many places. These data can be used to create new services, but consideration must be given to the individual’s privacy. We can safely collect personal data while adding noise by applying differential privacy. However, because such data are very noisy, the accuracy of machine learning trained by the data greatly decreased. In this study, our objective is to build a highly accurate machine learning model using these data. We focus on the decision tree machine learning algorithm, and, instead of applying it as is, we use a preprocessing technique wherein pseudodata are generated using a copula while removing the effect of noise added by differential privacy. In detail, the proposed novel protocol consists of three steps: generating a covariance matrix from the differentially private numerical data, generating a discrete cumulative distribution function from differentially private numerical data, and generating copula-based numerical samples. Simulation results using synthetic and real datasets verify the utility of the proposed method not only for the decision tree algorithm but also for other machine learning algorithms such as deep neural networks. This method will help create machine learning models, such as recommendation systems, using differential privacy data.;Machine Learning Model Generation With Copula-Based Synthetic Dataset for Local Differentially Private Numerical Data;Not health related;Not health related;0
"K. Chaturvedi; A. Braytee; D. K. Vishwakarma; M. Saqib; D. Mery; M. Prasad";2021;With the recent surge in threats to public safety, the security focus of several organizations has been moved towards enhanced intelligent screening systems. Conventional X-ray screening, which relies on the human operator is the best use of this technology, allowing for the more accurate identification of potential threats. This paper explores X-ray security imagery by introducing a novel approach that generates realistic synthesized data, which opens up the possibility of using different settings to simulate occlusion, radiopacity, varying textures, and distractors to generate cluttered scenes. The generated synthetic data is effective in the training of deep networks. It allows better generalization on training data to deal with domain adaptation in the real world. The extensive set of experiments in this paper provides evidence for the efficacy of synthetic datasets over human-annotated datasets for automated X-ray security screening. The proposed approach outperforms the state-of-the-art approach for a diverse threat object dataset on mean Average Precision (mAP) of region-based detectors and classification/regression-based detectors.;Automated Threat Objects Detection with Synthetic Data for Real-Time X-ray Baggage Inspection;Not health related;Not health related;0
"P. Albert; M. Saadeldin; B. Narayanan; B. M. Namee; D. Hennessy; A. O’Connor; N. O’Connor; K. McGuinness";2021;Monitoring species-specific dry herbage biomass is an important aspect of pasture-based milk production systems. Being aware of the herbage biomass in the field enables farmers to manage surpluses and deficits in herbage supply, as well as using targeted nitrogen fertilization when necessary. Deep learning for computer vision is a powerful tool in this context as it can accurately estimate the dry biomass of a herbage parcel using images of the grass canopy taken using a portable device. However, the performance of deep learning comes at the cost of an extensive, and in this case destructive, data gathering process. Since accurate species-specific biomass estimation is labor intensive and destructive for the herbage parcel, we propose in this paper to study low supervision approaches to dry biomass estimation using computer vision. Our contributions include: a synthetic data generation algorithm to generate data for a herbage height aware semantic segmentation task, an automatic process to label data using semantic segmentation maps, and a robust regression network trained to predict dry biomass using approximate biomass labels and a small trusted dataset with gold standard labels. We design our approach on a herbage mass estimation dataset collected in Ireland and also report state-of-the-art results on the publicly released Grass-Clover biomass estimation dataset from Denmark. Our code is available at https://git.io/J0L2a.;Semi-supervised dry herbage mass estimation using automatic data and synthetic images;Not health related;Not health related;0
"J. Ramirez; O. Mendoza-Schrock";2012;In this work, we explore low-dimensional representations of high-dimensional data derived from electro-optical synthetic vehicle images. The collection of vehicle images consists of four different vehicle models: Toyota Camry, Toyota Avalon, Toyota Tacoma, and Nissan Sentra. This data contains 3,601 160 _ 213 gray-scale vehicle images sampled uniformly over a camera view hemisphere. We use the non-linear manifold learning technique of diffusion maps with Gaussian kernel to explore low-dimensional structure the high-dimensional cloud of vehicle image observations. Diffusion maps have been shown to be a valuable tool in the analysis of high-dimensional data and the technique is able to extract an approximation for the underlying structure inherent to the data. Our analysis includes examining how the diffusion time and kernel width leads to different low-dimensional representations and we present a novel technique to relate the kernel width to the distribution of data in the observation space. In addition, we present initial results for multi-class vehicle classification through low-dimensional embedding coordinates and the out-of-sample extension of unlabeled vehicle images.;Diffusion maps for exploring electro-optical synthetic vehicle image data;Not health related;Not health related;0
"A. Ummadisingu; K. Takahashi; N. Fukaya";2022;The food packaging industry handles an immense variety of food products with wide-ranging shapes and sizes, even within one kind of food. Menus are also diverse and change frequently, making automation of pick-and-place difficult. A popular approach to bin-picking is to first identify each piece of food in the tray by using an instance segmentation method. However, human annotations to train these methods are unreli-able and error-prone since foods are packed close together with unclear boundaries and visual similarity making separation of pieces difficult. To address this problem, we propose a method that trains purely on synthetic data and successfully transfers to the real world using sim2real methods by creating datasets of filled food trays using high-quality 3d models of real pieces of food for the training instance segmentation models. Another concern is that foods are easily damaged during grasping. We address this by introducing two additional methods- a novel adaptive finger mechanism to passively retract when a collision occurs, and a method to filter grasps that are likely to cause damage to neighbouring pieces of food during a grasp. We demonstrate the effectiveness of the proposed method on several kinds of real foods.;Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection;Not health related;Not health related;0
"K. Mason; S. Vejdan; S. Grijalva";2019;Collecting, analyzing and gaining insight from large volumes of data is now the norm in an ever increasing number of industries. Data analytics techniques, such as machine learning, are powerful tools used to analyze these large volumes of data. Synthetic data sets are routinely relied upon to train and develop such data analytics methods for several reasons: to generate larger data sets than are available, to generate diverse data sets, to preserve anonymity in data sets with sensitive information, etc. Processing, transmitting and storing data is a key issue faced when handling large data sets. This paper presents an “On the fly” framework for generating big synthetic data sets, suitable for these data analytics methods, that is both computationally efficient and applicable to a diverse set of problems. An example application of the proposed framework is presented along with a mathematical analysis of its computational efficiency, demonstrating its effectiveness. Empirical results indicate that the proposed data generation framework provides a reduction in computational time of ≈ 33% when compared to the alternative approach of generating the data set in full.;An “On The Fly” Framework for Efficiently Generating Synthetic Big Data Sets;Not health related;Not health related;1
"U. T. Tantipongpipat; C. Waites; D. Boob; A. A. Siva; R. Cummings";2021;We introduce the DP-auto-GAN framework for synthetic data generation, which combines the low dimensional representation of autoencoders with the flexibility of Generative Adversarial Networks (GANs). This framework can be used to take in raw sensitive data and privately train a model for generating synthetic data that will satisfy similar statistical properties as the original data. This learned model can generate an arbitrary amount of synthetic data, which can then be freely shared due to the post-processing guarantee of differential privacy. Our framework is applicable to unlabeled mixed-type data, that may include binary, categorical, and real-valued data. We implement this framework on both binary data (MIMIC-III) and mixed-type data (ADULT), and compare its performance with existing private algorithms on metrics in unsupervised settings. We also introduce a new quantitative metric able to detect diversity, or lack thereof, of synthetic data.;Differentially Private Synthetic Mixed-Type Data Generation For Unsupervised Learning;Not health related;Not health related;0
"Q. Yan; J. Zheng; S. Reding; S. Li; I. Doytchinov";2022;We present a visual localization system that learns to estimate camera poses in the real world with the help of synthetic data. Despite significant progress in recent years, most learning-based approaches to visual localization target at a single domain and require a dense database of geo-tagged images to function well. To mitigate the data scarcity issue and improve the scalability of the neural localization models, we introduce TOPO-DataGen, a versatile synthetic data generation tool that traverses smoothly between the real and virtual world, hinged on the geographic camera viewpoint. New large-scale sim-to-real benchmark datasets are proposed to showcase and evaluate the utility of the said synthetic data. Our experiments reveal that synthetic data generically enhances the neural network performance on real data. Furthermore, we introduce CrossLoc, a cross-modal visual representation learning approach to pose estimation that makes full use of the scene coordinate ground truth via self-supervision. Without any extra data, CrossLoc significantly outperforms the state-of-the-art methods and achieves substantially higher real-data sample efficiency. Our code and datasets are all available at crossloc. github. io.;CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data;Not health related;Not health related;0
"D. Cullen; J. Halladay; N. Briner; R. Basnet; J. Bergen; T. Doleck";2022;Anonymous network traffic is more pervasive than ever due to the accessibility of services such as virtual private networks (VPN) and The Onion Router (Tor). To address the need to identify and classify this traffic, machine and deep learning solutions have become the standard. However, high-performing classifiers often scale poorly when applied to real-world traffic classification due to the heavily skewed nature of network traffic data. Prior research has found synthetic data generation to be effective at alleviating concerns surrounding class imbalance, though a limited number of these techniques have been applied to the domain of anonymous network traffic detection. This work compares the ability of a Conditional Tabular Generative Adversarial Network (CTGAN), Copula Generative Adversarial Network (CopulaGAN), Variational Autoencoder (VAE), and Synthetic Minority Over-sampling Technique (SMOTE) to create viable synthetic anonymous network traffic samples. Moreover, we evaluate the performance of several shallow boosting and bagging classifiers as well as deep learning models on the synthetic data. Ultimately, we amalgamate the data generated by the GANs, VAE, and SMOTE into a comprehensive dataset dubbed CMU-SynTraffic-2022 for future research on this topic. Our findings show that SMOTE consistently outperformed the other upsampling techniques, improving classifiers’ F1-scores over the control by ~7.5% for application type characterization. Among the tested classifiers, Light Gradient Boosting Machine achieved the highest F1-score of 90.3% on eight application types.;Evaluation of Synthetic Data Generation Techniques in the Domain of Anonymous Traffic Classification;Not health related;Not health related;0
"L. G. T. Ribas; M. P. Cocron; J. L. Da Silva; A. Zimmer; T. Brandmeier";2021;The use of vehicle in-cabin monitoring has been increasing to fulfil the specifications of European safety regulations. These regulations present several requirements for detecting driver distraction, and more complex requirements are soon to be expected in higher automation levels. Today's restraint systems provide optimal protection in standard frontal seat positions and deviations to this might cause severe airbag-induced injuries. This makes in-cabin monitoring critical to improve safety and mitigate dangerous situations in case of a crash, and especially in high levels of autonomous driving. Defining the best sensor positioning inside the vehicle's cabin is a challenge due to its constraints and limitations. The main aim of this work was to verify if simulated 3D human models integrated into a 3D modelled vehicle interior environment can be used to run Deep Learning based human pose estimation models. To perform such task, we utilized the software MakeHuman combined with Blender, to build the virtual environment and create photorealistic scenes containing selected front occupants' postures use cases and then feed into Openpose and Mask R-CNN models. The results showed that using a 2D HPE (Human Pose Estimation) network pre-trained on real data, can detect successfully photorealistic synthetic data of humans under complex scenarios. It is also shown that complex and rare postures can cause failure on 2D HPE detections, as shown in the literature review. This work helps to define the most suitable camera positions which, in combination with specific camera lenses, can deliver quality images for a robust pose detection.;In-Cabin vehicle synthetic data to test Deep Learning based human pose estimation models;Not health related;Not health related;0
"K. Yang; R. Gu; M. Wang; M. Toyoura; G. Xu";2022;A key challenge in the task of human pose and shape estimation is occlusion, including self-occlusions, object-human occlusions, and inter-person occlusions. The lack of diverse and accurate pose and shape training data becomes a major bottleneck, especially for scenes with occlusions in the wild. In this paper, we focus on the estimation of human pose and shape in the case of inter-person occlusions, while also handling object-human occlusions and self-occlusion. We propose a novel framework that synthesizes occlusion-aware silhouette and 2D keypoints data and directly regress to the SMPL pose and shape parameters. A neural 3D mesh renderer is exploited to enable silhouette supervision on the fly, which contributes to great improvements in shape estimation. In addition, keypoints-and-silhouette-driven training data in panoramic viewpoints are synthesized to compensate for the lack of viewpoint diversity in any existing dataset. Experimental results show that we are among the state-of-the-art on the 3DPW and 3DPW-Crowd datasets in terms of pose estimation accuracy. The proposed method evidently outperforms Mesh Transformer, 3DCrowdNet and ROMP in terms of shape estimation. Top performance is also achieved on SSP-3D in terms of shape prediction accuracy. Demo and code will be available at https://igame-lab.github.io/LASOR/.;LASOR: Learning Accurate 3D Human Pose and Shape via Synthetic Occlusion-Aware Data and Neural Mesh Rendering;Not health related;Not health related;0
"S. Iannucci; H. A. Kholidy; A. D. Ghimire; R. Jia; S. Abdelwahed; I. Banicescu";2017;Property-graphs are becoming popular for Intrusion Detection Systems (IDSs) because they allow to leverage distributed graph processing platforms in order to identify malicious network traffic patterns. However, a benchmark for studying their performance when operating on big data has not yet been reported. In general, benchmarking a system involves the execution of workloads on datasets, where both of them must be representative of the application of interest. However, few datasets containing real network traffic are openly available due to privacy concerns, which in turn could limit the scope and results of the benchmark. In this work, we build two synthetic data generators for benchmarking next generation IDSs by introducing the support for property-graphs in two well-known graph generation algorithms: Barabási-Albert and Kronecker. We run an extensive experimental evaluation using a publicly available dataset as seed for the data generation, and we show that the proposed approach is able to generate synthetic datasets with high veracity, while also exhibiting linear performance scalability.;A Comparison of Graph-Based Synthetic Data Generators for Benchmarking Next-Generation Intrusion Detection Systems;Not health related;Not health related;0
"J. Kainulainen; K. Rautiainen; J. Lemmetyinen; M. T. Hallikainen; F. Martin-Porqueras; M. Martin-Neira";2011;L-band radiometry is widely considered the best technique for Earth observing satellites to measure sea surface salinity (SSS). Interferometric aperture synthesis is a new technology applicable in spaceborne remote sensing at low frequencies. The challenge of the technology comes with decreased radiometric resolution and complexity in calibration compared to conventional radiometer systems. Due to these issues and the overall newness of the concept, validation of the technology for salinity retrieval purposes is desired. In this paper, we describe an intense measurement campaign carried out with the complete interferometric aperture synthesis radiometer system HUT-2-D, designed and operated by the Helsinki University of Technology. The campaign aimed at the detection of a changing salinity level in the Baltic Sea, in the coastal areas of Finland. We describe the campaign comprising details of the ground truth collection, sea surface emission modeling, and radiometric data analysis. We have a special emphasis on the assessment of the impact of the sea state on the radiometric measurements, which is considered one of the major obstacles for SSS retrieval at the L-band. For this purpose, we present a new correlation between sea roughness information collected with the Global Navigation Satellite System reflectometer and radiometric data measured by an L-band radiometer system.;Detection of a Sea Surface Salinity Gradient Using Data Sets of Airborne Synthetic Aperture Radiometer HUT-2-D and a GNSS-R Instrument;Not health related;Not health related;0
"D. Salvi; P. Bestagini; S. Tubaro";2022;In recent years, numerous techniques to manipulate multimedia data and generate hyper-realistic synthetic content have been presented. These inauthentic data are hazardous as they can lead to numerous threats and dangers when misused. This has led the forensic community to propose multiple approaches to tackle both detection and attribution problems. Solving the detection problem consists in determining whether some given data is genuine or false. Solving the attribution problem consists in determining which specific technique has been used to manipulate or generate the observed data. In this paper we address the attribution problem on synthetic speech. We consider a set of methods initially proposed for synthetic speech detection, and adapt them to identify which speech generation algorithm has been used to synthesize a speech track. Our goal is to sample the versatility of these systems and verify how far the detection and attribution tasks are from each other. We test the models in a closed-set scenario and compare their performance with that of a well-established baseline. Moreover, we propose different solutions to address the task in an open-set situation. The encouraging results show that the considered methods can provide a representation of the input signal that is meaningful for both detection and attribution.;Exploring the Synthetic Speech Attribution Problem Through Data-Driven Detectors;Not health related;Not health related;0
"D. Li; G. Liao; Y. Liao; L. Yang";2015;With appropriate geometry configuration, helicopter-borne rotating synthetic aperture radar (ROSAR) can break through the limitations of monostatic synthetic aperture radar (SAR) on forward-looking imaging. With this capability, ROSAR has extensive potential applications, such as self-navigation and self-landing. Moreover, it has many advantages if combined with the frequency modulated continuous wave (FMCW) technology. A novel geometric configuration and an imaging algorithm for helicopter-borne FMCW-ROSAR are proposed. Firstly, by performing the equivalent phase center principle, the separated transmitting and receiving antenna system is equalized to the case of system configuration with antenna for both transmitting and receiving signals. Based on this, the accurate two-dimensional spectrum is obtained and the Doppler frequency shift effect induced by the continuous motion of the platform during the long pulse duration is compensated. Next, the impacts of the velocity approximation error on the imaging algorithm are analyzed in detail, and the system parameters selection and resolution analysis are presented. The well-focused SAR image is then obtained by using the improved Omega-K algorithm incorporating the accurate compensation method for the velocity approximation error. Finally, correctness of the analysis and effectiveness of the proposed algorithm are demonstrated through simulation results.;Modified Omega-K algorithm for processing helicopter-borne frequency modulated continuous waveform rotating synthetic aperture radar data;Not health related;Not health related;0
"A. Cortés; C. Rodríguez; G. Vélez; J. Barandiarán; M. Nieto";2022;A major challenges of deep learning (DL) is the necessity to collect huge amounts of training data. Often, the lack of a sufficiently large dataset discourages the use of DL in certain applications. Typically, acquiring the required amounts of data costs considerable time, material and effort. To mitigate this problem, the use of synthetic images combined with real data is a popular approach, widely adopted in the scientific community to effectively train various detectors. In this study, we examined the potential of synthetic data-based training in the field of intelligent transportation systems. Our focus is on camera-based traffic sign recognition applications for advanced driver assistance systems and autonomous driving. The proposed augmentation pipeline of synthetic datasets includes novel augmentation processes such as structured shadows and gaussian specular highlights. A well-known DL model was trained with different datasets to compare the performance of synthetic and real image-based trained models. Additionally, a new, detailed method to objectively compare these models is proposed. Synthetic images are generated using a semi-supervised errors-guide method which is also described. Our experiments showed that a synthetic image-based approach outperforms in most cases real image-based training when applied to cross-domain test datasets (+10% precision for GTSRB dataset) and consequently, the generalization of the model is improved decreasing the cost of acquiring images.;Analysis of Classifier Training on Synthetic Data for Cross-Domain Datasets;Not health related;Not health related;0
"T. N. Duong; T. G. Do; T. N. Cao; M. H. Tran";2022;Recommendation systems have been widely adopted to help users with the information overload from the large volume of online multimedia content by providing them with appropriate options. While modern hybrid recommendation systems require an immense amount of data, several existing online privacy issues make users skeptical about sharing their personal information with service providers. This work introduces various novel methods utilizing the baseline estimate to learn user interests from their interactions. Subsequently, synthetic user feature vectors are implemented to estimate the user-item correlations, providing an additional fine-tuning factor for neighborhood-based collaborative filtering systems. Comprehensive experiments show that utilizing the user-item similarity can boost the accuracy of hybrid neighborhood-based systems by at least 2.11% while minimizing the need for tracking users’ digital footprints.;User-Item Correlation in Hybrid Neighborhood-Based Recommendation System with Synthetic User Data;Not health related;Not health related;0
"B. Flanagan; R. Majumdar; H. Ogata";2022;While data privacy is a key aspect of Learning Analytics, it often creates difficulty when promoting research into underexplored contexts as it limits data sharing. To overcome this problem, the generation of synthetic data has been proposed and discussed within the LA community. However, there has been little work that has explored the use of synthetic data in real-world situations. This research examines the effectiveness of using synthetic data for training academic performance prediction models, and the challenges and limitations of using the proposed data sharing method. To evaluate the effectiveness of the method, we generate synthetic data from a private dataset, and distribute it to the participants of a data challenge to train prediction models. Participants submitted their models as docker containers for evaluation and ranking on holdout synthetic data. A post-hoc analysis was conducted on the top 10 participant’s models by comparing the evaluation of their performance on synthetic and private validation datasets. Several models trained on synthetic data were found to perform significantly poorer when applied to the non-synthetic private dataset. The main contribution of this research is to understand the challenges and limitations of applying predictive models trained on synthetic data in real-world situations. Due to these challenges, the paper recommends model designs that can inform future successful adoption of synthetic data in real-world educational data systems.;Fine Grain Synthetic Educational Data: Challenges and Limitations of Collaborative Learning Analytics;Not health related;Not health related;0
"K. Halupka; R. Garnavi; S. Moore";2019;Tree-like structures, such as blood vessels, often express complexity at very fine scales, requiring high-resolution grids to adequately describe their shape. Such sparse morphology can alternately be represented by locations of centreline points, but learning from this type of data with deep learning is challenging due to it being unordered, and permutation invariant. In this work, we propose a deep neural network that directly consumes unordered points along the centreline of a branching structure, to identify the topology of the represented structure in a single-shot. Key to our approach is the use of a novel multi-task loss function, enabling instance segmentation of arbitrarily complex branching structures. We train the network solely using synthetically generated data, utilizing domain randomization to facilitate the transfer to real 2D and 3D data. Results show that our network can reliably extract meaningful information about branch locations, bifurcations and endpoints, and sets a new benchmark for semantic instance segmentation in branching structures.;Deep Semantic Instance Segmentation of Tree-Like Structures Using Synthetic Data;Not health related;Not health related;0
"A. Britto Mattos; D. A. Borges Oliveira; E. da silva Morais";2018;Recently, Deep Learning-based methods have obtained high accuracy for the problem of Visual Speech Recognition. However, while good results have been reported for words and sentences, recognizing shorter segments of speech, like phones, has proven to be much more challenging due to the lack of temporal and contextual information. In this work, we address the problem of recognizing visemes, that are the visual equivalent of phonemes-the smallest distinguishable sound unit in a spoken word. Viseme recognition has application in tasks such as lip synchronization, but acquiring and labeling a viseme dataset is complex and time-consuming. We tackle this problem by creating a large-scale synthetic 2D dataset based on realistic 3D facial models, automatically labelled. Then, we extract real viseme images from the GRID corpus-using audio data to locate phonemes via forced phonetic alignment and the registered video to extract the corresponding visemes-and evaluate the applicability of the synthetic dataset for recognizing real-world data.;Improving CNN-Based Viseme Recognition Using Synthetic Data;Not health related;Not health related;0
"V. S. Frost; G. J. Minden";1986;A data compression technique is developed for synthetic aperture radar (SAR) imagery. The technique is based on an SAR image model and is designed to preserve the local statistics in the image by an adaptive variable rate modification of block truncation coding (BTC). A data rate of approximately 1.6 bit/pixel is achieved with the technique while maintaining the image quality and cultural (pointlike) targets. The algorithm requires no large data storage and is computationally simple.;A Data Compression Technique for Synthetic Aperture Radar Images;Not health related;Not health related;0
"N. Howard; A. Park; T. Z. Shabestary; A. Gruenstein; R. Prabhavalkar";2021;"We consider the problem of recognizing speech utterances spoken to a device which is generating a known sound waveform; for example, recognizing queries issued to a digital assistant which is generating responses to previous user inputs. Previous work has proposed building acoustic echo cancellation (AEC) models for this task that optimize speech enhancement metrics using both neural network as well as signal processing approaches.Since our goal is to recognize the input speech, we consider enhancements which improve word error rates (WERs) when the predicted speech signal is passed to an automatic speech recognition (ASR) model. First, we augment the loss function with a term that produces outputs useful to a pre-trained ASR model and show that this augmented loss function improves WER metrics. Second, we demonstrate that augmenting our training dataset of real world examples with a large synthetic dataset improves performance. Crucially, applying SpecAugment style masks to the reference channel during training aids the model in adapting from synthetic to real domains. In experimental evaluations, we find the proposed approaches improve performance, on average, by 57% over a signal processing baseline and 45% over the neural AEC model without the proposed changes.";A Neural Acoustic Echo Canceller Optimized Using An Automatic Speech Recognizer and Large Scale Synthetic Data;Not health related;Not health related;0
"I. LaHaie; A. Dias; G. Darling";1984;"The digital processing requirements of several algorithms for extracting the spectrum of a detected synthetic aperture radar (SAR) image from the raw SAR data are described and compared. The most efficient algorithms for image spectrum extraction from raw SAR data appear to be those containing an intermediate image formation step. It is shown that a recently developed compact formulation of the image spectrum in terms of the raw data is computationally inefficient when evaluated directly, in comparison with the classical method where matched-filter image formation is an intermediate result. It is also shown that a proposed indirect procedure for digitally implementing the same compact formulation is somewhat more efficient than the classical matched-filtering approach. However, this indirect procedure includes the image formation process as part of the total algorithm. Indeed, the computational savings afforded by the indirect implementation are identical to those obtained in SAR image formation processing when the matched-filtering algorithm is replaced by the well-known ""dechirp-Fourier transform"" technique. Furthermore, corrections to account for slant-to-ground range conversion, spherical earth, etc., are often best implemented in the image domain, making intermediate image formation a valuable processing feature.";Digital processing considerations for extraction of ocean wave image spectra from raw synthetic aperture radar data;Not health related;Not health related;0
"Z. Niu; M. Z. Reformat; W. Tang; B. Zhao";2020;The fourth industrial revolution - Industry 4.0 - puts emphasis on the application of intelligent technologies in the area of monitoring and identification of electrical equipment. High precision and non-contact qualities make the infrared thermography one of the most suitable technologies for intelligent inspection of high-voltage apparatus. Yet, due to imperfect data acquisition methods and difficulties in collecting data, electrical equipment images are limited in quantities and imbalanced in representing different types of devices. Additionally, it is not easy to extract representative features of infrared images due to their low-intensity contrast and uneven distribution. In this paper, a data-driven framework is proposed for the identification of electrical equipment based on infrared images. The main technique of this proposed system is a novel process of generating synthetic infrared images. For this purpose, an Edge-Oriented Generative Adversarial Network (EOGAN) is developed. The EOGAN is designed to create realistic infrared images that can be used as augmented data for developing data-driven identification methods. Extracted edge features of electrical equipment are utilized as prior information to guide the process of generating realistic infrared images. Finally, comparative experiments are carried out to show the effectiveness of the proposed EOGAN-based framework for equipment identification in the presence of limited and imbalanced image datasets.;Electrical Equipment Identification Method With Synthetic Data Using Edge-Oriented Generative Adversarial Network;Not health related;Not health related;0
"S. A. A. Shah; M. Bennamoun; M. K. Molton";2019;This paper proposes a novel machine learning approaches to predict the outcome of facial rejuvenation prior to a cosmetic procedure. This is achieved by estimating the required amount of dermal filler volume that needs to be applied on the face by learning the underlying structural mapping from the pretreatment and posttreatment 3D face images. We develop and train our proposed deep neural network, called Rejuv3DNet, designed specifically to predict the dermal filler volume. We also propose the kernel regression (KR)-based model to validate and improve our volume estimation results using regression. Our other contributions include the development of the first 3D face cosmetic dataset, which consists of real-world pretreatment and posttreatment 3D face images and a novel technique for the generation of synthetic cosmetic treatment 3D face images. Our experimental results show that the proposed Rejuv3DNet and the KR model achieve 62.5% and 66.67%, respectively, on real-world data, while these techniques achieve a prediction accuracy of 75.2% and 89.5%, and 77.2% and 90.1% on our two different synthetic datasets. Our proposed techniques have been found to be computationally efficient, achieving near real-time prediction performance. The reported accuracies are our preliminary results for proof of concept, which can be improved with more data. The proposed approach has the potential for further investigation in the cosmetic surgery domain.;Machine Learning Approaches for Prediction of Facial Rejuvenation Using Real and Synthetic Data;Not health related;Not health related;0
"F. Chen; H. Kataoka; Y. Satoh";2017;Traffic informatory signs, which is a category of traffic signs and text-based signs, is very important to both drivers and intelligent transport systems. Previous studies have usually sought to extract text lines in signs to apply to optical character recognition (OCR) system, but they do not work well in real-world conditions with severe disturbance. In this paper, we report on our study of place name text detection and recognition on traffic informatory signs using convolutional neural networks (CNNs) and transform traditional text detection and recognition into word-level multi-class robust image classification. In our study, each place name corresponds to one class. Because the number of word classes is large and collecting real images for training dataset is difficult, we generate several synthetic datasets mainly by means of two methods and use them to train the CNN respectively. One method generates with standard templates of the signs, while the other method renders text in natural images which have no relationship with the signs. Our experimental results show that our method can achieve high levels of accuracy when reading traffic informatory signs in real-world conditions. Accuracy of 0.891 and 0.981 are achieved in the former and latter method of generating dataset, which verify that proposed methods are effective in our task. We also analyze the dependence of color channels in text detection during our task, which help generate the more efficacious synthetic dataset.;Text Detection in Traffic Informatory Signs Using Synthetic Data;Not health related;Not health related;0
"A. Wijayasiri; T. Banerjee; S. Ranka; S. Sahni; M. Schmalz";2017;Hybrid multicore processors (HMPs) are poised to dominate the landscape of the next generation of computing on the desktop as well as on exascale systems. HMPs consist of general purpose CPU cores along with specialized co-processors and can provide high performance for a wide spectrum of applications at significantly lower energy requirements per FLOP. In this paper, we develop parallel algorithms and software for constructing multi-resolution SAR images on HMPs. We develop several load balancing algorithms for optimizing time performance and energy on HMPs. We also present a systematic approach for deriving the energy-time performance trade-offs on HMPs in the presence of Dynamic Voltage Frequency Scaling. Pareto-optimal curves are presented on a system consisting of 24 traditional cores and a GPU.;Parallel Dynamic Data Driven Approaches for Synthetic Aperture Radar;Not health related;Not health related;0
"D. Rukhovich; D. Mouritzen; R. Kaestner; M. Rufli; A. Velizhev";2019;This paper addresses the problem of scale estimation in monocular SLAM by estimating absolute distances between camera centers of consecutive image frames. These estimates would improve the overall performance of classical (not deep) SLAM systems and allow metric feature locations to be recovered from a single monocular camera. We propose several network architectures that lead to an improvement of scale estimation accuracy over the state of the art. In addition, we exploit a possibility to train the neural network only with synthetic data derived from a computer graphics simulator. Our key insight is that, using only synthetic training inputs, we can achieve similar scale estimation accuracy as that obtained from real data. This fact indicates that fully annotated simulated data is a viable alternative to existing deep-learning-based SLAM systems trained on real (unlabeled) data. Our experiments with unsupervised domain adaptation also show that the difference in visual appearance between simulated and real data does not affect scale estimation results. Our method operates with low-resolution images (0.03 MP), which makes it practical for real-time SLAM applications with a monocular camera.;Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data;Not health related;Not health related;0
"M. Ballout; M. Tuqan; D. Asmar; E. Shammas; G. Sakr";2020;In this paper, we study the value of using synthetically produced videos as training data for neural networks used for action categorization. Motivated by the fact that texture and background of a video play little to no significant roles in optical flow, we generated simplified textureless and background-less videos and utilized the synthetic data to train a Temporal Segment Network (TSN). The results demonstrated that augmenting TSN with simplified synthetic data improved the original network accuracy (68.5%), achieving 71.8% on HMDB-51 when adding 4,000 videos and 72.4% when adding 8,000 videos. Also, training using simplified synthetic videos alone on 25 classes of UCF-101 achieved 30.71% when trained on 2500 videos and 52.7% when trained on 5000 videos. Finally, results showed that when reducing the number of real videos of UCF-25 to 10% and combining them with synthetic videos, the accuracy drops to only 85.41%, compared to a drop to 77.4% when no synthetic data is added.;The benefits of synthetic data for action categorization;Not health related;Not health related;0
"M. Lyssenko; C. Gladisch; C. Heinzemann; M. Woehrle; R. Triebel";2021;The evaluation of camera-based perception functions in automated driving (AD) is a significant challenge and requires large-scale high-quality datasets. Recently proposed metrics for safety evaluation additionally require detailed per-instance annotations of dynamic properties such as distance and velocities that may not be available in openly accessible AD datasets. Synthetic data from 3D simulators like CARLA may provide a solution to this problem as labeled data can be produced in a structured manner. However, CARLA currently lacks instance segmentation ground truth. In this paper, we present a back projection pipeline that allows us to obtain accurate instance segmentation maps for CARLA, which is necessary for precise per-instance ground truth information. Our evaluation results show that per-pedestrian depth aggregation obtained from our instance segmentation is more precise than previously available approximations based on bounding boxes especially in the context of crowded scenes in urban automated driving.;Instance Segmentation in CARLA: Methodology and Analysis for Pedestrian-oriented Synthetic Data Generation in Crowded Scenes;Not health related;Not health related;0
"S. O. Ooko; D. Mukanyiligira; J. P. Munyampundu; J. Nsenga";2021;Diseases that affect the respiratory system are one of the main causes of death across the globe. There is a need for a personalized, easy to use and convenient mechanism to self-detect a potentially contagious disease, thus limiting the spread of infections. The integration of Artificial Intelligence (AI) and Internet of Things (IoT) provides a great opportunity to bring detection and monitoring of respiratory diseases at home. However, the development of efficient AI models has been hindered by the lack of datasets for the targeted biomarkers, with privacy concerns limiting open data access and sharing. Starting from an existing small dataset of COPD, this study leverages the emerging synthetic data technology to artificially augment its size to have adequate data for training a TinyML model for predicting COPD using a Neural Network model, thus improving the inference accuracy of portable noninvasive self-diagnostic kits for respiratory disease. An online platform, Mostly AI is used to synthetically enhance data, the platform uses deep neural networks with inbuilt mechanisms that retain valuable information while providing a good as a real anonymous dataset. Next, the Keras Neural Network is used to train the edge AI models. The performance of the model was evaluated and the results show that when the same training parameters were applied, the model trained from synthetic data performed with an accuracy almost similar to that based on real open datasets. The use of synthetic data will complement the few breaths that are collected in healthcare facilities. This will enable the training of efficient AI models for respiratory diseases.;Synthetic Exhaled Breath Data-Based Edge AI Model for the Prediction of Chronic Obstructive Pulmonary Disease;health related;health related;1
"R. Azaro; G. Bozza; C. Estatico; A. Massa; M. Pastorino; D. Pregnolato; A. Randazzo";2006;A new approach for the inversion of synthetic and measured scattered data is proposed in this paper. The approach is based on an iterative technique in which the nonlinear equations of the inverse-scattering problem are solved within the pth-order Born approximation. A regularization scheme based on an inexact-Newton method is applied. Several numerical simulations and experimental results are reported. Multiple separated dielectric cylinders are localized and reconstructed in a noisy environment.;New results on electromagnetic imaging based on the inversion of synthetic and measured scattered-field data;Not health related;Not health related;0
"J. J. Bird; D. R. Faria; C. Premebida; A. Ekárt; P. P. S. Ayrosa";2020;Autonomous speaker identification suffers issues of data scarcity due to it being unrealistic to gather hours of speaker audio to form a dataset, which inevitably leads to class imbalance in comparison to the large data availability from non-speakers since large-scale speech datasets are available online. In this study, we explore the possibility of improving speaker recognition by augmenting the dataset with synthetic data produced by training a Character-level Recurrent Neural Network on a short clip of five spoken sentences. A deep neural network is trained on a selection of the Flickr8k dataset as well as the real and synthetic speaker data (all in the form of MFCCs) as a binary classification problem in order to discern the speaker from the Flickr speakers. Ranging from 2,500 to 10,000 synthetic data objects, the network weights are then transferred to the original dataset of only Flickr8k and the real speaker data, in order to discern whether useful rules can be learnt from the synthetic data. Results for all three subjects show that fine-tune learning from datasets augmented with synthetic speech improve the classification accuracy, F1 score, precision, and the recall when applied to the scarce real data vs non-speaker data. We conclude that even with just five spoken short sentences, data augmentation via synthetic speech data generated by a Char- RNN can improve the speaker classification process. Accuracy and related metrics are shown to improve from around 93% to 99% for three subjects classified from thousands of others when fine-tuning from exposure to 2500-1000 synthetic data points. High F1 scores, precision and recall also show that issues due to class imbalance are also solved.;Overcoming Data Scarcity in Speaker Identification: Dataset Augmentation with Synthetic MFCCs via Character-level RNN;Not health related;Not health related;0
"S. Shah; D. Gandhi; J. Kothari";2020;"Machine learning has made a drastic impact in today's world. Developments in machine learning are happening every day at an exponential rate. However, there are still some fields that are comparatively untouched by its impact. Areas like the medical and sports sector that could benefit immensely by utilizing the advancements in Machine Learning and still are lagging solely because of one crucial reason: unavailability of data. The unavailability of data results in scarcity of data used for training the machine learning models, which directly affects the accuracy of the models, making them less reliable for real-time usage. To counter this roadblock, this paper is proposing a solution to generating synthetic data in this paper. As the name suggests, a synthetic dataset is a repository of data that is generated programmatically. So, it is not collected by any real-life survey or experiment. It's the primary purpose; therefore, it is to be flexible and rich enough to help a Machine Learning practitioner conduct fascinating experiments with various classification, regression, and clustering algorithms. Thus, using this approach, iterative regression analysis was applied to generate synthetic data using a data set that was used in the field of sports. The generated data was then used along with the original dataset to train a new model that brought about a significant increase in the accuracy of the model to predict features.";Machine learning based Synthetic Data Generation using Iterative Regression Analysis;Not health related;Not health related;0
"Y. -T. Hu; J. Wang; R. A. Yeh; A. G. Schwing";2021;Extracting detailed 3D information of objects from video data is an important goal for holistic scene understanding. While recent methods have shown impressive results when reconstructing meshes of objects from a single image, results often remain ambiguous as part of the object is unobserved. Moreover, existing image-based datasets for mesh reconstruction don't permit to study models which integrate temporal information. To alleviate both concerns we present SAIL-VOS 3D: a synthetic video dataset with frame-by-frame mesh annotations which extends SAIL-VOS. We also develop first baselines for reconstruction of 3D meshes from video data via temporal models. We demonstrate efficacy of the proposed baseline on SAIL-VOS 3D and Pix3D, showing that temporal information improves reconstruction quality. Resources and additional information are available at http://sailvos.web.illinois.edu.;SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction from Video Data;Not health related;Not health related;0
"J. Han; S. Karaoglu; H. -A. Le; T. Gevers";2021;This paper is to provide an overview of how object features from images influence face detection performance, and how to select synthetic faces to address specific features. To this end, we investigate the effects of occlusion, scale, viewpoint, background, and noise by using a novel synthetic image generator based on 3DU Face Dataset. To examine the effects of different features, we selected three detectors (Faster RCNN, HR, SSH) as representative of various face detection methodologies. Comparing different configurations of synthetic data on face detection systems, it showed that our synthetic dataset could complement face detectors to become more robust against features in the real world. Our analysis also demonstrated that a variety of data augmentation is necessary to address nuanced differences in performance.;Object features and face detection performance: Analyses with 3D-rendered synthetic data;Not health related;Not health related;0
"Y. -T. Hu; J. Wang; R. A. Yeh; A. G. Schwing";2021;Extracting detailed 3D information of objects from video data is an important goal for holistic scene understanding. While recent methods have shown impressive results when reconstructing meshes of objects from a single image, results often remain ambiguous as part of the object is unobserved. Moreover, existing image-based datasets for mesh reconstruction don’t permit to study models which integrate temporal information. To alleviate both concerns we present SAIL-VOS 3D: a synthetic video dataset with frame-by-frame mesh annotations which extends SAIL-VOS. We also develop first baselines for reconstruction of 3D meshes from video data via temporal models. We demonstrate efficacy of the proposed baseline on SAIL-VOS 3D and Pix3D, showing that temporal information improves reconstruction quality. Resources and additional information are available at http://sailvos.web.illinois.edu.;SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction from Video Data;Not health related;Not health related;0
"S. Noh; S. Back; R. Kang; S. Shin; K. Lee";2020;In this paper, we present a deep learning-based approach to detect and identify multiple fasteners from various camera poses. To distinguish fasteners of similar size and shape from each other, we propose a part identifier network and simple visual calibration method using a reference image. Though the camera poses changes, the model can infer the actual scale of detected parts by just capturing a reference object at once. Also, we present a synthetic data generation pipeline that adopts domain randomization and can automatically generate a training set for various fastener identification. In the experiment, we evaluated the real-world performance of the fully synthetically trained model and showed that it could be directly applied to real-world part identification. This indicates that our approach has the potential to accelerate the model retraining procedure for various part identification tasks since data acquisition requires almost no cost.;Automatic Detection and Identification of Fasteners with Simple Visual Calibration using Synthetic Data;Not health related;Not health related;0
"J. Lyu; Z. Wang; F. Xu";2022;In portraits, eyeglasses may occlude facial regions and generate cast shadows on faces, which degrades the performance of many techniques like face verification and expression recognition. Portrait eyeglasses removal is critical in handling these problems. However, completely removing the eyeglasses is challenging because the lighting effects (e.g., cast shadows) caused by them are often complex. In this paper, we propose a novel framework to remove eyeglasses as well as their cast shadows from face images. The method works in a detect-then-remove manner, in which eyeglasses and cast shadows are both detected and then removed from images. Due to the lack of paired data for supervised training, we present a new synthetic portrait dataset with both intermediate and final supervisions for both the detection and removal tasks. Furthermore, we apply a cross-domain technique to fill the gap between the synthetic and real data. To the best of our knowledge, the proposed technique is the first to remove eyeglasses and their cast shadows simultaneously. The code and synthetic dataset are available at https://gethub.com/StoryMY/take-off-eyeglasses.;Portrait Eyeglasses and Shadow Removal by Leveraging 3D Synthetic Data;Not health related;Not health related;0
"P. Viertel; M. König; J. Rexilius";2021;"Palynology, the study of pollen, is becoming the focus of attention in computer vision in recent years. Various proposed automated classification and segmentation methods have been evaluated on a number of data sets. However, as of 2021 most data sets are sparse; they either contain only a small number of pollen classes, images in total or are imbalanced overall. In this work, we explore the possibility of creating synthetic pollen grain images from less than 2,000 images per pollen class via a Generative Adversarial Network (GAN). For that purpose, we selected two distinct pollen classes from a state of the art pollen data set and evaluated the data set with and without synthetic data on a Convolutional Neural Network (CNN). The enriched data set performed better overall (+1.4%) and specifically for the two pollen classes (+2%). We also drastically reduced the no. of real images and were still able to achieve a score of 60% to 80%. The experiments show, that our synthesized pollen images are visually close to real-life pollen grains and can be used to enrich imbalanced data sets as an addition to traditional data augmentation methods.";PollenGAN: Synthetic Pollen Grain Image Generation for Data Augmentation;Not health related;Not health related;1
"H. Viggh; S. Loughran; Y. Rachlin; R. Allen; J. Ruprecht";2023;On-orbit satellite servicing can benefit from increased local autonomy when long time delays prevent effective teleoperation. One required perception capability is automated detection and classification of spacecraft components. Recent developments in Deep Learning (DL) object detection can be applied to spacecraft component detection and classification. However, training such algorithms requires large amounts of labeled training images. In the satellite servicing domain, large datasets of labeled on-orbit satellite images generally do not exist. In this paper, we compare two approaches for training DL object detectors for data starved applications. In the first approach, a small number of real satellite images were manually labeled and then augmented via transformations to obtain sufficient training images. In the second, synthetic images were generated in simulation and automatically labeled, creating four datasets, one the same size as the augmented real dataset and three larger datasets. Our main objective was to demonstrate that synthetic images can be used to successfully train spacecraft component detectors, eliminating the need to collect and manually label large sets of real satellite images. Our secondary objective was to assess the effect of the size of the synthetic training dataset on detection performance. The small set of real images used are photographs of the Hubble Space Telescope (HST) taken during HST servicing missions. The Unity video game engine was used to simulate the HST and space environment, and to generate synthetic image datasets in which the lighting, viewing geometry, and surface material properties were varied. Labeling involved drawing bounding boxes around five object classes: the telescope aperture cover, telescope baffle, solar panels, antennas, and spacecraft bus. Multiple DL Faster Regional Convolutional Neural Network (Faster R-CNN) object detectors were trained to detect all five classes using multiple random subsets of the augmented real dataset and of the synthetic datasets. All of the detectors were tested on random subsets of images from the augmented real dataset. Receiver Operating Characteristic (ROC) curves were generated and the Area Under the Curve (AUC) was calculated for each class. For each Faster R-CNN detector, the average AUC across all classes was used as a single figure of merit for object detection performance. We found that detectors trained on synthetic Unity images can perform as well or better than detectors trained on an augmented set of a small number of real images, and that performance improves as the number of synthetic training images increases.;Training Deep Learning Spacecraft Component Detection Algorithms Using Synthetic Image Data;Not health related;Not health related;0
"I. L. Carreño; R. Ramakrishna; A. Scaglione; D. Arnold; C. Roberts; S. -T. Ngo; S. Peisert; D. Pinney";2020;In this paper, we present SoDa, an irradiance-based synthetic Solar Data generation tool to generate realistic sub-minute solar photovoltaic (PV) output power time series, that emulate the weather pattern for a certain geographical location. Our tool relies on the National Solar Radiation Database (NSRDB) to obtain irradiance and weather data patterns for the site. Irradiance is mapped onto a PV model estimate of a solar plant's 30-min power output, based on the configuration of the panel. The working hypothesis to generate high-resolution (e.g. 1 second) solar data is that the conditional distribution of the time series of solar power output given the cloud density is the same for different locations. We therefore propose a stochastic model with a switching behavior due to different weather regimes as provided by the cloud type label in the NSRDB, and train our stochastic model parameters for the cloudy states on the high-resolution solar power measurements from a Phasor Measurement Unit (PMU). In the paper we introduce the stochastic model, and the methodology used for the training of its parameters. The numerical results show that our tool creates synthetic solar time series at high resolutions that are statistically representative of the measured solar power and illustrate how to make use of the tool to create synthetic data for arbitrary sites in the footprint covered by the NSRDB.;SoDa: An Irradiance-Based Synthetic Solar Data Generation Tool;Not health related;Not health related;0
"M. J. Collins; Jin Huang";1998;Spatial analysis of synthetic aperture radar (SAR) image data holds much promise for characterizing and discriminating environmental scene elements. The autocorrelation function (ACF) has been identified as a potentially useful spatial metric because it admits an analysis with conventional linear system theory. Recent models of spatial scattering suggest that ACF-based texture analysis of SAR image data is capable of discriminating between a variety of area extensive targets. The incorporation of texture in an image classification or segmentation system requires some understanding of the uncertainties in the texture estimates. In this paper, the authors introduce a particular ACF model and examine the errors associated with estimating its parameters from image measurements. They also conduct an analysis of two important classes of errors: imaging system errors and estimation errors. They found that as the proportion of raw signal used to create the image increases the effects of system errors rapidly degrade ACF performance. This has implications for operationally produced image products that do not use an autofocusing procedure. They also found that the agreement between theoretical and observed estimation errors is quite good, so that the scale of these errors may be accurately estimated during a spatial analysis of the image data. They found some residual bias that may be attributed to both the use of the ACF itself and to the way the ACF model was constructed.;Uncertainties in the estimation of ACF-based texture in synthetic aperture radar image data;Not health related;Not health related;0
"T. Hai Nguyen; E. Prifti; N. Sokolovska; J. -D. Zucker";2019;Information from metagenomic data from human microbiome may improve diagnosis and prognosis for multiple human diseases. However, to achieve a prediction based on bacterial abundance information remains a challenge. Indeed, the number of features being much higher than the number of samples, we face difficulties related to high dimensional data processing, as well as overfitting. In this study, we investigate several convolutional neural network architectures for synthetic images and some experimental techniques to generate and train these synthetic images. We also explore supervised learning for visualizing high dimensional data that use data on genus, species and higher taxonomic level information. In addition, some dimensionality reduction approaches are examined on very high dimensional data such as gene families abundance. We evaluated our approach on six different metagenomic datasets including five types of diseases with more than 1000 samples. Our method displays promising results and can be used in different omics data settings, including integrative ones.;Disease Prediction Using Synthetic Image Representations of Metagenomic Data and Convolutional Neural Networks;health related;health related;0
"H. Tang; K. Jia";2023;Deep learning in computer vision has achieved great success with the price of large-scale labeled training data. However, exhaustive data annotation is impracticable for each task of all domains of interest, due to high labor costs and unguaranteed labeling accuracy. Besides, the uncontrollable data collection process produces non-IID training and test data, where undesired duplication may exist. All these nuisances may hinder the verification of typical theories and exposure to new findings. To circumvent them, an alternative is to generate synthetic data via 3D rendering with domain randomization. We in this work push forward along this line by doing profound and extensive research on bare supervised learning and downstream domain adaptation. Specifically, under the well-controlled, IID data setting enabled by 3D rendering, we systematically verify the typical, important learning insights, e.g., shortcut learning, and discover the new laws of various data regimes and network architectures in generalization. We further investigate the effect of image formation factors on generalization, e.g., object scale, material texture, illumination, camera view-point, and background in a 3D scene. Moreover, we use the simulation-to-reality adaptation as a downstream task for comparing the transferability between synthetic and real data when used for pre-training, which demonstrates that synthetic data pre-training is also promising to improve real test results. Lastly, to promote future research, we develop a new large-scale synthetic-to-real benchmark for image classification, termed S2RDA, which provides more significant challenges for transfer from simulation to reality.;A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation;Not health related;Not health related;0
"S. Sánchez; P. R. Marpu; A. Plaza; A. Paz-Gallardo";2015;This work investigates the parallel implementation of target decomposition and unsupervised classification algorithms for polarimetric synthetic aperture radar (POLSAR) data processing. The algorithms are implemented using two different parallel programming models: 1) clusters of CPUs, using message passing interface (MPI), and 2) commodity graphic processing units (GPUs), using the compute device unified architecture (CUDA). POLSAR data processing generally involves a large amount of computations as the full polarimetric information needs to be decomposed and analyzed. Our experiments reveal that GPU architectures provide a good framework for massive parallelization of POLSAR data processing. For instance, it is found that a single GPU can be more efficient than a cluster of 128 nodes with speedups of more than $100 \times $ in comparison with the single processor times. The proposed implementation makes the best use of low-level features in the GPU architecture such as shared memories, while also providing coalesced accesses to memory in order to achieve maximum performance.;Parallel Implementation of Polarimetric Synthetic Aperture Radar Data Processing for Unsupervised Classification Using the Complex Wishart Classifier;Not health related;Not health related;0
"J. Jeong; H. Song; J. Park; P. Resende; B. Bradaï; K. Jo";2022;In the autonomous car, perception with point cloud semantic segmentation helps obtain a wealth of information about the surrounding road environment. Despite the massive progress of recent researches, the existing machine learning networks are still insufficient for online applications of autonomous driving due to too subdivided classes, the lack of training data, and their heavy computing load. To solve these problems, we propose a fast and lite point cloud semantic segmentation network for autonomous driving, which utilizes LiDAR synthetic data to improve the performance by transfer learning. First, we modify the labeling classes and generate the LiDAR synthetic data-set for additional training to alleviate the lack of training data of the realistic data-set. Then, to lower the computing load, we adopt a projection-based method and apply a lightweight segmentation network to projected LiDAR images, which has drastically reduced computing. Finally, we verified and evaluated the proposed network in this paper through experiments. Experimental results show that the proposed network can perform the three-dimensional point cloud semantic segmentation in an online way, in which the inference speed overwhelms the existing algorithms.;Fast and Lite Point Cloud Semantic Segmentation for Autonomous Driving Utilizing LiDAR Synthetic Training Data;Not health related;Not health related;0
"P. Neumann; G. Muncill";2004;This paper presents the results obtained using the adaptive simulated annealing (ASA) algorithm to invert the test cases from the Geoacoustic Inversion Techniques Workshop held in May 2001. The ASA algorithm was chosen for use in our inversion software for its speed and robustness when searching the geoacoustic parameter solution space to minimize the difference between the observed and the modeled transmission loss (TL). Earlier work has shown that the ASA algorithm is approximately 15 times faster than a modified Boltzmann annealing algorithm, used in prior versions of our TL inversion software, with comparable fits to the measured data. Results are shown for the synthetic test cases, 0 through 3, and for the measured data cases, 4 and 5. The inversion results from the synthetic test cases showed that subtle differences between range-dependent acoustic model version 1.5, used to generate the test cases, and parabolic equation (PE) 5.0, used as the propagation loss model for the inversion, were significant enough to result in the inversion algorithm finding a geoacoustic environment that produced a better match to the synthetic data than the true environment. The measured data cases resulted in better fits using ASTRAL automated signal excess prediction system TL 5.0 than using the more sophisticated PE 5.0 as a result of the inherent range averaging present in the ASTRAL 5.0 predictions.;Using the adaptive simulated annealing algorithm to estimate ocean-bottom geoacoustic properties from measured and synthetic transmission loss data;Not health related;Not health related;0
"Z. Wang; S. H. Elyas; R. J. Thomas";2016;In this paper we review and expand our previous work on developing random topology networks as a way to generate synthetic power grid data. We first present an algorithm that is able to generate the small-world “eletric” topology appropriate for a transmission network. Next we re-examine the definitions of the proposed numerical measure called “Bus Type Entropy”, to characterize the correlated assignment of generation, load, and connection buses in a real power grid, using the IEEE test cases and newly obtained realistic grid data. In order to study the scaling property of bus type assignment versus network work, we revise the entropy definition for better statistical property and improved numerical stability. We then develop an more efficient search algorithm for the best bus type assignments with the help of the derived scaling function. Finally we introduce some most recent work to generate other electrical parameters such as generation capacities and load settings.;Generating synthetic electric power system data with accurate electric topology and parameters;Not health related;Not health related;0
"M. Cinquini; F. Giannotti; R. Guidotti";2021;Synthetic data generation has been widely adopted in software testing, data privacy, imbalanced learning, artificial intelligence explanation, etc. In all such contexts, it is important to generate plausible data samples. A common assumption of approaches widely used for data generation is the independence of the features. However, typically, the variables of a dataset de-pend on one another, and these dependencies are not considered in data generation leading to the creation of implausible records. The main problem is that dependencies among variables are typically unknown. In this paper, we design a synthetic dataset generator for tabular data that is able to discover nonlinear causalities among the variables and use them at generation time. State-of-the-art methods for nonlinear causal discovery are typically inefficient. We boost them by restricting the causal discovery among the features appearing in the frequent patterns efficiently retrieved by a pattern mining algorithm. To validate our proposal, we design a framework for generating synthetic datasets with known causalities. Wide experimentation on many synthetic datasets and real datasets with known causalities shows the effectiveness of the proposed method.;Boosting Synthetic Data Generation with Effective Nonlinear Causal Discovery;Not health related;Not health related;0
"E. Hatay; J. Ma; H. Sun; J. Fang; Z. Gao; H. Yu";2021;Due to the popularity and mobility of smart phones, phone-related pedestrian distracted behaviors, e.g., Texting, Game Playing, and Phone calls, have caused many traffic fatalities and accidents. As an advanced driver-assistance or autonomous-driving system, computer vision could be used to automatically detect distractions from cameras installed on the vehicle for useful safety intervention. The state-of-the-art method models this problem as a standard supervised learning method with a two-branch Convolutional Neural Network (CNN) followed by a voting on all image frames. In contrast, this paper proposes a new synthetic dataset named SYN-PPDB (448 synchronized video pairs of 53,760 computer game images) for this research problem and models it as a transfer learning problem from synthetic data to real data. A new deep learning model embedded with spatial-temporal feature learning and pose-aware transfer learning is proposed. Experimental results show that we could improve the state-of-the-art overall recognition accuracy from 84.27% to 96.67%.;Learning to Detect Phone-related Pedestrian Distracted Behaviors with Synthetic Data;Not health related;Not health related;0
"J. Ponge; M. Enbergs; M. Schüngel; B. Hellingrath; A. Karch; S. Ludwig";2021;Spatial agent-based simulations of infectious disease epidemics require a high-resolution regional population model. However, only aggregated demographic data is available for most geographic regions. Furthermore, the infectious disease application case can require the fusion of multiple data sources (e.g. census and public health statistics), inducing demand for a modular and extensible modeling approach. In this work we provide a novel sequential sample-free approach to generate synthetic baseline populations for agent-based simulations, combining synthetic reconstruction and combinatorial optimization. We applied the approach to generate a population model for the German state of North Rhine-Westphalia (17.5 million inhabitants) which yielded an average accuracy of around 98% per attribute. The resulting population model is publicly available and has been utilized in multiple simulation-based infectious disease case studies. We suggest that our research can pave the way for more geographically granular synthetic populations to be used in model-driven infectious disease epidemics prediction and prevention.;Generating Synthetic Populations Based On German Census Data;health related;Not health related;1
"T. Wang; Y. Tan; Y. Wang; B. Jin; A. Monti; A. L. Sangiovanni-Vincentelli";2022;The difficulty in acquiring fault label data is a major obstacle to the application of data-driven fault isolation in DC microgrids. To remove this barrier, this paper introduces an approach of generating synthetic data with the line currents measured during normal operation as the substitute for fault label data in training an ensemble model, which is aimed to isolate line short-circuit faults in DC microgrids. Based on a high-frequency model of DC microgrids, we prove that the line currents during the closing of unloaded DC lines have similar high-frequency features as those during a line short-circuit fault. On this basis, the synthetic data are obtained through performing discrete wavelet packet transform on the line currents measured during the circuit breaker operation. With normal operating data and synthetic data, an ensemble model is trained as the fault classifier. In the verification tests of different fault scenarios in a three-terminal DC microgrid model, the detection rates of the proposed ensemble model are over 90% while the false positive rates are below 0.5%. These results prove the effectiveness of using synthetic data as the fault labels in the ensemble learning-based fault isolation in DC microgrids.;Synthetic Data in DC Microgrids: Label Creation for Ensemble Learning for Fault Isolation;Not health related;Not health related;0
"S. Ickin; K. Vandikas; F. Moradi; J. Taghia; W. Hu";2020;Quality of Experience (QoE) models need good generalization that necessitates sufficient amount of user-labeled datasets associated with measurements related to underlying QoE factors. However, obtaining QoE datasets is often costly, since they are preferably collected from many subjects with diverse background, and eventually dataset sizes and representations are limited. Models can be improved by sharing and merging those collected local datasets, however regulations such as GDPR make data sharing difficult, as those local user datasets might contain sensitive information about the subjects. A privacy-preserving machine learning approach such as Federated Learning (FL) is a potential candidate that enables sharing of QoE data models between collaborators without exposing ground truth, but only by means of sharing the securely aggregated form of extracted model parameters. While FL can enable a seamless QoE model management, if collaborators do not have the same level of data quality, more iterations of information sharing over a communication channel might be necessary for models to reach an acceptable accuracy. In this paper, we present an ensemble based Bayesian synthetic data generation method for FL, LOO (Leave-One-Out), which reduces the training time by 30% and the network footprint in the communication channel by 60%.;Ensemble-based Synthetic Data Synthesis for Federated QoE Modeling;Not health related;Not health related;0
"S. Abdul-Rauf; H. Schwenk; P. Lambert; M. Nawaz";2016;In this paper, we present information retrieval as a powerful tool for addressing an imperative problem in the field of statistical machine translation, i.e., improving translation quality when not enough parallel corpora are available. We devise a framework, which uses information retrieval to create a synthetic corpus from the easily available monolingual corpora. We propose an improved unsupervised training approach with a data selection mechanism, which selects only the most appropriate sentences, thus reducing the amount of data, which is less related to the domain in the additional bitext. We also introduce a new method to choose sentences based on their relative similarity/difference from the query sentence. Using the synthetic corpus created by our method, we are able to improve state-of-the-art statistical machine translation systems.;Empirical Use of Information Retrieval to Build Synthetic Data for SMT Domain Adaptation;Not health related;Not health related;0
"N. Rossenbach; M. Zeineldeen; B. Hilmes; R. Schlüter; H. Ney";2021;Recent publications on automatic-speech-recognition (ASR) have a strong focus on attention encoder-decoder (AED) architectures which tend to suffer from over-fitting in low resource scenarios. One solution to tackle this issue is to generate synthetic data with a trained text-to-speech system (TTS) if additional text is available. This was successfully applied in many publications with AED systems, but only very limited in the context of other ASR architectures. We investigate the effect of varying pre-processing, the speaker embedding and input encoding of the TTS system w.r.t. the effectiveness of the synthesized data for AED-ASR training. Additionally, we also consider internal language model subtraction for the first time, resulting in up to 38% relative improvement. We compare the AED results to a state-of-the-art hybrid ASR system, a monophone based system using connectionist-temporal-classification (CTC) and a monotonic transducer based system. We show that for the later systems the addition of synthetic data has no relevant effect, but they still outperform the AED systems on LibriSpeech-100h. We achieve a final word-error-rate of 3.3%/10.0% with a hybrid system on the clean/noisy test-sets, surpassing any previous state-of-the-art systems on Librispeech-100h that do not include unlabeled audio data.;Comparing the Benefit of Synthetic Training Data for Various Automatic Speech Recognition Architectures;Not health related;Not health related;0
"V. S. Chundawat; A. K. Tarun; M. Mandal; M. Lahoti; P. Narang";2024;Synthetic tabular data generation becomes crucial when real data are limited, expensive to collect, or simply cannot be used due to privacy concerns. However, producing good quality synthetic data is challenging. Several probabilistic, statistical, generative adversarial networks and variational autoencoder-based approaches have been presented for synthetic tabular data generation. Once generated, evaluating the quality of the synthetic data is quite challenging. Some of the traditional metrics have been used in the literature, but there is lack of a common, robust, and single metric. This makes it difficult to properly compare the effectiveness of different synthetic tabular data generation methods. In this article, we propose a new universal metric, TabSynDex, for the robust evaluation of synthetic data. The proposed metric assesses the similarity of synthetic data with real data through different component scores, which evaluate the characteristics that are desirable for “high-quality” synthetic data. Being a single score metric and having an implicit bound, TabSynDex can also be used to observe and evaluate the training of neural network-based approaches. This would help in obtaining insights that was not possible earlier. We present several baseline models for comparative analysis of the proposed evaluation metric with existing generative models. We also give a comparative analysis between TabSynDex and existing synthetic tabular data evaluation metrics. This shows the effectiveness and universality of our metric over the existing metrics.;A Universal Metric for Robust Evaluation of Synthetic Tabular Data;Not health related;Not health related;0
"S. Gujar; T. Shah; D. Honawale; V. Bhosale; F. Khan; D. Verma; R. Ranjan";2022;Data-driven models function admirably in solving real-world problems. However, obtaining relevant data is difficult. Also, sometimes more diverse data is needed to identify the limitations of trained Machine Learning models. Creating such data samples based on earlier known metadata is a common practice. However, this process can induce bias in the dataset unknowingly. Generative Adversarial Networks (GAN) based data generation models generate more data based on initial data distribution. Thus, data generation models may reflect bias in the generated synthetic data. In this study, the authors have proposed an interactive synthetic data generation Graphical User Interface (GUI) tool. The tool is equipped with Bias detection and mitigation algorithms which will notify users about the pre-existing bias and provide methods to mitigate it. Similarly, this tool can be used to evaluate synthetic data generated using GAN-based models against fairness metrics. The authors have found that Learning Fair Representation (LFR) bias mitigation method has performed 62% & 17.5% better than Prejudice remover and Disparate impact remover for German Credit & Adult original datasets. These results were concluded based on bias detection metrics such as Statistical Parity Difference (SPD) and Disparate Impact (DI). The proposed data generation tool used with LFR method can reduced SPD metric by 93% on original German Credit data. The authors conclude that both original and synthetic datasets had a bias. Therefore, the fairness level of any dataset should be checked vigilantly.;GenEthos: A Synthetic Data Generation System With Bias Detection And Mitigation;Not health related;Not health related;0
"K. K. Thiel; F. Naumann; E. Jundt; S. Günnemann; G. Klinker";2022;Augmented reality applications use object tracking to estimate the pose of a camera and to superimpose virtual content onto the observed object. Today, a number of tracking systems are available, ready to be used in industrial applications. However, such systems are hard to handle for a service maintenance engineer, due to obscure configuration procedures. In this article, we investigate options towards replacing the manual configuration process with a machine learning approach based on automatically synthesized data. We present an automated process of creating object tracker facilities exclusively from synthetic data. The data is highly enhanced to train a convolutional neural network, while still being able to receive reliable and robust results during real world applications only from simple RGB cameras. Comparison against related work using the LINEMOD dataset showed that we are able to outperform similar approaches. For our intended industrial applications with high accuracy demands, its performance is still lower than common object tracking methods with manual configuration. Yet, it can greatly support those as an add-on during initialization, due to its higher reliability.;C.DOT - Convolutional Deep Object Tracker for Augmented Reality Based Purely on Synthetic Data;Not health related;Not health related;0
"S. M. Hur Rizvi; S. K. Sadanandan; A. K. Srivastava";2020;Voltage dependency of load greatly impacts power system studies (e.g. voltage stability). The constant power load model generally used for analysis is inadequate and conservative to accurately represent voltage dependency of load. Estimation of these parameters in an online manner can enhance the accuracy of power system analysis. Phasor Measurement Units (PMUs) provide high resolution power grid data for improved real-time wide-area system monitoring and control. These high-resolution PMU data have made real-time estimation of ZIP parameters possible, but PMU data may hold multiple types of anomalies arising in the physical system or the cyber system (measurements and communication networks). Increasing the grid situational awareness for noisy and bad/anomalous measurement data has become significant for data-driven PMU applications. Many algorithms developed for anomaly detection have been only validated with simulated data and not with realistic synchophasor data. Approach to actual field data is really challenging due to confidentiality and security grounds. This paper presents a method for generating realistic synchrophasor data for the given synthetic network as well as online adaptive ZIP parameter estimation algorithm. The developed algorithm, firstly uses en-semble based anomaly detector (E-BAD) to flag all bad data present in PMU data streams and then an adaptive algorithm for estimation window formation and, a novel estimation method Least square assisted with sensitivity based variable elimination (LSVE-ZIP) are used for ZIP parameter estimation. Moreover, a confidence factor computation framework is proposed to give insight regarding accuracy of estimated parameters. Parameter estimation performance is validated using numerous PSS/E and RTDS generated scenarios with noise and bad data. These set of synchrophasor data will be made available publicly later for other researchers.;Real-time ZIP Load Parameter Tracking using Adaptive Window and Variable Elimination with Realistic Synthetic Synchrophasor Data;Not health related;Not health related;0
"E. E. Berkson; J. D. VanCor; S. Esposito; G. Chern; M. Pritt";2019;The low/no-shot problem refers to a lack of available data for training deep learning algorithms. In remote sensing, complete image data sets are rare and do not always include the targets of interest. We propose a method to rapidly generate highfidelity synthetic satellite imagery featuring targets of interest over a range of solar illuminations and platform geometries. Specifically, we used the Digital Imaging and Remote Sensing Image Generation model and a custom image simulator to produce synthetic imagery of C130 aircraft in place of real Worldview-3 imagery. Our synthetic imagery was supplemented with real Worldview-3 images to test the efficacy of training deep learning algorithms with synthetic data. We deliberately chose a challenging test case of distinguishing C130s from other aircraft, or neither. Results show a negligible improvement in automatic target classification when synthetic data is supplemented with a small amount of real imagery. However, training with synthetic data alone only achieves F1-scores in line with a random classifier, suggesting that there is still significant domain mismatch between the real and synthetic datasets.;Synthetic Data Generation to Mitigate the Low/No-Shot Problem in Machine Learning;Not health related;Not health related;0
"M. Zare; J. Wojtusiak";2018;Patient data are regarded as highly sensitive and protected information by federal, state and local policies that make it available to only those who have been given access to Protected Health Information (PHI). In many applications, the access to PHI and real patient data can be substituted with generated realistic synthetic data used instead of real patient data. While methods exist that can generate synthetic data, it is unclear how to evaluate synthetic data quality. The objective of this paper is to present investigation of a new method for statistically testing the quality of synthetic patient data. Weighted Itemsets Error (WIE) measure compares frequent itemsets in the synthetic data with expected itemsets in real data, thus allowing for evaluating cooccurrence of data items. The derived measure is tested in the context of synthetic data comprising of medical diagnoses. The results demonstrate the effects of parameters that control WIE measure, and indicate that WIE is a simple yet powerful approach for evaluating synthetic datasets.;Weighted Itemsets Error (WIE) Approach for Evaluating Generated Synthetic Patient Data;health related;Not health related;1
"C. -Y. Yang; Z. Jiang; S. -Y. Gu; J. -N. Hwang; J. -H. Yoo";2022;The Alberta Infant Motor Scale (AIMS) is a well-known assessment scheme that evaluates the gross motor development of infants by recording the number of specific poses achieved. With the aid of the image-based pose recognition model, the AIMS evaluation procedure can be shortened and automated, providing early diagnosis or indicator of potential developmental disorder. Due to limited public infant-related datasets, many works use the SMIL-based method to generate synthetic infant images for training. However, this domain mismatch between real and synthetic training samples often leads to performance degradation during inference. In this paper, we present a CNN-based model which takes any infant image as input and predicts the coarse and fine-level pose labels. The model consists of an image branch and a pose branch, which respectively generates the coarse-level logits facilitated by the unsupervised domain adaptation and the 3D keypoints using the HRNet with SMPLify optimization. Then the outputs of these branches will be sent into the hierarchical pose recognition module to estimate the fine-level pose labels. We also collect and label a new AIMS dataset, which co—tains 750 real and 4000 synthetic infants images with AIMS pose labels. Our experimental results show that the proposed method can significantly align the distribution of synthetic and real-world datasets, thus achieving accurate performance on fine-grained infant pose recognition.;Unsupervised Domain Adaptation Learning for Hierarchical Infant Pose Recognition with Synthetic Data;Not health related;Not health related;0
"L. J. A. Jansen; T. Gräupl; N. Mäurer; K. Morioka; C. Schmitt";2023;To accommodate the increasing number of aircraft and the growing demand for a secure data connection in the air, the LDACS A/G has been developed and is on the verge of being standardized. To assist with the standardization process, a flight demonstration was conducted to demonstrate the protocol’s ability to operate successfully in a real-world setting. For this, radio prototypes that implement the LDACS A/G protocol were installed on the ground and aboard an aircraft. To thoroughly test these prototypes, tailored realistic flight data had to be sent, received, and analyzed.In this paper, we present the Finfrastructure software framework, that we use to assess the performance of different radio prototypes. We discuss our considerations for measuring the performance of different radio prototypes. By using a simple IPv6-based interface for each prototype, the framework supports various environments, such as lab tests, simulations, and flight campaigns. We measure, among others, latency, throughput, and link stability. Our results show the different performances of the data link protocol implementations which makes them comparable based on the same metrics. The results obtained from the comparison of different LDACS implementations can be utilized in the standardization process, as they demonstrate that LDACS can meet the requirements of a modern digital data link for aviation.;A Software Framework for Synthetic Aeronautical Data Traffic Generation in Support of LDACS Evaluation Activities;Not health related;Not health related;0
"M. A. Farooq; W. Yao; G. Costache; P. Corcoran";2023;In this research work, we proposed a novel ChildGAN, a pair of GAN networks for generating synthetic boys and girls facial data derived from StyleGAN2. ChildGAN is built by performing smooth domain transfer using transfer learning. It provides photo-realistic, high-quality data samples. A large-scale dataset is rendered with a variety of smart facial transformations: facial expressions, age progression, eye blink effects, head pose, skin and hair color variations, and variable lighting conditions. The dataset comprises more than 300k distinct data samples. Further, the uniqueness and characteristics of the rendered facial features are validated by running different computer vision application tests which include CNN-based child gender classifier, face localization and facial landmarks detection test, identity similarity evaluation using ArcFace, and lastly running eye detection and eye aspect ratio tests. The results demonstrate that synthetic child facial data of high quality offers an alternative to the cost and complexity of collecting a large-scale dataset from real children. The complete dataset along with the trained model are open-sourced on our GitHub website and GitHub page: https://github.com/MAli-Farooq/ChildGAN.;ChildGAN: Large Scale Synthetic Child Facial Data Using Domain Adaptation in StyleGAN;Not health related;Not health related;0
"C. T. MacKay; T. -S. Moh";2021;A picture is worth a thousand words, or if you want it labeled, it's worth about ten cents per bounding box. Data is the fuel that powers modern technologies run by AI engines. High quality data is important to produce accurate machine learning models. Acquiring high quality labeled data however, can be expensive and time consuming. For small companies, academic researchers, or hobbyists, gathering large datasets that are not already publicly available is challenging. This research paper will describe the ability to generate labeled image data synthetically which can be used in supervised learning for object detection. This paper describes a system using 3D modeling software in conjunction with Generative Adversarial Networks and image augmentation that can create a diverse dataset of images containing objects with bounding boxes and labels. The result of this effort is an accurate object detector in an environment of aerial surveillance with no cost to the end user.;Learning for Free: Object Detectors Trained on Synthetic Data;Not health related;Not health related;0
"M. Hittmeir; R. Mayer; A. Ekelhart";2022;The use of synthetic data is a widely acknowledged privacy-preserving measure that reduces identity and attribute disclosure risks in micro-data. The idea is to learn the statistical properties of an original dataset, store this information in a model, and then use this model to generate artificial samples and build a synthetic dataset that resembles the original. One of the many different approaches of synthetization tools relies on describing the original dataset by using a Bayesian network. This method is implemented in the open-source tool DataSynthesizer and has proven particularly suitable for datasets with a small to moderate number of attributes. In this paper, we will substitute the greedy algorithm used for learning the Bayesian network by a substantially faster genetic algorithm. In addition, our goal is to protect particularly sensitive attributes by decreasing specific correlations in the synthetic data that may reveal personal information. We will thus show how to customize the network structures for specific machine learning tasks. Our experiments demonstrate that this technique allows to further decrease the disclosure risks and, hence, add to the applicability of synthetic data as technique for privacy preservation.;Efficient Bayesian Network Construction for Increased Privacy on Synthetic Data;Not health related;Not health related;0
"L. Piano; F. G. Pratticò; A. Sebastian Russo; L. Lanari; L. Morra; F. Lamberti";2023;Instance-level object re-identification is a fundamental computer vision task, with applications from image retrieval to intelligent monitoring and fraud detection. In this work, we propose the novel task of damaged object re-identification, which aims at distinguishing changes in visual appearance due to deformations or missing parts from subtle intra-class variations. To explore this task, we leverage the power of computer-generated imagery to create, in a semi-automatic fashion, high-quality synthetic images of the same bike before and after a damage occurs. The resulting dataset, Bent & Broken Bicycles (BB-Bicycles), contains 39,200 images and 2,800 unique bike instances spanning 20 different bike models. As a baseline for this task, we propose TransReI3D, a multi-task, transformer-based deep network unifying damage detection (framed as a multi-label classification task) with object re-identification. The BBBicycles dataset is available at https://tinyurl.com/37tepf7m;Bent & Broken Bicycles: Leveraging synthetic data for damaged object re-identification;Not health related;Not health related;0
"M. Mounir; P. Karsmakers; T. v. Waterschoot";2021;Detecting the onset of notes in music excerpts is a fundamental problem in many music signal processing tasks, including analysis, synthesis, and information retrieval. When addressing the note onset detection (NOD) problem using a data-driven methodology, a major challenge is the availability and quality of labeled datasets used for both model training/tuning and evaluation. As most of the available datasets are manually annotated, the amount of annotated music excerpts is limited and the annotation strategy and quality varies across data sets. To counter both problems, in this paper we propose to use semi-synthetic datasets where the music excerpts are mixes of isolated note recordings. The advantage resides in the annotations being automatically generated while mixing the notes, as isolated note onsets are straightforward to detect using a simple energy measure. A semi-synthetic dataset is used in this work for augmenting a real piano dataset when training a convolutional Neural Network (CNN) with three novel model training strategies. Training the CNN on a semi-synthetic dataset and retraining only the CNN classification layers on a real dataset results in higher average F1-score (F1) scores with lower variance.;CNN-based Note Onset Detection using Synthetic Data Augmentation;Not health related;Not health related;0
"P. Stolfi; I. Valentini; M. C. Palumbo; P. Tieri; A. Grignolio; F. Castiglione";2019;Investigation about the mechanisms involved in the onset of type 2 diabetes in absence of familiarity is the focus of a research project which has led to the development of a computational model that recapitulates the aetiology of the disease. The model simulates the metabolic and immunological alterations related to type-2 diabetes associated to several clinical, physiological and behavioural characteristics of representative virtual patients. In this study, the results of 46170 simulations corresponding to the same number of virtual subjects, experiencing different lifestyle conditions, are analysed for the construction of a statistical model able to recapitulate the simulated dynamics. The resulting machine learning model adequately predicts the synthetic data and can therefore be used as a computationally-cheaper version of the detailed mathematical model, ready to be implemented on mobile devices to allow self assessment by informed and aware individuals.;Potential predictors of type-2 diabetes risk: machine learning, synthetic data and wearable health devices;health related;health related;1
"K. Baek; H. Shim";2022;Transfer learning for GANs successfully improves generation performance under low-shot regimes. However, existing studies show that the pretrained model using a single benchmark dataset is not generalized to various target datasets. More importantly, the pretrained model can be vulnerable to copyright or privacy risks as membership inference attack advances. To resolve both issues, we propose an effective and unbiased data synthesizer, namely Primitives - PS, inspired by the generic characteristics of natural images. Specifically, we utilize 1) the generic statistics on the frequency magnitude spectrum, 2) the elementary shape (i.e., image composition via elementary shapes) for representing the structure information, and 3) the existence of saliency as prior. Since our synthesizer only considers the generic properties of natural images, the single model pretrained on our dataset can be consistently transferred to various target datasets, and even outperforms the previous methods pretrained with the natural images in terms of Fréchet inception distance. Extensive analysis, ablation study, and evaluations demonstrate that each component of our data synthesizer is effective, and provide insights on the desirable nature of the pretrained model for the transferability of GANs.;Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data;Not health related;Not health related;0
"E. R. B. Calma; M. C. Pacis";2021;Voltage stability refers to the ability of an electrical network to sustain the steady state voltage at each bus upon the subjection of the network to a sudden change or disturbance. Fast Voltage Stability Index (FVSI) and Line Stability Index (LMN) are indices used to determine the voltage stability of a line or component within the electrical network. This research focuses on the voltage stability analysis of Power Transmission networks with Distributed Generation. The FVSI and LMN will be derived from Artificial Neural Network (ANN) using synthesized Phasor Measurement Unit (PMU) data placed optimally across the network. The algorithm used for determining the optimal PMU placement was Mixed Integer Linear Programming and its data was synthesized from Newton Raphson Load Flow. Mixed Integer Linear Programming for Optimal PMU placement and Newton Raphson Load Flow were conducted using MATLAB programming. The ANN training and simulation is conducted using the nntool MATLAB toolbox. The test cases for this study are the IEEE 14, 30, and 57 bus systems. The developed algorithm yielded accurate results upon validation using Mean Squared Error (MSE) and Regression Analysis.;Artificial Neural Network-based Voltage Stability Analysis of Power Transmission Networks with Distributed Generation using Phasor Measurement Unit Synthetic Data;Not health related;Not health related;0
"G. Del Grosso; G. Pichler; P. Piantanida";2021;Power consumption data isvery useful as it allows to optimize power grids, detect anomalies and prevent failures, on top of being useful for diverse research purposes. However, the use of power consumption dataraises significant privacy concerns, as this data usually belongs to clients of a power company. As a solution, we propose a method to generate synthetic power consumption samples that faithfully imitate the originals, but are detached from the clients and their identities. Our method is based on Generative Adversarial Networks (GANs). Our contribution is twofold. First, we focus on the quality of the generated data, which is not a trivial task as no standard evaluation methodsare available. Then, we study the privacy guarantees provided to members of the training set of our neuralnetwork. As a minimum. requirement for privacy, we demand our neural network to be robust to membership inference attacks, as theseprovide a gateway for further attacks in addition to presenting a privacy threat on their own. We findthat there is acompromise to be made between the privacy and the performance provided the algorithm.;Privacy-Preserving Synthetic Smart Meters Data;Not health related;Not health related;0
"R. Shi; P. Steenkiste; M. Veloso";2018;Real passenger data available to city planners are usually incomplete. The goal of our work is to generate synthetic passenger data using a novel methodology that leverages joint traffic-passenger modeling and simulation on a city scale. A demonstration of such an idea in generating synthetic bus passenger data was implemented. Specifically, we 1) learned a bus passenger demand model from indirect people-mobility data to generate bus passenger demand samples, and we 2) developed a bus passenger behavior model, which runs jointly with a traffic simulator (SUMO), to generate synthetic bus passenger data. We applied the proposed methodology for a case study of Porto city, Portugal. The synthetic bus passenger data presents significant similarity in terms of spatial-temporal distributions to the real-world bus passenger data collected by the bus automated fare collection (AFC) system in Porto.;Generating Synthetic Passenger Data through Joint Traffic-Passenger Modeling and Simulation;Not health related;Not health related;0
"N. Watthanasutthi; V. Muangsin";2016;Population synthesis is a process to create data records of individual persons and households with associated attributes that closely resemble the real population. It is the basis of microsimulation models for various applications such as urban planning, crime modeling and epidemiology. This work aims to create synthetic Thai population at the provincial scale. Our synthetic population generator is based on the synthetic reconstruction method, which is most suitable where only aggregate census data are available, as in Thailand. With available census tabulations from various government agencies, the generator is configured to combine 16 tabulation data at individual and household levels using conditional probabilities. The order of conditional probabilities is designed according to dependencies between the attributes and the difference in resolutions of the data from multiple sources. The main contribution of this work is the method to generate complex household types. Many family related attributes are used to create family relationships among individuals. Then, families and individuals are assigned into households according to household statistics. The generator is evaluated by creating synthetic population of Phitsanulok, a province with 835,555 individuals and 296,807 households localized in 18 municipality areas of 9 districts. The aggregated tabulations of the synthetic population are compared to the original ones. The results show that the distributions of their aggregated attributes are very close to the source data. Therefore, the synthetic population is a good approximate of the real population.;Generating synthetic population at individual and household levels with aggregate data;Not health related;Not health related;0
"A. Lerro; M. Battipede; G. Sangaletti; D. Barbera; S. Antinori";2020;The work deals with safety assessment for the innovative modular digital air data system developed under the MIDAS project, funded in the Clean Sky 2 frame. The MIDAS's main innovation is based on estimation of aerodynamic angles (angle of attack and sideslip) using synthetic sensors instead of classical vanes (or sensors), whereas pressure and temperature are directly measured with Pitot and temperature probes. As known, air data systems are safety-critical ones that are usually redundant in order to be compliant with the safety requirements of airworthiness regulations. In order to meet safety objectives, a single (simplex) ADS is studied in order evaluate its own safety characteristics starting from reliability known data for any air data system items. These results are then collected in order to provide necessary information to the aircraft integrator that is in charge to define the system redundancy. The present paper introduces the common aeronautical procedures for system safety assessment to be applied to a safety critical system based on synthetic sensors for the first time. Standard procedures are applied to the MIDAS project in order to evaluate its safety characteristics with respect to safety objectives defined at aircraft level in accordance with the aircraft integrator. Results shown in this work provide information on criticality allocated to any air data functionalities that will be used to study the overall ADS redundancy.;Safety Assessment for Certified Air Data Systems based on Synthetic Sensors;Not health related;Not health related;0
"G. Goodman; N. Bourbakis";2020;Cardiovascular Disease (CVD) is one of the most detrimental health issues the world experiences each year. Approximately 647,000 people die from CVD each year in America according to the Centers for Disease Control and Prevention. In other words, one person every 37 seconds. Globally, 17.9 million people die from CVD each year as reported by the World Health Organization. Financially, America spends approximately $219 billion each year. The American Heart Association estimates the financial cost to reach $1.1 trillion by 2035. These metrics show the need to continue research in aiding individuals affected with CVD. In this paper, we use the theory of our previous work, a wearable ultrasound vest to create a real-time near 3D model of the heart, to create the beginnings of a heart state prediction system. That is, we create a synthetic dataset using the ranges of normal and abnormal heart chamber volumes to calculate external surface areas and generate data via statistical bootstrapping. Then, feed this system into a Machine Learning algorithm called a Constrained State Preserved Extreme Learning Machine (CSPELM). Our results show that we can differentiate between abnormal data (Atrial Fibrillation, Chronic Mitral Regurgitation, and Post-Myocardial Infarction) from normal data, where higher percentages for abnormal and low percentages for normal is the goal, with CSPELM predictions of 60.28-88.33% to 33.61%, respectively.;Predicting the Change in State of the Human Heart Based on Synthetic Heart Chamber Volume Data;health related;Not health related;1
"M. Lenatti; A. Paglialonga; V. Orani; M. Ferretti; M. Mongelli";2023;The aim of this study is to apply and characterize eXplainable AI (XAI) to assess the quality of synthetic health data generated using a data augmentation algorithm. In this exploratory study, several synthetic datasets are generated using various configurations of a conditional Generative Adversarial Network (GAN) from a set of 156 observations related to adult hearing screening. A rule-based native XAI algorithm, the Logic Learning Machine, is used in combination with conventional utility metrics. The classification performance in different conditions is assessed: models trained and tested on synthetic data, models trained on synthetic data and tested on real data, and models trained on real data and tested on synthetic data. The rules extracted from real and synthetic data are then compared using a rule similarity metric. The results indicate that XAI may be used to assess the quality of synthetic data by (i) the analysis of classification performance and (ii) the analysis of the rules extracted on real and synthetic data (number, covering, structure, cut-off values, and similarity). These results suggest that XAI can be used in an original way to assess synthetic health data and extract knowledge about the mechanisms underlying the generated data.;Characterization of Synthetic Health Data Using Rule-Based Artificial Intelligence Models;health related;health related;1
"V. Sapre; S. Kalambur; D. Sitaram; R. Bastian";2018;Transportation in urban cities is a complicated issue that is worsening with the over-population of major cities around the world. Being part of the second most populous country in the world, Indian cities face an exaggerated form of the issue. To solve this issue, major advancements in urban transportation are required. The mobility of a city must be understood thoroughly, to make changes that will have the most prominent effect. Building infrastructure to analyze traffic is very expensive and thus not an option. Synthetically generated data is one of the solutions in this case. A mobility model of the city is needed for this purpose. Traffic demand needed to create the model was collected by using the raw GPS traces hosted by OpenStreetMap. Using the model, traffic demand is scaled up to mimic vehicle population of the city. Running this newly generated traffic demand through SUMO (Simulator for Urban Mobility) gives us highly detailed data on the microscopic behaviour of traffic.;Synthetic Generation of Traffic Data for Urban Mobility;Not health related;Not health related;0
"P. Acharya; D. Lohn; V. Ross; M. Ha; A. Rich; E. Sayyad; T. Höllerer";2021;Synthetic data is highly useful for training machine learning systems performing image-based 3D reconstruction, as synthetic data has applications in both extending existing generalizable datasets and being tailored to train neural networks for specific learning tasks of interest. In this paper, we introduce and utilize a synthetic data generation suite capable of generating data given existing 3D scene models as input. Specifically, we use our tool to generate image sequences for use with Multi-View Stereo (MVS), moving a camera through the virtual space according to user-chosen camera parameters. We evaluate how the given camera parameters and type of 3D environment affect how applicable the generated image sequences are to the MVS task using five pre-trained neural networks on image sequences generated from three different 3D scene datasets. We obtain generated predictions for each combination of parameter value and input image sequence, using standard error metrics to analyze the differences in depth predictions on image sequences across 3D datasets, parameters, and networks. Among other results, we find that camera height and vertical camera viewing angle are the parameters that cause the most variation in depth prediction errors on these image sequences.;Using Synthetic Data Generation to Probe Multi-View Stereo Networks;Not health related;Not health related;0
"M. Moscoso; A. Novikov; G. Papanicolaou; C. Tsogka";2020;In this paper, we consider imaging the reflectivity of scatterers from intensity-only data recorded by a single moving transducer that both emits and receives signals, forming a synthetic aperture. By exploiting frequency illumination diversity, we obtain multiple intensity measurements at each location, from which we determine field cross correlations using an appropriate phase controlled illumination strategy and the inner product polarization identity. The field cross correlations obtained this way do not, however, provide all the missing phase information because they are determined up to a phase that depends on the receiver's location. The main result of this paper is an algorithm with which we recover the field cross correlations up to a single phase that is common to all the data measured over the synthetic aperture, so all the data are synchronized. Thus, we can image coherently with data over all frequencies and measurement locations as if full phase information was recorded.;Synthetic Aperture Imaging With Intensity-Only Data;Not health related;Not health related;0
"S. A. Fineberg; T. L. Casavant; H. J. Siegel";1990;Experimentation aimed at determining the potential benefit of mixed-mode SIMD/MIMD (single instruction, multiple data/multiple instruction, multiple data) parallel architectures is reported. The experimentation is based on timing measurements made on the PASM (Partitionable SIMD/MIMD) system prototype utilizing carefully coded synthetic variations of a well-known algorithm. The synthetic algorithms used to measure and evaluate this system were based on bitonic sorting of sequences stored in the processing elements. This computation was mapped to both the hybrids of the SIMD and MIMD modes. The computations were coded in these four ways and experiments were performed that explore the tradeoffs among them. The results of these experiments are presented and discussed, with special consideration of the effects of the system's architecture. The goal is to obtain implementation-independent analyses of the attributes of mixed-mode parallel processing with respect to the computational characteristics of the application being examined.<>;Experimental analysis of communication/data-conditional aspects of a mixed-mode parallel architecture via synthetic computations;Not health related;health related;0
"M. Zhong; J. Kim; Z. Zheng";2023;While traffic volume data from loop detectors have been the common data source for link flow estimation, the detectors only cover a subset of links. These days, other data sources such as vehicle trajectory data collected from vehicle tracking sensors are also incorporated. However, trajectory data are often sparse in that the observed trajectories only represent a small subset of the whole population, where the exact route sampling rate is unknown and may vary over space and time. In this paper, we develop a method that leverage these two limited data sources to enhance link flow estimation. This study proposes a novel generative modelling framework, where we formulate a vehicle’s link-to-link movements as a sequential decision-making problem using the Markov Decision Process framework. We propose an Inverse Reinforcement Learning-based method, based on which synthetic population vehicle trajectories can be generated to estimate link flows across the whole network. The proposed method ensures the generated population vehicle trajectories are consistent with the observed traffic volume and trajectory data. The proposed generative modelling framework is compared to two existing methods in a synthetic road network and validated in a real road network.;Estimating Link Flows in Road Networks With Synthetic Trajectory Data Generation: Inverse Reinforcement Learning Approach;Not health related;Not health related;0
"A. Altomare; E. Cesario";2015;The success of Cloud Computing and the resulting expansion of large data centers result in a huge rise of electrical power consumption by hardware facilities. Consolidation of virtual machines (VM) is one of the key strategies used to reduce the energy consumed by Cloud servers. Nevertheless, the effectiveness of a consolidation strategy strongly depends on the forecast of the needs of the VM resources. This paper describes the experimental evaluation of a system for energy-aware allocation of virtual machines, driven by predictive data mining models. In particular, migrations are driven by the forecast of the future computational needs (CPU, RAM) of each virtual machine, in order to efficiently allocate those on the available servers. Experimental results, performed both on a real Cloud and synthetic data, show encouraging benefits in terms of energy saving.;Predictive Models for Energy-Efficient Clouds: An Analysis on Real-Life and Synthetic Data;Not health related;Not health related;0
"J. Barros; L. Oliveira";2021;Current state-of-the-art in speed measurement technologies includes magnetic inductive loop detectors, Doppler radar, infrared sensors, and laser sensors. Many of these systems rely on intrusive methods that require intricate installation and maintenance processes that hinder traffic while leading to high acquisition and maintenance costs. Speed measurement from monocular videos appears as an alternative in this context. However, most of these systems present as a drawback the requirement of camera calibration - a fundamental step to convert the vehicle speed from pixels per frame to some real-world unit of measurement (e.g. km/h). Considering that, we propose a speed measurement system based on monocular cameras with no need for calibration. Our proposed system was trained from a synthetic data set containing 12,290 instances of vehicle speeds. We extract the motion information of the vehicles that pass in a specific region of the image by using dense optical flow, using it as input to a regressor based on a customized VGG-16 network. The performance of our method was evaluated over the Luvizon's data set, which contains real-world scenarios with 7,766 vehicle speeds, ground-truthed by a high precision system based on properly calibrated and approved inductive loop detectors. Our proposed system was able to measure 85.4% of the speed instances within an error range of [-3, + 2] km/h, which is ideally defined by the regulatory authorities in several countries. Our proposed system does not rely on any distance measurements in the real world as input, eliminating the need for camera calibration.;Deep speed estimation from synthetic and monocular data;Not health related;Not health related;0
"Y. Wang; X. Tang; J. Fan; G. Xiao";2020;Instance segmentation of scanning electron microscope images provides useful information for quantitative analysis of particle morphometry and distribution that contributes to various biomedical research such as the phenotyping of drug delivery systems. Compared to the conventional segmentation methodologies, the learning-based approaches stand out, benefiting from the prosperous development of artificial intelligence. However, most of the current learning-based segmentation methods require sufficient manually annotated training data, which is considered to be laborious. To alleviate this problem, we present a novel weakly supervised framework for instance segmentation on scanning electron microscope images. In the proposed framework, only one instance from each raw image is manually labeled to generate a synthesized dataset, which will be further used to select the training set. With the weakly annotated training dataset, the instance segmentation network is trained and applied to segment the particles of raw testing images. Based on our experimental results, the trained network gains 75% recall and 74% average precision on the tested images, which is seen as a reasonable performance considering the data complexity in our research. The overall experiments demonstrate that the proposed weakly supervised framework is able to provide an efficient solution to the instance segmentation of biomedical images.;Weakly Supervised Instance Segmentation of SEM Image via Synthetic Data;health related;Not health related;1
"K. Lee; D. Moloney";2017;Stereo vision is a very active field in the realm of computer vision and in recent years Convolutional Neural Networks (CNNs) have proven to be very competitive against the state-of-the-art. However the performance of these networks are limited by the quality of the data that is used when training the CNNs. Data acquisition of high quality labelled images is a time-consuming and expensive process. By exploiting the power of modern-day powerful GPUs, we present a synthetic dataset with fully rectified stereo image pairs and accompanying accurate ground truth information that can be used for training and testing stereo algorithms. We provide validation of the quality of our dataset by performing quantitative experiments that suggest pre-training deep learning algorithms on synthetic data can perform competitively against networks trained on real life data. Testing on the KITTI data-set[1], we found the accuracy performance difference between the real and synthetically trained networks was within a margin of 1.8%. We also illustrate the functionality synthetic data can provide, by conducting a key performance index on a selection of conventional and deep learning stereo algorithms available on embedded platforms and compared them under common metrics. We also focused on power consumption and performance and we were able to achieve a compute the matching cost from a CNN performing inference on an embedded device at 11.9 FPS at 1.2 Watts.;Evaluation of synthetic data for deep learning stereo depth algorithms on embedded platforms;Not health related;Not health related;0
"K. Fang; V. Mugunthan; V. Ramkumar; L. Kagal";2022;There are several shortcomings in current methods of generating synthetic data using Generative Adversarial Networks (GANs). First, they tend to only emulate certain attributes of the original dataset. Second, they do not effectively model unbalanced discrete columns, long tails, or bimodal distributions of continuous columns. Lastly, these approaches often do not consider the potential for information leakage from the generated data. We propose UniformGAN, a GAN with a novel uniform loss function, which addresses these challenges and provides strong privacy guarantees using differential privacy. UniformGAN pre-processes datasets to transpose each column into a uniform distribution. We use a modified Deep Convolutional Generative Adversarial Network (DCGAN) architecture in which we replace ReLU activation functions with the more robust SeLU, which has significantly better performance and better convergence properties, and apply Dense-Sparse-Dense training to our network. We also use differential privacy to add noise to the discriminator during training. Along with UniformGAN, we provide a configurable command-line tool to generate and evaluate synthetic datasets on numerous metrics. It allows users to generate synthetic datasets from CTGAN, TableGAN, UniformGAN, or a custom framework and analyze the resultant datasets. This tool will help data scientists and industry users compare different synthetic dataset generation models and enable them to improve existing methods. We evaluated UniformGAN using multiple datasets, including the Adult, Covertype, and Credit Kaggle datasets, as well as two insurance-related Kaggle datasets. The results show that, when used on datatsets containing a large number of continuous columns, UniformGAN out performs other methods by producing synthetic data with similar correlations and distributions as the original dataset while ensuring privacy.;Overcoming Challenges of Synthetic Data Generation;Not health related;Not health related;0
"S. Thorve; A. Vullikanti; H. S. Mortveit; S. Swarup; M. V. Marathe";2022;Synthetic data is gaining rapid importance in many application domains due to privacy issues, bias, lack, or simply unavailability of real data. It is important that the synthetic data be a good representation of real data for successfully completing the task at hand. Thus, devising characteristic validation metrics is crucial and remains an open problem in many domains (e.g., image generation). Good validation metrics must be able to disentangle the differences between the quality and the variability coverage of the synthetic data. We propose to use a 3-dimensional metric (precision _, recall _, coverage _) to describe the fidelity and diversity of the synthetic data. In this paper, we improve on existing definitions of precision, recall, and coverage to extend to large scale time series data. Traditional nearest neighbor manifolds from the literature are replaced by unsupervised learning techniques such as clustering to deal with large scale fine resolution time series while computing the validation metrics. The proposed metrics are employed to validate synthetic data in the domain of residential energy demand. In addition, we extend these definitions to datasets that have a natural hierarchical structure. We propose a hierarchical data-tree model in which precision, recall, and coverage can be computed at multiple inherent (and/or custom) levels of groupings of the data.;Fidelity and diversity metrics for validating hierarchical synthetic data: Application to residential energy demand;Not health related;Not health related;0
"T. Sutojo; A. Syukur; S. Rustad; G. Fajar Shidik; H. Agus Santoso; P. Purwanto; M. Muljono";2020;Machine learning is widely used in various fields, its ability to study data without having to determine the functional relationships that govern a system. However, small datasets often make it difficult for learning algorithms to make accurate predictions. To overcome this, an oversampling technique is needed. However, for the regression learning model this is not easy to do, because in regression to place synthesis data in a certain feature space must be accompanied by an appropriate target value, usually represented by an estimate function. Therefore in this paper oversampling is done by distributing synthetic data according to the Bus, Star, and Mesh topology, using the SMOTE (Synthetic Minority Over-sampling Technique) method. In the experiment, one of the ISE (Istanbul Stock Exchange) public datasets and one of the CF (Color Filter) real datasets were tested to measure the performance of the proposed oversampling technique. Besides, the results of experiments conducted on the same dataset using the MPV, FCM, and MMPV methods were used as a comparison. The results show that oversampling using the Bus, Star, or Mesh distribution results in better performance than without using oversampling. The ISE dataset tested using the proposed method has an average RMSE value smaller than the MPV, FCM, and MMPV methods. For CF datasets, the proposed method has an average RMSE value smaller than the MPV, FCM, and MMPV methods when the amount of training data is smaller than the amount of testing data.;Investigating the Impact of Synthetic Data Distribution on the Performance of Regression Models to Overcome Small Dataset Problems;Not health related;Not health related;0
"Y. Yannikos; C. Winter; M. Schneider";2012;Increasing amounts of data require improvements in effectiveness and efficiency of forensic tools. If new tools have been developed, they have to be evaluated, e.g. by applying test data. 3LSPG has recently been proposed as a framework for generating synthetic test data by simulating activities of subjects using Markov chains. However, the generation of test data should also be efficient. In this paper, we show how to improve the efficiency of 3LSPG considerably compared to its original proposal. We show how to speed-up the calculation of state transition probabilities in the Markov model of 3LSPG by proposing an algorithm that is much faster and more reliable than the one originally used. The simplex algorithm serves as basis for our algorithm although it is typically used for the different purpose of solving optimization problems. Our algorithm helps to enable the creation of synthetic data for forensic tool testing with 3LSPG in significantly shorter time.;Synthetic Data Creation for Forensic Tool Testing: Improving Performance of the 3LSPG Framework;Not health related;Not health related;0
"P. Ran; S. Chen; M. Serhir; D. Lesselier";2021;Imaging of a subwavelength microstructure made of a periodic grid-like finite set of circular rods is carried out from transient scattered field data in different configurations of sources and receivers. The goal is to identify the position of possibly missing rods. Time reversal is confirmed as a cheap yet efficient first-order diagnostic method even in the demanding context of a subwavelength microstructure. Tools of deep learning, expected to be valid in general circumstances if data acquired in sufficient numbers, are in parallel developed to image the microstructure. To that effect, recurrent neural networks (RNN) and convolutional neural networks (CNN) are both used. Pros and cons of all approaches are illustrated by comprehensive simulations from synthetic data computed via a finite difference time domain method (FD-TD) software carefully tailored to the microstructure model and used also to make the networks learn the microstructures. The analysis is completed from examples of laboratory-controlled data acquired on a microstructure prototype set within a microwave anechoic chamber. These examples confirm the good promises of neural networks even with rather scarce data as exemplified in a forward scattering case—fixed source and receiver antennas face each other and the microstructure is rotated between them.;Imaging of Subwavelength Microstructures by Time Reversal and Neural Networks, From Synthetic to Laboratory-Controlled Data;Not health related;Not health related;0
"R. M. Narayanan; Zhixi Li; S. Papson";2008;Synthetic aperture radar (SAR) and inverse synthetic aperture radar (ISAR) have proven capabilities for non-cooperative target recognition (NCTR) applications. Multiple looks of the same target (at different aspect angles, frequencies, etc.) can be exploited to enhance target recognition by fusing the information from each look. Such fusion can be performed at the raw data level or at the processed image level depending on what is available. At the data level, physics based image fusion techniques can be developed by processing the raw data collected from multiple inverse synthetic aperture radar (ISAR) sensors, even if these individual images are at different resolutions. The technique maps multiple data sets collected by multiple radars with different system parameters on to the same spatial-frequency space. The composite image can be reconstructed using the inverse 2-D Fourier transform over the separated multiple integration areas. An algorithm called the matrix Fourier transform (MFT) is proposed to realize such a complicated integral. At the image level, a persistence framework can be used to enhance target features in large, aspect-varying datasets. The model focuses on cases containing rich aspect data from a single depression angle. The goal is to replace the datapsilas intrinsic viewing geometry dependencies with target-specific dependencies. Both direct mapping functions and cost functions are presented for data transformation. An intensity-only mapping function is realized to illustrate the persistence model in terms of a canonical example, visualization, and classification.;Fusion of multiple-look synthetic aperture radar images at data and image levels;Not health related;Not health related;0
"H. Vietz; T. Rauch; M. Weyrich";2022;Vision applications are becoming increasingly important for product quality surveillance in manufacturing. Training consistently well-performing visual detection algorithms based on convolutional neural networks is very challenging. Typically, there is too much training data for engineers to keep track of possible gaps in it. But even small cases of missing training data e.g. certain viewing angles can lead to trained CNNs that are unable to detect objects, that seem obvious to engineers i.e. cognition gaps. This paper presents how synthetic training data can be created in a targeted manner to close cognitive gaps of a CNN for specific use-cases. The proposed methodology uses 3D rendering to create new image data by variating scene parameters. The created data is used to reveal a cognition gap of a CNN. We show that by using this created synthetic data to train the CNN the cognition gap can be successfully closed. This is evaluated with the well-known AlexNet CNN used as a visual bicycle detector. The bicycle example is used as a stand-in for a geometrically interesting, but simple product, that is manufactured in large and growing amounts.;Synthetic Training Data Generation for Convolutional Neural Networks in Vision Applications;Not health related;Not health related;0
"S. Zhang; C. Song; R. Radkowski";2019;The objective of this contribution is to introduce setforge, a set of software tools for synthetic data generation for convolutional neural network (CNN) training. Our focus is on CNNs for 6-degree-of-freedom pose estimation using RGB-D data. To determine the pose of physical objects in 6-degree-of-freedom is an essential task for many augmented reality applications. The recent years have shown the advent of trainable methods such as CNNs and others. However, those approaches require training data. The tools, this paper introduces, allow one to generate training data from 3D models. They come with plenty of features for random data generation and augmentation, adapting colors, hue, and noise. We contribute these tools as open-source software available on Github. A prototype CNN demonstrates how one can utilize it. An augmented reality demo application also shows its real-time pose estimation performance.;Setforge - Synthetic RGB-D Training Data Generation to Support CNN-Based Pose Estimation for Augmented Reality;Not health related;Not health related;0
"X. Guo; W. Wu; D. Wang; J. Su; H. Su; W. Gan; J. Huang; Q. Yang";2022;In this paper, we take an early step towards video representation learning of human actions with the help of large-scale synthetic videos, particularly for human motion representation enhancement. Specifically, we first introduce an automatic action-related video synthesis pipeline based on a photorealistic video game. A large-scale human action dataset named GATA (GTA Animation Transformed Actions) is then built by the proposed pipeline, which includes 8.1 million action clips spanning over 28K action classes. Based on the presented dataset, we design a contrastive learning framework for human motion representation learning, which shows significant performance improvements on several typical video datasets for action recognition, e.g., Charades, HAA 500 and NTU-RGB. Besides, we further explore a domain adaptation method based on cross-domain positive pairs mining to alleviate the domain gap between synthetic and realistic data. Extensive properties analyses of learned representation are conducted to demonstrate the effectiveness of the proposed dataset for enhancing human motion representation learning.;Learning Video Representations of Human Motion from Synthetic Data;Not health related;Not health related;0
"Jingtian Zhang; Lining Zhang; H. P. H. Shum; Ling Shao";2016;Human action recognition is an important problem in robotic vision. Traditional recognition algorithms usually require the knowledge of view angle, which is not always available in robotic applications such as active vision. In this paper, we propose a new framework to recognize actions with arbitrary views. A main feature of our algorithm is that view-invariance is learned from synthetic 2D and 3D training data using transfer dictionary learning. This guarantees the availability of training data, and removes the hassle of obtaining real world video in specific viewing angles. The result of the process is a dictionary that can project real world 2D video into a view-invariant sparse representation. This facilitates the training of a view-invariant classifier. Experimental results on the IXMAS and N-UCLA datasets show significant improvements over existing algorithms.;Arbitrary view action recognition via transfer dictionary learning on synthetic training data;Not health related;Not health related;0
"A. P_aczek; A. P_uciennik; A. Kotecka-Blicharz; M. Jarzab; D. Mrozek";2020;The use of machine learning has increased over the years, especially in the world of molecular data. Generally, the inference of relationships between features is determined by statistical models. The phenotype (observable clinical characteristics) can result from the expression of the genotype (genetic code) or environmental factors. Molecular datasets have limited information, while supporting clinical data is ambiguous. There are no well-established approaches for combining clinical information with genomic repositories. The genomic tests that are available only use molecular data and give physicians a result which can be integrated clinically. In this article, we present the strategy where clinical data, regardless of its limitations, is combined in one predictive model with molecular features. We predict the risk of malignancy in the thyroid nodules based on the results of fine-needle aspiration biopsy and expression of selected genes. We utilize a Bayesian network (BN) framework to discover relationships between molecular features and assess the impact of added clinical data quality on the performance of the chosen gene set. Bayesian network offering both prognostic and diagnostic perspectives is a perfect non-parametric technique for feature selection, feature extraction, and prediction purposes. We show that certain clinical factors could work as a synthetic feature and provide predictive abilities beyond what genes alone can offer. The experimental results demonstrate a higher performance of predictive models based on molecular and clinical data than when using only molecular data. We also explain why, one should consider the source of clinical data, but be aware of the quality of variables.;Bayesian Assessment of Diagnostic Strategy for a Thyroid Nodule Involving a Combination of Clinical Synthetic Features and Molecular Data;health related;health related;1
"D. García; M. Millán";2011;The massive and extensive use of information technologies has generated a significant increase in both data and applications to manipulate it. These applications should be tested with a enough and representative volume of data. Real-world data is generally not available for issues of confidentiality and privacy. Thus, it is necessary to have synthetic data generators for this purpose. In this paper a free software prototype (PSDG) designed by generating data is described. The prototype is capable of generating and loading synthetic data into an empty database. Shuffling, Nulling out, table-to-table synchronization techniques have been implemented in order to generate the data.;A prototype of synthetic data generator;Not health related;Not health related;0
"S. Muench; D. Bhat; L. Heindel; P. Hantschke; M. Roellig; M. Kaestner";2022;This paper proposes a computationally efficient methodology to predict the damage progression in solder contacts of electronic components using temperature-time curves. For this purpose, two machine learning algorithms, a Multilayer Perceptron and a Long Short-Term Memory network, are trained and compared with respect to their prediction accuracy and the required amount of training data. The training is performed using synthetic, normally distributed data that is realistic for automotive applications. A finite element model of a simple bipolar chip resistor in surface mount technology configuration is used to numerically compute the synthetic data. As a result, both machine learning algorithms show a relevant accuracy for the prediction of accumulated creep strains. With a training data length of 350 hours (12.5 % of the available training data), both models show a constantly good fitting performance of R2 of 0.72 for the Multilayer Perceptron and R2 of 0.87 for the Long Short-Term Memory network. The prediction errors of the accumulated creep strains are less than 10 % with an amount of 350 hours training data and decreases to less than 5 % when using further data. Therefore, both approaches are promising for the lifetime prediction directly on the electronic device.;Performance Assessment of different Machine Learning Algorithm for Life-Time Prediction of Solder Joints based on Synthetic Data;Not health related;Not health related;0
"C. Hecker; D. Riley; M. van der Meijde; F. D. van der Meer";2016;Rock-forming minerals (such as feldspar and quartz) can be identified and quantified from thermal infrared (TIR) laboratory spectroscopy using spectral models. This paper uses synthetic airborne TIR spectra to test whether the hyperspectral Spatially Enhanced Broadband Array Spectrograph System (SEBASS) would theoretically be able to detect quartz and feldspar minerals and quantitatively predict mineral modes in felsic igneous rocks. Data from a previous laboratory study were used to simulate TIR spectra with band locations and noise levels of the SEBASS sensor. The quantitative partial least squares regression (PLSR) models from that study were applied to newly created synthetic SEBASS data, and results were compared with the predictions from the previous study. Predicted compositions based on SEBASS band positions are nearly identical (_ = 0.995) to those based on laboratory resolution. Results are still reliable [prediction errors within 0.4% (absolute)] to the original laboratory PLSR predictions when adding up to 1% noise (about five times the SEBASS noise level) to the synthetic data. Prediction errors rapidly increase when noise levels beyond 1% are used. These results show that SEBASS' spectral resolution, spectral coverage, and signal-to-noise levels are sufficient to quantitatively predict quartz and feldspar amounts, and feldspar compositions with models based on PLSR. Spectral distortions, such as reduced spectral contrast, tilts, and vertical shifts, must be compensated for before these quantitative models are applied. A mean and standard deviation (MASD) normalization is proposed using a set of ground data for compensating systematic errors that are common to all image pixels.;Noise Simulation and Correction in Synthetic Airborne TIR Data for Mineral Quantification;Not health related;Not health related;0
"G. H. M; P. D. Shenoy; V. K. R";2022;The introduction of Electronic Health Records (EHRs) is causing fast transformation in healthcare. EHR contains the patient private information and health history in digital form. Hence, EHR data cannot be shared due to privacy concerns to the Machine Learning(ML) research community, through which we can make the healthcare system smarter and provide quality healthcare services to the patients. As a result, synthetic data is utilised as a backup when real-world data (such as EHR data) is unavailable. Synthetic data can be shared without revealing any private information of the patient. This paper focuses on generating synthetic data from the real dataset. As a use case, we have selected Chronic Kidney Disease(CKD) dataset (real) and generated three datasets – real, synthetic, and a combination of real + synthetic. To test the accuracy of the synthetic data, we ran six supervised machine learning algorithms on these three datasets with all characteristics and reduced features to see if the patient had CKD or not. Supervised ML algorithms on the three datasets are assessed based on the following performance metrics - Confusion Matrix, Accuracy, Recall, Precision, and F1-Score. According to the results, XGBoost surpasses with 100 percent accuracy on all three datasets with full features and a 100 percent accuracy on the mix of real and synthetic datasets with feature reduction.;Performance Analysis of Real and Synthetic Data using Supervised ML Algorithms for Prediction of Chronic Kidney Disease;health related;health related;1
"L. Zhou; X. Song; R. L. Weaver";2020;Ambient noise correlation has been used extensively to retrieve traveltimes of surface waves. However, studies of retrieving amplitude information and attenuation from ambient noise are limited. In this study, we develop methods and strategies to extract Rayleigh wave amplitude and attenuation from ambient noise correlation, based on theoretical derivation, numerical simulation, and practical considerations of real seismic data. The synthetic data included a numerical simulation of a highly anisotropic noise source and Earth-like temporally varying strength. Results from synthetic data validate that amplitudes and attenuations can indeed be extracted from noise correlations for a linear array. A temporal flattening procedure is effective in speeding up convergence while preserving relative amplitudes. The traditional one-bit normalization and other types of temporal normalization that are applied to each individual station separately are problematic in recovering attenuation and should be avoided. In this study, we propose an ‘asynchronous’ temporal flattening procedure for real data that does not require all stations to have data at the same time. Furthermore, we present the detailed procedure for amplitude retrieval from ambient noise. Tests on real data suggest attenuations extracted from our noise-based methods are comparable with those from earthquakes. Our study shows an exciting promise of retrieving amplitude and attenuation information from ambient noise correlations and suggests practical considerations for applications to real data.;Retrieval of amplitude and attenuation from ambient seismic noise: synthetic data and practical considerations;Not health related;Not health related;0
"P. Kiran Rao; S. Chatterjee";2022;The objective of this study was to develop a system for chronic kidney disease (CKD) and to identify relevant prognostic features using a clinical dataset. Accurate classification and major risk factors in chronic kidney disease lead to better prognosis and assist nephrologists. Due to privacy and other factors, the data source is not balanced to trail any models. Therefore, it is difficult to achieve consistent accuracy with an imbalanced dataset, and there will be a variance in results with different machine learning models. In the proposed study, GAN's generated synthesised dataset, which is very close to the original dataset, is used. A hybrid synthesised dataset consists of the original dataset along with the synthesised data generated with the GAN model. The proposed model also includes the most important risk variables for CKD. The metrics used in the study include F1-score, accuracy, and from the plot, it shows that the TabNet with GAN's synthetic data is more consistent and more accurate than the traditional machine learning techniques with imbalanced dataset. The proposed model iterated for 150 times to get the variance, which is much less than in proposed techniques with hybrid preprocessed datasets. The proposed work significantly increased the classification accuracy of chronic kidney disease. These models and parameters show how important health status data is for predicting the risk of and development of kidney disease.;TabNet to Identify Risks in Chronic Kidney Disease Using GAN's Synthetic Data;health related;health related;1
"D. Panfilo; A. Boudewijn; S. Saccani; A. Coser; B. Svara; C. R. Chauvenet; C. A. Mami; E. Medvet";2023;"The recent and rapid progresses in Machine Learning (ML) tools and methodologies paved the way for an accessible market of ML services. In principle, small and medium-sized enterprises, as well as big companies, could act as providers and consumers of services, resulting in an intense exchange of ML services where a consumer may ask many providers for a service preview based on its particular business case, that is, its data. In practice, however, many potential service consumers are reluctant to release their data, when seeking for ML services, because of privacy or intellectual property concerns. As a consequence, the market of ML services is not as fluid as it could be. An alternative to providing real data when looking for an ML service consists in generating and releasing synthetic data. The synthetic data should 1) allow the service provider to preview an ML service whose performance is predictive of the one the same service will achieve on the real data; and 2) prevent the disclosure of the real data. In this paper, we propose a data synthesis technique tailored to a family of very relevant business cases: supervised and unsupervised learning on single-table datasets and relational datasets. Our technique is based on generative deep learning models and we instantiate it in three variants: standard Variational Autoencoders (VAEs),  $\beta $ -VAEs, and Introspective VAEs. We experimentally evaluate the two variants to measure the degree to which they meet the two requirements above, using several performance indexes that capture different aspects of the quality of the generated data. The results suggest that data synthesis is a practical answer to the need of decoupling ML service providers and consumers and, hence, can favor the arising of an active and accessible market of ML services.";A Deep Learning-Based Pipeline for the Generation of Synthetic Tabular Data;Not health related;Not health related;0
"L. Shi; M. Wank; Y. Chen; Y. Wang; Y. Liu; E. C. Hector; P. X. K. Song";2023;Objective: We propose a new analytic framework, “Artificial Synthetic Imaging Data (ASID) Workflow,” for sleep classification from a wearable device comprising: 1) the creation of ASID from data collected by a non-invasive wearable device that permits real-time multi-modal physiological monitoring on heart rate (HR), 3-axis accelerometer, electrodermal activity, and skin temperature, denoted as “Temporal E4 Data” (TED) and 2) the use of an image classification supervised learning algorithm, convolutional neural network (CNN), to classify periods of sleep. Methods: We investigate ASID Workflow under 6 settings (3 data resolutions _ 2 HR scenarios). Competing machine/deep learning classification algorithms, including logistic regression, support vector machine, random forest, k-nearest neighbors, and Long Short-Term Memory, are applied to TED as comparisons, termed “Competing Workflow.” Results: The ASID Workflow achieves excellent performance with mean weighted accuracy across settings of 94.7%, and is superior to the Competing Workflow with high and low resolution data regardless of the inclusion of HR modality. This superiority is maximized for low resolution data without HR. Additionally, CNN has a relatively low subject-wise test computational cost compared with competing algorithms. Conclusion: We demonstrate the utility of creating ASID from multi-modal physiological data and applying a preexisting image classification algorithm to achieve better classification accuracy. We shed light on the influence of data resolution and HR modality on the Workflow's performance. Significance: Applying CNN to ASID allows us to capture both temporal and spatial dependency among physiological variables and modalities by using 2D images' topological structure that competing algorithms fail to utilize.;Sleep Classification With Artificial Synthetic Imaging Data Using Convolutional Neural Networks;Not health related;Not health related;0
"A. Acharya; S. Sikdar; S. Das; H. Rangwala";2022;Individual-level data (microdata) that characterizes a population, is essential for studying many real-world problems. However, acquiring such data is not straightforward due to cost and privacy constraints, and access is often limited to aggregated data (macro data) sources. In this study, we examine synthetic data generation as a tool to extrapolate difficult-to-obtain high-resolution data by combining information from multiple easier-to-obtain lower-resolution data sources. In particular, we introduce a framework that uses a combination of univariate and multivariate frequency tables from a given target geographical location in combination with frequency tables from other auxiliary locations to generate synthetic microdata for individuals in the target location. Our method combines the estimation of a dependency graph and conditional probabilities from the target location with the use of a Gaussian copula to leverage the available information from the auxiliary locations. We perform extensive testing on two real-world datasets and demonstrate that our approach outperforms prior approaches in preserving the overall dependency structure of the data while also satisfying the constraints defined on the different variables.;GenSyn: A Multi-stage Framework for Generating Synthetic Microdata using Macro Data Sources;Not health related;Not health related;0
"R. Guo; B. Ayinde; H. Sun";2021;In recent years, learning based shadow detection and removal approaches have shown prospects and, in most cases, yielded state-of-the-art results. The performance of these approaches, however, relies heavily on the construction of training database of shadow images, shadow-free versions, and shadow maps as the ground truth. This conventional data gathering is time-consuming, expensive, or even practically intractable to realize especially for outdoor scenes with complicated shadow patterns, thus limiting the size of the data available for training. In this paper, we leverage on large high quality synthetic image database and domain adaptation to mitigate the bottlenecks resulting from insufficient training samples and domain bias. Specifically, our approach utilizes adversarial training to predict near-pixel-perfect shadow map from synthetic shadow image for downstreaming shadow removal steps. At inference time, we capitalize on domain adaptation via image style transfer to map the style of real-world scene to that of synthetic scene for the purpose of detecting and subsequently removing shadow. Comprehensive experiments indicate that our approach outperforms state-of-the-art methods on selected benchmark datasets.;Efficient Shadow Detection and Removal using Synthetic Data with Domain Adaptation;Not health related;Not health related;0
"W. Armstrong; S. Drakontaidis; N. Lui";2023;Images of spacecraft photographed from other space-craft operating in outer space are difficult to come by, especially at a scale typically required for deep learning tasks. Semantic image segmentation, object detection and localization, and pose estimation are well researched areas with powerful results for many applications, and would be very useful in autonomous spacecraft operation and rendezvous. However, recent studies show that these strong results in broad and common domains may generalize poorly even to specific industrial applications on earth. To address this, we propose a method for generating synthetic image data that are labelled for semantic segmentation, generalizable to other tasks, and provide a prototype synthetic image dataset consisting of 2D monocular images of unmanned spacecraft, in order to enable further research in the area of autonomous spacecraft rendezvous. We also present a strong benchmark result (SØrensen-Dice coefficient 0.8723) on these syn-thetic data, suggesting that it is feasible to train well-performing image segmentation models for this task, especially if the target spacecraft and its configuration are known.;Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft;Not health related;Not health related;0
"M. W. Rodrigues; L. E. Zárate";2019;Industrial environments demand constant monitoring in their activities, aiming to guarantee effectiveness concerning safety, production, and quality. The probability of failure in the security issue can lead to a succession of other failures, which will inevitably lead to increased risk in disaster systems, causing a high environmental, social, and economic impact. An old proverb states, “prevention is better than cure”, so we have developed a methodology for detecting and describing the patterns that lead to abrupt changes in the behavior of sensor signals in specific operating scenarios. For this, we have developed a parameterizable time series generator, which allows us to represent several types of scenarios where these sensors can operate. The generator seeks to overcome the problem of access or lack of temporal data generated by real sensors such as dams in the miner industry, for example. We approach three scenarios of time series where stationarity predominates, where variations and trends are propagated, and where anomalous or extreme events may occur. Also, we propose strategies to characterize the synthetic time series, and finally, we use unsupervised machine learning techniques to analyze the temporal evolution of the sensor system. In our analyses, we are using evaluative metrics for validating the proposed methodology, such as the stability index of the clusters along the windowing, the instability index of the sensors intracluster and intercluster, and also by the agglomerative coefficient between the clusters by temporal windowing.;Time Series Analysis Using Synthetic Data for Monitoring the Temporal Behavior of Sensor Signals;Not health related;Not health related;0
"D. McDuff; X. Liu; J. Hernandez; E. Wood; T. Baltrusaitis";2021;Synthetic data is a powerful tool in training data hungry deep learning algorithms. However, to date, camera-based physiological sensing has not taken full advantage of these techniques. In this work, we leverage a high-fidelity synthetics pipeline for generating videos of faces with faithful blood flow and breathing patterns. We present systematic experiments showing how physiologically-grounded synthetic data can be used in training camera-based multi-parameter cardiopulmonary sensing. We provide empirical evidence that heart and breathing rate measurement accuracy increases with the number of synthetic avatars in the training set. Furthermore, training with avatars with darker skin types leads to better overall performance than training with avatars with lighter skin types. Finally, we discuss the opportunities that synthetics present in the domain of camera-based physiological sensing and limitations that need to be overcome.;Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing;Not health related;Not health related;0
"A. I. Károly; S. Tirczka; H. Gao; I. J. Rudas; P. Galambos";2024;Recent problems in robotics can sometimes only be tackled using machine learning technologies, particularly those that utilize deep learning (DL) with transfer learning. Transfer learning takes advantage of pretrained models, which are later fine-tuned using smaller task-specific datasets. The fine-tuned models must be robust against changes in environmental factors such as illumination since, often, there is no guarantee for them to be constant. Although synthetic data for pretraining has been shown to enhance DL model generalization, there is limited research on its application for fine-tuning. One limiting factor is that the generation and annotation of synthetic datasets can be cumbersome and impractical for the purpose of fine-tuning. To address this issue, we propose two methods for automatically generating annotated image datasets for object segmentation, one for real-world and another for synthetic images. We also introduce a novel domain adaptation approach called filling the reality gap (FTRG), which can blend elements from real-world and synthetic scenes in a single image to achieve domain adaptation. We demonstrate through experimentation on a representative robot application that FTRG outperforms other domain adaptation techniques, such as domain randomization or photorealistic synthetic images, in creating robust models. Furthermore, we evaluate the benefits of using synthetic data for fine-tuning in transfer learning and continual learning with experience replay using our proposed methods and FTRG. Our findings indicate that fine-tuning with synthetic data can produce superior results compared to solely using real-world data.;Increasing the Robustness of Deep Learning Models for Object Segmentation: A Framework for Blending Automatically Annotated Real and Synthetic Data;Not health related;Not health related;0
"M. Wilchek; Y. Wang";2021;Personally identifiable information (PII) continues to be used in predictive modeling by academic researchers and industry organizations. Most notably, the healthcare industry has been a popular testbed for innovative approaches from academia and institutions to address research using PII in predictive applications and synthetic data generation. The majority of these approaches that generate synthetic PII are based on actual data or obfuscating real data parts. Privacy leakage and ethical disclosure results continue to be among the largest issues that are difficult to avoid in synthetic PII generation techniques. In this analysis, we propose a novel method to generate synthetic, differential privacy data while avoiding the common pitfalls and capable of being leveraged broadly. Evidence is also shown that proves how our novel approach can maintain inference for modeling and potential risks tied to PII features. We conclude with a summarization of our findings and results and a short discussion on how using PII data may impact organizations interested in developing predictive applications.;Synthetic Differential Privacy Data Generation for Revealing Bias Modelling Risks;Not health related;Not health related;0
"A. Esser; S. Rinderknecht";2020;Synthetic Driving Cycles have been used in numerous studies to describe a certain driving profile of relevance. An important purpose of synthetic cycles is to limit the necessary time on a test-rig or to reduce the computational effort within simulations, which is achieved by compressing a larger amount of gathered operating data from a certain vehicle or a vehicle fleet to a necessary minimum. Interestingly, despite the intensive use of the synthetic driving cycles, there is only limited literature on the validation of using synthetic driving cycles. Therefore, the scope of this work is to further investigate under which conditions synthetic driving cycles can be used to replace the entirety of the relevant operating data in the evaluation of a vehicle's consumption. We apply a longitudinal vehicle simulation model to calculate the fuel and electric consumption of vehicles with different powertrain concepts on many generated synthetic driving cycles for different compression rates. We then compare that to the consumption if considering the original driving data. A legislative driving cycle (WLTC) as well as naturalistic driving data sets are used for the evaluation. The results show, that synthetic driving cycles allow for a compact representation of the original data sets but possible compression rates depend on the specific driving data. The presented two-step process can be extended to a generalized validation process for the use of synthetic driving cycles.;Process for the Validation of Using Synthetic Driving Cycles Based on Naturalistic Driving Data Sets;Not health related;Not health related;0
"A. M. Schreiber; M. Hong; J. W. Rozenblit";2021;Depth estimation is an important challenge in the field of augmented reality. Supervised deep learning methods of depth estimation can be difficult to use in novel settings due to the need for labeled training data. The work presented in this paper overcomes the challenge in a laparoscopic surgical simulation environment by using synthetic data generation for RGB-D training data. We also provide a neural network architecture that can generate real-time 448x448 depth map outputs suitable for use in AR applications. Our approach shows satisfactory performance when tested on a non-synthetic test dataset with an RMSE of 2.50 cm, MAE of 1.04 cm, and _ < 1.25 of 0.987.;Monocular Depth Estimation using Synthetic Data for an Augmented Reality Training System in Laparoscopic Surgery;health related;health related;0
"G. Visani; G. Graffi; M. Alfero; E. Bagli; F. Chesani; D. Capuzzo";2022;The switch from a Model-Centric to a Data-Centric mindset is putting emphasis on data and its quality rather than algorithms, bringing forward new challenges. In particular, the sensitive nature of the information in highly regulated scenarios needs to be accounted for. Specific approaches to address the privacy issue have been developed, as Privacy Enhancing Technologies. However, they frequently cause loss of information, putting forward a crucial trade-off among data quality and privacy. A clever way to bypass such a conundrum relies on Synthetic Data: data obtained from a generative process, learning the real data properties. Both Academia and Industry realized the importance of evaluating synthetic data quality: without all-round reliable metrics, the innovative data generation task has no proper objective function to maximize. Despite that, the topic remains under-explored. For this reason, we systematically catalog the important traits of synthetic data quality and privacy, and devise a specific methodology to test them. The result is DAISYnt (aDoption of Artificial Intelligence SYnthesis): a comprehensive suite of advanced tests, which sets a de facto standard for synthetic data evaluation. As a practical use-case, a variety of generative algorithms have been trained on real-world Credit Bureau Data. The best model has been assessed, using DAISYnt on the different synthetic replicas. Further potential uses, among others, entail auditing and fine-tuning of generative models or ensuring high quality of a given synthetic dataset. From a prescriptive viewpoint, eventually, DAISYnt may pave the way to synthetic data adoption in highly regulated domains, ranging from Finance to Healthcare, through Insurance and Education.;Enabling Synthetic Data adoption in regulated domains;Not health related;Not health related;0
"B. -C. Tai; S. -C. Li; Y. Huang; P. -C. Wang";2022;With the emergence of AI(artificial intelligence), it is becoming more and more critical for organizations to utilize it to their advantage. However, organizations that possess a decent amount of data might not have the technical competence to perform machine learning, and vice versa. Hence, it is reasonable for the two kinds of organizations to work together to realize the value of the data. With the increasing concern over data privacy, regulations such as GDPR(General Data Protection Regulation) prevent an organization from sharing data with another unless the data is processed to the point that the individuals in the data are not identifiable. Various ways of data anonymization have been proposed and developed, including the ones that utilize neural networks to achieve the goal, like AE, VAE, and GAN. With the addition of a differential privacy framework like TensorFlow Privacy, privacy can be guaranteed, but data still needs to be usable after privacy protection measures are deployed. The present study aims to integrate TensorFlow Privacy into the synthetic data generation process and evaluate its usefulness for daily use in the industries. Since TensorFlow Privacy brings a provable privacy guarantee to synthetic data, the present study focuses on the evaluation of data utility. TensorFlow is widely used for machine learning in the industry and academically. TensorFlow Privacy, which is also developed by Google, can prove to be a valuable addition to the synthetic data generation pipeline. The result shows that VAE with TensorFlow Privacy 1) generates synthetic data with good data utility in most cases in terms of descriptive statistics and machine learning classification tasks, and 2) The customizable TensorFlow Privacy parameters work as intended in terms of privacy-utility trade-off.;Examining the Utility of Differentially Private Synthetic Data Generated using Variational Autoencoder with TensorFlow Privacy;Not health related;Not health related;0
"L. Solida; G. Chicco; E. Bompard; T. Huang; A. Mazza; M. R. Rapizza";2022;This paper addresses the preparation of synthetic models for electrical transmission systems, using open (publicly available) data. Starting from information gathered from maps with multiple data such as OpenStreetMap, the nodes and lines are extracted and are used to establish the network topology. An example constructed in an Italian region is shown.;Topological Aspects of Building Synthetic Models for Power Transmission Networks from Public Data;Not health related;Not health related;0
"T. K. Tran; C. T. Nguyen; N. H. Phung; T. T. H. Truong; D. C. Tran; T. T. Nguyen; T. M. P. Le";2022;Down’s Syndrome (DS) is one of the most prevalent types of chromosomal abnormality. Developmental delays and disabilities on the physical and mental levels result from this syndrome. Hence, it must be detected as soon as possible. However, Down syndrome screening data tend to have a large overall data pool with a small proportion of positive cases, leading to an imbalanced class problem that causes classifiers to become biased. Moreover, the number of these data is also another problem, especially in Vietnam. These data are often private and challenging to collect correctly. This study utilizes synthetic data generating methods to maximize the detection rate of some traditional classification models. The final results of this study indicate that it is possible to improve the down syndrome prediction quality of the classifiers by adequately incorporating the SMOTE-based and GANs-based methods to generate synthetic data.;Evaluation of Synthetic Data Generating Methods in Down Syndrome Prediction During the First Trimester Screening in Vietnam;Not health related;Not health related;0
"M. Van Aerde; G. MacKinnon; B. Hellinga";1991;The provision of two-way communications between the central traffic control center and all vehicles, which are equipped with route guidance systems (RGS), makes it possible to assemble very efficiently a continuous estimate of the prevailing dynamic or time-varying O-D (Origin-Destination) matrix. This paper describes the mathematics of a four step procedure for developing such a time varying OD matrix from vehicle probe data and discusses the important statistical limitations of the final OD estimates. It is shown that, unless there is a very high level of market penetration in the network, the resulting OD estimate will contain such a high level of statistical uncertainty as to render it of limited practical value. consequently, it is anticipated that the resulting OD estimate will even in the long term not be unsuitable to be utilized on its own, and that instead this estimate will always need to be further processed as a seed solution matrix for a subsequent dynamic synthetic OD generation analysis.;The generation of synthetic O-D demands from real-time vehicle probe data: Potential and limitations;Not health related;Not health related;0
"V. Provalov; E. Stavinova; P. Chunaev";2021;This study proposes a novel method for evaluating and comparing recommender systems using synthetic user and item data and parametric synthetic user-item response (rating) functions. The method compares recommender systems on classes of synthetic data, oppositely to how it is usually done on particular real or synthetic datasets. The usage of classes particularly allows for managing the effects of the No Free Lunch theorem for recommender systems. Furthermore, we implement the method in the form of a flexible framework (that we call SynEvaRec) for conducting comparison experiments under different scenarios of synthetic data behaviour. Our experimental study shows that SynEvaRec helps to determine scenarios (e.g. in terms of data classes) where one recommender system is more preferable than another by means of recommendation quality. Moreover, the results turn to be rather stable over several synthetic dataset instances based on the same real-world dataset indicating the robustness of our method. The datasets, the framework implementation and the results related to our study are publicly available on GitHub.;SynEvaRec: A Framework for Evaluating Recommender Systems on Synthetic Data Classes;Not health related;Not health related;0
"C. Wisultschew; R. Hernández; C. Pastor; J. Portilla";2022;Light detection and ranging (LiDAR) sensors are increasing in popularity due to the advantages they provide over 2-D sensors in IoT object detection and classification applications, because of their ability to provide very precise distances to objects. Deep learning algorithms need a huge amount of data during training to obtain high accuracy results. When using 2-D images, a vast quantity of data sets are publicly available, but this is not the case for LiDAR point clouds. Each LiDAR model generates a point cloud with unique properties, which causes the data sets not to be compatible between different LiDAR models. As a result, when using deep learning with LiDARs, it is necessary to generate the data sets manually. For this purpose, the data must be captured and then labeled one by one, which is a very time and cost-consuming process. To overcome this issue and to reduce the development time when using LiDAR sensors with deep learning algorithms, a methodology is proposed in this article to automatically generate point cloud data sets using a 3-D simulator for autonomous cars. In this regard, a data set can be generated for any LiDAR model by adding the specific LiDAR parameters to the simulator. Besides, custom scenarios can be designed and generated, based on the final deployment location, to provide a simulated solution very close to the final implementation. With the proposed methodology, a simulation can be performed to select the LiDAR that best fits certain application requirements, in contrast to the traditional approach where the LiDAR must first be purchased.;Synthetic LiDAR-Labeled Data Set Generation to Train Deep Neural Networks for Object Classification in IoT at the Edge;Not health related;Not health related;0
"J. Drögemüller; C. X. Garcia; E. Gambaro; M. Suppa; J. Steil; M. A. Roa";2021;This paper proposes a novel approach to automatically generate labeled training data for predicting parallel-jaw grasps from stereo-matched depth images. We generate realistic depth images using Semi-Global Matching to compute disparity maps from synthetic data, which allows producing images that mimic the typical artifacts from real stereo matching in our data, thus reducing the gap from simulation to real execution. Our pipeline automatically generates grasp annotations for single or multiple objects on the synthetically rendered scenes, avoiding any manual image pre-processing steps such as inpainting or denoising. The labeled data is then used to train a CNN-model that predicts parallel-jaw grasps, even in scenarios with large amount of unknown depth values. We further show that scene properties such as the presence of obstacles (a bin, for instance) can be added to our pipeline, and the training process results in grasp prediction success rates of up to 90%.;Automatic generation of realistic training data for learning parallel-jaw grasping from synthetic stereo images;Not health related;Not health related;0
"R. Pfeffer; K. Bredow; E. Sax";2019;Applications in the field of Deep Learning are constantly increasing the need for extensive, annotated data sets. Simulation software offers the possibility to create data sets of almost any size and thus the potential to cover this need. In the context of autonomous driving the test effort in the development processes increases to an extent that the use of virtual driving environments comes into focus. Deep Learning offers the opportunity to learn advanced driving strategies, starting from image recognition up to trajectory implementation.In this paper different driving simulation environments are used as data sources for the training of a neural network and examined for applicability on the basis of the working example of vehicle detection in image data.The visualizations include CarMaker, Carla and Grand Theft Auto V with different procedures for exporting ground truth information. For object detection, a pre-trained Convolutional Neural Network (CNN) is used and the effects on the detector quality of the data source and data size are investigated.An important part of the task of generating suitable data is the annotation process. The quality of the generated data is a decisive factor in achieving high performance training results. The test executions show that virtual data sources in training achieve lower detection rates in contrast to real data, but are more cost-effective in the overall context by scales.;Trade-off analysis using synthetic training data for neural networks in the automotive development process;Not health related;Not health related;0
"P. Melzi; C. Rathgeb; R. Tolosana; R. Vera-Rodriguez; A. Morales; D. Lawatsch; F. Domin; M. Schaubert";2023;This study investigates the possibility of mitigating the demographic biases that affect face recognition technologies through the use of synthetic data. Demographic biases have the potential to impact individuals from specific demographic groups, and can be identified by observing disparate performance of face recognition systems across demographic groups. They primarily arise from the unequal representations of demographic groups in the training data. In recent times, synthetic data have emerged as a solution to some problems that affect face recognition systems. In particular, during the generation process it is possible to specify the desired demographic and facial attributes of images, in order to control the demographic distribution of the synthesized dataset, and fairly represent the different demographic groups. We propose to fine-tune with synthetic data existing face recognition systems that present some demographic biases. We use synthetic datasets generated with GANDiffFace, a novel framework able to synthesize datasets for face recognition with controllable demographic distribution and realistic intra-class variations. We consider multiple datasets representing different demographic groups for training and evaluation. Also, we fine-tune different face recognition systems, and evaluate their demographic fairness with different metrics. Our results support the proposed approach and the use of synthetic data to mitigate demographic biases in face recognition.;Synthetic Data for the Mitigation of Demographic Biases in Face Recognition;Not health related;Not health related;0
"J. Fulir; L. Bosnar; H. Hagen; P. Gospodneti_";2023;Metal defect segmentation poses a great challenge for automated inspection systems due to the complex light reflection from the surface and lack of training data. In this work we introduce a real and synthetic defect segmentation dataset pair for multi-view inspection of a metal clutch part to overcome data shortage. Model pre-training on our synthetic dataset was compared to similar inspection datasets in the literature. Two techniques are presented to increase model training efficiency and prediction coverage in darker areas of the image. Results were collected over three popular segmentation architectures to confirm superior effectiveness of synthetic data and unveil various challenges of multi-view inspection.;Synthetic Data for Defect Segmentation on Complex Metal Surfaces;Not health related;Not health related;0
"K. Shouryadhar; P. Kiran Rao; S. Chatterjee";2022;This research was conducted with the goals of developing models that have an accurate classification of chronic kidney disease (CKD) and locating significant prognostic factors within a clinical dataset. In chronic kidney disease, accurate classification and identification of major risk factors contribute to improved prognoses and provide assistance to nephrologists. The data source is not balanced enough to serve as a benchmark for any machine learning or deep learning models due to privacy concerns and other factors. As a result, it is difficult to achieve consistent accuracy with an imbalanced dataset, and there will be a variance in results with various machine learning models due to the fact that there are so many different factors involved. A multiple-level ensemble learning system was utilised in the proposed research in order to classify chronic kidney disease using a hybrid synthesised dataset. A hybrid synthesised dataset is one that contains both the original dataset as well as the synthesised data that was produced using the ADASYN method. The most important risk factors for chronic kidney disease are accounted for in the model that was proposed. The F1-score and accuracy were two of the metrics that were utilised in this research. Furthermore, the plot demonstrates that the multilevel ensemble is superior to the conventional machine learning techniques in terms of consistency and accuracy. The variance was calculated using the proposed model by iterating for a total of 150 times, which is a significant reduction when compared to multi-level ensemble techniques using hybrid preprocessed datasets. The accuracy of classifying patients with chronic kidney disease was significantly improved by using a multi-level ensemble. These models and parameters highlight the significance of current health status information in estimating the likelihood of developing kidney disease as well as its progression.;Multilevel Ensemble Method to Identify Risks in Chronic Kidney Disease Using Hybrid Synthetic Data;health related;health related;1
"G. H. Tuell; J. R. Lucas; D. B. Graham";1999;It is widely recognized that synthetic aperture radar (SAR) technology holds great promise for shoreline mapping applications. Because SAR offers flexibility over weather and time of day restrictions, it may be especially valuable when mapping remote areas such as southwest Alaska. The authors have mapped a five nautical mile stretch of the coast of Castle Bay, Alaska using both 1-meter airborne SAR and RADARSAT fine mode imagery. They have compared the resulting shoreline manuscripts with shoreline data produced using their conventional photogrammetric process. Although the different data sets are separated temporally by several months, and tidal differences exist, the resulting comparisons yield an opportunity to quantify the performance of SAR for this application. As part of their work, they have developed a semi-automated approach to remove positional biases from SAR imagery, and they use the residual vector from this approach as a metric for evaluating accuracy. They present results to date in the analysis of RADARSAT data.;An accuracy assessment of shoreline data for Castle Bay Alaska compiled from synthetic aperture radar;Not health related;Not health related;0
"M. V. Sowmyashree; T. V. Ramachandra";2013;Earth's surface consists of land features such as vegetation, soil, water, etc. Modeling of the earth's surface requires identification and understanding of the dynamics of land features. Analysis of land feature dynamics would reveal the changes that occur due to human induced activities or natural phenomenon. This plays a major role in providing up-to-date information of the natural resources. Data acquired remotely through space-borne sensors at regular intervals in visible and microwave bands aid in spatial mapping of the land features. Data acquired in visible and IR (Infrared) bands have been used for land use and land cover analysis. However, these data fails when there are cloud cover due to non-selective scattering. In this context, RADAR remote sensing would be useful as it provide information during all seasons due to long penetration properties. In present study, RADARSAT-2 single polarized HH (i.e., Horizontal to Horizontal with C-band) has been used to derive land features with spatial extent. Radar data interpretation and analysis is considered challenging and have both advantages and disadvantages in land use feature extraction. This study assess the performance of classification algorithms (Gaussian Maximum likelihood classifier (GMLC), Neural network classifier, Decision tree classifier (DTC), Contextual classification using sequential maximum a posteriori (SMAP) estimation for feature extraction using multi-temporal single polarized RADARSAT data, texture extracted data and fused data (optical sensor-LANDSAT ETM+ with SAR data). Accuracy assessments suggest that fused data perform better with all algorithms.;Algorithms for feature extraction from synthetic aperture radar data;Not health related;Not health related;0
"F. Ragusa; D. Di Mauro; A. Palermo; A. Furnari; G. M. Farinella";2021;We consider the problem of object segmentation in cultural sites. Since collecting and labeling large datasets of real images is challenging, we investigate whether the use of synthetic images can be useful to achieve good segmentation performance on real data. To perform the study, we collected a new dataset comprising both real and synthetic images of 24 artworks in a cultural site. The synthetic images have been automatically generated from the 3D model of the considered cultural site using a tool developed for that purpose. Real and synthetic images have been labeled for the task of semantic segmentation of artworks. We compare three different approaches to perform object segmentation exploiting real and synthetic data. The experimental results point out that the use of synthetic data helps to improve the performances of segmentation algorithms when tested on real images. Satisfactory performance is achieved exploiting semantic segmentation together with image-to-image translation and including a small amount of real data during training. To encourage research on the topic, we publicly release the proposed dataset at the following url:https://iplab.dmi.unict.it/EGO-CH-OBJ-SEG/.;Semantic Object Segmentation in Cultural Sites using Real and Synthetic Data;Not health related;Not health related;1
"O. A. Abraham; H. Ochiai; M. D. Hossain; Y. Taenaka; Y. Kadobayashi";2023;Electricity thefts are conventionally manually detected by inspections, accusations, and the failure of meters. However, the recent evolution of machine learning may allow the automatic detection of electricity theft only from the patterns of meter readings. Electric consumption heavily relies on many factors, e.g., the lifestyle of the day and the weather, and thus the accuracy of detection is questioned. We propose an electricity theft detection framework for smart homes with knowledge-based synthetic attack data. This allows training of the attack classifier only from the legitimate power consumption data, i.e, without attack actions and associated labels. We identified five attack patterns as the knowledge which consisted of smart attacks and legacy attacks. We have conducted comprehensive evaluations with nine machine learning models using the Almanac of Minutely Power dataset version 2 (AMPds2) dataset fine-grained time-series data of a smart home. We found that Gradient Boosting-based algorithms achieved the best, and Random Forest performed alternatively with almost 100% accuracy for detecting and classifying legacy attacks. Some smart attacks were not detected, but those algorithms achieved good performance in detection and classification.;Electricity Theft Detection for Smart Homes with Knowledge-Based Synthetic Attack Data;Not health related;Not health related;0
"F. Hagelskjær; A. G. Buch";2022;Pose estimation is the task of determining the 6D position of an object in a scene. Pose estimation aid the abilities and flexibility of robotic set-ups. However, the system must be configured towards the use case to perform adequately. This configuration is time-consuming and limits the usability of pose estimation and, thereby, robotic systems. Deep learning is a method to overcome this configuration procedure by learning parameters directly from the dataset. However, obtaining this training data can also be very time-consuming. The use of synthetic training data avoids this data collection problem, but a configuration of the training procedure is necessary to overcome the domain gap problem. Additionally, the pose estimation parameters also need to be configured. This configuration is jokingly known as grad student descent as parameters are manually adjusted until satisfactory results are obtained. This paper presents a method for automatic configuration using only synthetic data. This is accomplished by learning the domain randomization during network training, and then using the domain randomization to optimize the pose estimation parameters. The developed approach shows state-of-the-art performance of 82.0 % recall on the challenging OCCLUSION dataset, outperforming all previous methods with a large margin. These results prove the validity of automatic set-up of pose estimation using purely synthetic data.;ParaPose: Parameter and Domain Randomization Optimization for Pose Estimation using Synthetic Data;Not health related;Not health related;0
"H. Yu; S. M. Hanafy; L. Liu";2022;Recorded seismograms are usually distorted by statics owing to complex geological conditions, such as lateral variations in sediment thickness or complex topographies. These distorted and discontinuous signals usually exist in either arrival times or amplitudes of waves, and they are most likely to be smeared as velocity perturbations along their associated raypaths. Therefore, statics may blur images of the target bodies or, even worse, introduce unexpected and false anomalies into subsurface structures. To partly resolve this problem, we develop a weighted statics correction method to estimate unwanted temporal shifts of traces using the closure-phase technique, which is utilized in astronomical imaging. In the proposed method, the source and receiver statics are regarded as independent quantities contributing to the waveform shifts based on their acquisition geometries. Numerical tests on both the synthetic and field cases show noticeable, although gradual, improvements in data quality compared to the conventional plus–minus (PM) method. In general, this method provides a straightforward strategy to reedit the travel times in seismic profiles without inverting for a near-surface velocity model. Moreover, it can be extended to any interferometrical methods in seismic data processing that satisfies the closure-phase conditions.;A Weighted Closure-Phase Statics Correction Method: Synthetic and Field Data Examples;Not health related;Not health related;0
"B. Abhilash; C. Syranidou; J. Linssen; D. Stolten";2021;To effectively analyze the impacts of rapidly evolving new and advanced distributed power generation, demand integration in distribution grids requires grid topologies that replicate the existing network topologies. Estimating these can provide datasets with quasi-network topologies maintaining functional electrical characteristics at any location of interest. Herein, these topologies will be used to identify vulnerabilities that affect grid stability, grid reinforcement, and will form the basis of economic analysis. In this study, a new process is developed for generating georeferenced, synthetic, low-voltage network topologies using openly available data procured from diverse sources for Germany as a case study. The topologies are first synthesized by collecting the geo-referenced building's data from OpenStreetMap (OSM). Then, a clustering technique is performed to group collected buildings by known information about the transformer count. Next, detailed algorithms are developed to collect street lines, generate graphs combining streets and buildings, transformer placements, and to assign real electrical parameters. Finally, the methodology is applied to Germany's building data (extracted from OSM), which yielded 500,000 low-voltage network topologies. The estimated topology data covers approximately 29 million buildings as electrical nodes and approximately 1.7 million km or 0.8 million km of electrical lines length with or without service drops.;Geo-referenced synthetic low-voltage distribution networks: A data-driven approach;Not health related;Not health related;0
"F. Khan; W. Shariff; M. A. Farooq; S. Basak; P. Corcoran";2023;Due to the real-time acquisition and reasonable cost of consumer cameras, monocular depth maps have been employed in a variety of visual applications. Regarding ongoing research in depth estimation, they continue to suffer from low accuracy and enormous sensor noise. To improve the prediction of depth maps, this paper proposed a lightweight neural facial depth estimation model based on single image frames. Following a basic encoder-decoder network design, the features are extracted by initializing the encoder with a high-performance pre-trained network and reconstructing high-quality facial depth maps with a simple decoder. The model can employ pixel representations and recover full details in terms of facial features and boundaries by employing a feature fusion module. When tested and evaluated across four public facial depth datasets, the suggested network provides more reliable and state-of-the-art results, with significantly less computational complexity and a reduced number of parameters. The training procedure is primarily based on the use of synthetic human facial images, which provide a consistent ground truth depth map, and the employment of an appropriate loss function leads to higher performance. Numerous experiments have been performed to validate and demonstrate the usefulness of the proposed approach. Finally, the model performs better than existing comparative facial depth networks in terms of generalization ability and robustness across different test datasets, setting a new baseline method for facial depth maps.;A Robust Light-Weight Fused-Feature Encoder-Decoder Model for Monocular Facial Depth Estimation From Single Images Trained on Synthetic Data;Not health related;Not health related;0
"Y. Boada; A. Vignoni; J. Picó";2017;Synthetic biology use mathematical models of biological circuits to predict the behavior of the designed synthetic devices, but also to help in the design of the circuit and for the selection of their biological components. Estimation of these models parameters remains a demanding problem that has been addressed by optimization of a weighted combination of different prediction errors, thus obtaining only one solution. This single-objective approach can be inadequate when trying to incorporate different kinds of experiments or to identify parameters for an ensemble of biological circuit models and even more when dealing with stochastic models and flow cytometry data. Stochasticity in biological systems, often referred to as gene expression noise, is ubiquitous and needs to be taken into account when modeling a biological system. Here we present a methodology based on multi-objective optimization to perform parameter estimation in stochastic models using flow citometry data. It uses a global multi-objective evolutionary algorithm and a multi-criteria decision making strategy to select the most suitable solutions. We obtain an approximation to the Pareto set that corresponds to the model parameters better fitting the experimental data. Then, the Pareto set is clustered according to the different experimental cases, allowing to analyze the sensitivity of model parameters. We show the methodology applicability through the case study of a genetic circuit which controls noisy protein expression in a cell population.;Multi-objective identification of synthetic circuits stochastic models using flow flcytometry data;Not health related;Not health related;0
"S. Koo; C. Park; S. Lee; J. Seo; S. Eo; H. Moon; H. Lim";2023;In a Data-Centric AI paradigm, the model performance is enhanced without altering the model architecture, as evidenced by real-world and benchmark dataset demonstrations. With the advancements of large language models (LLM), it has become increasingly feasible to generate high-quality synthetic data, while considering the need to construct fully synthetic datasets for real-world data containing numerous personal information. However, in-depth validation of the solely synthetic data setting has yet to be conducted, despite the increased possibility of models trained exclusively on fully synthetic data emerging in the future. Therefore, we examined the question, “Do data quality control techniques (known to positively impact data-centric AI) consistently aid models trained exclusively on synthetic datasets?”. To explore this query, we performed detailed analyses using synthetic datasets generated for speech recognition postprocessing using the BackTranScription (BTS) approach. Our study primarily addressed the potential adverse effects of data quality control measures (e.g., noise injection and balanced data) and training strategies in the context of synthetic-only experiments. As a result of the experiment, we observed the negative effect that the data-centric methodology drops by a maximum of 44.03 points in the fully synthetic data setting.;Uncovering the Risks and Drawbacks Associated With the Use of Synthetic Data for Grammatical Error Correction;Not health related;Not health related;0
"Z. Chen; Y. Liu; H. Shen; T. Luo; R. Zhao";2022;"Color fundus (CF) image registration is crucial for accurate information fusion; it could obtain more details of retinal structure to assist clinical diagnosis. Existing methods suf-fer from costing time or dataset size, making CF image reg-istration still a challenging task. In this paper, we propose a novel pair-metamorphosis-decouple synthetic data scheme for learning-based CF image registration and ameliorate the registration model for retinal image. Specifically, we take ad-vantage of the pairing information of the registration task to decouple the differences between the pairing data and expand the representative ability of the dataset by synthesizing data. Furthermore, the registration framework is ameliorated ac-cording to the characteristics of the blood vessels in the retinal image. Experiments on the public dataset (FIRE) show that our synthetic data scheme could bring general performance promotion to registration models, and our registration method is superior to other state-of-the-art unsupervised algorithms.";A Pair-Metamorphosis-Decouple Synthetic Data Scheme for Color Fundus Image Registration;Not health related;Not health related;0
"C. Pandey; V. Tiwari; R. S. Rathore; R. H. Jhaveri; D. S. Roy; S. Selvarajan";2023;Mobile Edge Computing (MEC) in 5G networks has emerged as a promising technology to enable efficient and low-latency services for mobile users. In this paper, we present a novel synthetic data generation approach tailored for evaluating MEC in 5G networks. Our methodology incorporates resource-efficient techniques to generate realistic synthetic datasets that capture the spatio-temporal patterns of mobile traffic and user behavior. By leveraging advanced modeling techniques, including multi-head attention and bidirectional LSTM, we accurately model the complex dependencies in the data while optimizing computational resources. The proposed synthetic data generator enables the creation of diverse datasets that closely resemble real-world scenarios, facilitating the evaluation of MEC performance and optimizing resource utilization. Through extensive experiments and evaluations, we demonstrate the effectiveness of our approach in enabling accurate assessments of MEC in 5G networks. Our work contributes to the field by providing a robust methodology for synthetic data generation specifically tailored for MEC evaluation, addressing the need for resource-efficient evaluation frameworks in the context of emerging technologies. The results of our study provide valuable insights for the design and optimization of MEC systems in real-world deployments.;Resource-Efficient Synthetic Data Generation for Performance Evaluation in Mobile Edge Computing Over 5G Networks;Not health related;Not health related;0
"V. C. Pezoulas; N. S. Tachos; G. Gkois; I. Olivotto; F. Barlocco; D. I. Fotiadis";2022;Goal: To develop a computationally efficient and unbiased synthetic data generator for large-scale in silico clinical trials (CTs). Methods: We propose the BGMM-OCE, an extension of the conventional BGMM (Bayesian Gaussian Mixture Models) algorithm to provide unbiased estimations regarding the optimal number of Gaussian components and yield high-quality, large-scale synthetic data at reduced computational complexity. Spectral clustering with efficient eigenvalue decomposition is applied to estimate the hyperparameters of the generator. A case study is conducted to compare the performance of BGMM-OCE against four straightforward synthetic data generators for in silico CTs in hypertrophic cardiomyopathy (HCM). Results: The BGMM-OCE generated 30000 virtual patient profiles having the lowest coefficient-of-variation (0.046), inter- and intra-correlation differences (0.017, and 0.016, respectively) with the real ones in reduced execution time. Conclusions: BGMM-OCE overcomes the lack of population size in HCM which obscures the development of targeted therapies and robust risk stratification models.;Bayesian Inference-Based Gaussian Mixture Models With Optimal Components Estimation Towards Large-Scale Synthetic Data Generation for In Silico Clinical Trials;health related;health related;0
"M. Boldeanu; M. González-Alonso; H. Cucu; C. Burileanu; J. M. Maya-Manzano; J. T. M. Buters";2022;Pollen allergies have become one of the most wide-spread afflictions that impact quality of life. This has made automatic pollen detection, classification and monitoring a very important topic of research. This paper introduces a new public annotated image data-set of pollen with almost 45 thousand samples obtained from an automatic instrument. In this work we apply some of the best performing convolutional neural networks architectures on the task of pollen classification as well as some fully convolutional networks optimized for image segmentation on complex microscope images. We obtain an F1 scores of 0.95 on the new data-set when the best trained model is used as a fully convolutional classifier and a class mean Intersection over Union (IoU) of 0.88 when used as an object detector.;Automatic Pollen Classification and Segmentation Using U-Nets and Synthetic Data;Not health related;Not health related;0
"H. Le; M. Nguyen; W. Q. Yan";2020;"The idea of Augmented Reality (AR) appeared in the early 60s, which recently received a large amount of public attention. AR allows us to work, learn, play, and connect with the world around us both virtually and physically in real-time. However, picking the AR marker to match the users' needs is one of the most challenging tasks due to different marker encryption/decryption methods and essential requirements. Barcode AR cards are fast and efficient, but they do not contain much visual information; pictorial coloured AR card, on the other hand, is slow and not reliable. This paper proposes a solution to obtain detectable arbitrary pictorial/colour AR cards in real-time by applying the benefit of machine learning and the power of synthetic data generation techniques. This technique solves the issue of labour-intensive tasks of manual annotations when building a massive training dataset of deep-learning. Thus, with a small number of input of the AR-enhanced target figures (as few as one for each coloured card), the synthetic data generated process will produce a deep-learning trainable dataset using computer-graphic rendering techniques (ten of thousands from just one image). Second, the generated dataset is then trained with a chosen object recognition convolutional neural network, acting as the AR marker tracking functionality. Our proposed idea works effectively well without modifying the original contents (of the chosen AR card). The benefits of using synthetic data generated techniques help us to improve the AR marker recognition accuracy and reduce the marker registration time. The trained model is capable of processing video sequences at approximately 25 frames per second without GPU Acceleration, which is suitable for AR experience on the mobile/web platform. We believed that it could be a promising low-cost AR approach in many areas, such as education and gaming.";Machine Learning with Synthetic Data – a New Way to Learn and Classify the Pictorial Augmented Reality Markers in Real-Time;Not health related;Not health related;0
"A. Bidel; T. Schelo; T. Hamacher";2021;Realistic distribution grid models are essential for the analysis and the evaluation of novel concepts needed for a consequent energy transition. Detailed models of actual power systems are often not available due to security concerns and confidentiality restrictions. In this paper, we propose a grid synthetization procedure based on high resolution spatial datasets released by Dutch Distribution System Operators. We manage to generate calculable radial networks and identify transformers. Subsequently, we dimension all asset types based on a heuristic approach and synthetic peak loads which we derive from the Dutch land register dataset and available statistics. The results are validated based on Complex Network Science and show a high statistical conformity with available data of actual distribution grids.;Synthetic Distribution Grid Generation Based on High Resolution Spatial Data;Not health related;Not health related;0
"P. Movahedi; V. Nieminen; I. M. Perez; T. Pahikkala; A. Airola";2023;The release of differentially private (DP) synthetic data has been proposed as a solution to sharing sensitive individual-level medical data for statistical analysis and machine learning model development. The approach holds promise to generate realistic data that preserves many of the statistical properties of the original data while giving privacy guarantees that bound the risk of leaking any sensitive information about the individuals in the data. However, evaluating the generalization of machine learning models trained on DP-synthetic data remains an open question. A model selected based on its accuracy on synthetic data does not necessarily generalize well to real-world data, leading to poor results and incorrect insights. In this study, we experimentally compare two different protocols for model evaluation and hyperparameter selection for classifiers trained on DP-synthetic medical data. In the first protocol, we use only synthetic data for model selection and final evaluation of selected model, whereas in the second one, we assume limited DP access to a private real validation and test set held by the data curator. Our results provide novel insights into the practical feasibility and utility of different evaluation protocols for classifiers trained on DP synthetic data based on a comprehensive empirical study.;Evaluating Classifiers Trained on Differentially Private Synthetic Health Data;health related;health related;1
"R. McCraith; L. Neumann; A. Vedaldi";2021;Vision is one of the primary sensing modalities in autonomous driving. In this paper we look at the problem of estimating the velocity of road vehicles from a camera mounted on a moving car. Contrary to prior methods that train end-to-end deep networks that estimate the vehicles' velocity from the video pixels, we propose a two-step approach where first an off-the-shelf tracker is used to extract vehicle bounding boxes and then a small neural network is used to regress the vehicle velocity from the tracked bounding boxes. Surprisingly, we find that this still achieves state-of-the-art estimation performance with the significant benefit of separating perception from dynamics estimation via a clean, interpretable and verifiable interface which allows us distill the statistics which are crucial for velocity estimation. We show that the latter can be used to easily generate synthetic training data in the space of bounding boxes and use this to improve the performance of our method further.;Real Time Monocular Vehicle Velocity Estimation using Synthetic Data;Not health related;Not health related;0
"M. S. Hossain; N. Sakib";2020;Renal cell cancer nuclei segmentation from histopathology image is an important step in cancer diagnosis and treatment. However, an automatic cell nuclei segmentation in medical application is a difficult task. Particularly a large amount of annotated data is required for deep learning network. In this work, synthetic but annotate renal cell nuclei data are generated based on non-synthetic data reference. Furthermore, background texture is created according to reference image. The Random shapes of nuclei polygon are created where polygon regions are filled up with foreground nuclei color. The synthetic cell nuclei patches are refined with speeded-up robust feature selection algorithm. The non-synthetic reference patches are considered to assign a score for each synthetic patch where synthetic patches with highest scores are collected. The synthetic patches with corresponding masks are utilized to train U-net segmentation network, which provides better performance than existing methods. In this approach, manual data annotation does not require, which is time-consuming and expensive. The renal cell cancer histopathology data has been collected from Cancer Image Archive, USA which is used to produce non-synthetic reference patches. The proposed framework is validated with Kidney Renal Clear Cell Carcinoma (KIRC) datasets. We have achieved average 0.923 cell nuclei segmentation accuracy from histopathology patches where accuracy provides better segmentation result than other existing methods.;Renal Cell Cancer Nuclei Segmentation from Histopathology Image Using Synthetic Data;health related;health related;1
"B. -F. Wu; L. -W. Chiu; Y. -C. Wu; C. -C. Lai; H. -M. Cheng; P. -H. Chu";2024;Remote photoplethysmography (rPPG) has been used to measure vital signs such as heart rate, heart rate variability, blood pressure (BP), and blood oxygen. Recent studies adopt features developed with photoplethysmography (PPG) to achieve contactless BP measurement via rPPG. These features can be classified into two groups: time or phase differences from multiple signals, or waveform feature analysis from a single signal. Here we devise a solution to extract the time difference information from the rPPG signal captured at 30 FPS. We also propose a deep learning model architecture to estimate BP from the extracted features. To prevent overfitting and compensate for the lack of data, we leverage a multi-model design and generate synthetic data. We also use subject information related to BP to assist in model learning. For real-world usage, the subject information is replaced with values estimated from face images, with performance that is still better than the state-of-the-art. To our best knowledge, the improvements can be achieved because of: 1) the model selection with estimated subject information, 2) replacing the estimated subject information with the real one, 3) the InfoGAN assistance training (synthetic data generation), and 4) the time difference features as model input. To evaluate the performance of the proposed method, we conduct a series of experiments, including dynamic BP measurement for many single subjects and nighttime BP measurement with infrared lighting. Our approach reduces the MAE from 15.49 to 8.78 mmHg for systolic blood pressure (SBP) and 10.56 to 6.16 mmHg for diastolic blood pressure(DBP) on a self-constructed rPPG dataset. On the Taipei Veterans General Hospital(TVGH) dataset for nighttime applications, the MAE is reduced from 21.58 to 11.12 mmHg for SBP and 9.74 to 7.59 mmHg for DBP, with improvement ratios of 48.47% and 22.07% respectively.;Contactless Blood Pressure Measurement Via Remote Photoplethysmography With Synthetic Data Generation Using Generative Adversarial Networks;Not health related;Not health related;0
"H. Daud; R. Razali; V. Asirvadam";2012;The aim of this work is to determine the significant level of parameters such as sediment thickness, frequency and step size for data generated from electromagnetic (EM) simulator for sea bed logging (SBL) application that have been processed using cubic spline interpolation using Analysis of Variance (ANOVA) method. Synthetics data were generated from a simulator that is able to replicate SBL environment. SBL is a new technique that uses controlled source electromagnetic (CSEM) for finding resistive layers or hydrocarbon reservoir in deep water environment. It uses very low frequency to obtain greater wavelength for longer wave penetration. In this work frequencies of 0.5Hz, 0.25Hz, 0.125Hz and 0.0625Hz were used and sediment thicknesses were varied from 1000m to 3000m incremented every 250m. Collected data were interpolated using cubic spline with step sizes variation of 4,8,12 and 16 and mean square error (MSE) were calculated between original and interpolated data. The ANOVA method was applied to these MSE to study any significant different between frequency level and spline step size level at each sediment thickness. It was found from the ANOVA tables that frequency level and step sizes level were having significant difference to the MSE for every sediment thickness below 2250m. For sediment thicknesses of equal to 2250m and above, frequency level is not significance to the MSE but the step sizes level are still significant to the MSE.;Sea bed logging applications: ANOVA analysis 2 for synthetic data from electromagnetic (EM) simulator;Not health related;Not health related;0
"J. Widodo; R. Arief; G. P. Dinanta; N. Setyaningrum; A. Setiyoko; A. P. Putra; A. Oktaviani; Wisyanto; E. W. Santoso; N. Hidayat; Awaluddin; F. Kurniawan; M. H. Pradono; P. Razi; Y. Izumi; J. T. Sri Sumantyo";2022;"Scientists have widely used remote sensing technology to observe surface phenomena, either happened by naturally or artificially. Land subsidence monitoring is one of the promising discoveries made by implementing remote sensing technology, particularly with radar images. The gap in radar image processing technology has been attempted to solve in this study with a real case study in Jakarta, Indonesia. The challenge of simplifying the complex processing step that involved different correction due to time acquisition in radar images have been explained in this study. Aside from that, many researchers believe excessive groundwater extraction and over-exploitation of groundwater can be caused land subsidence, though other factors like land deformation and geological change can accelerate the possibilities. Prior studies have illustrated the subsidence level and methodology to investigate how subsurface change can be indicated by radar image scattering. According to that, this study focused on subsidence escalation in Jakarta. Although other areas can have similar issues, further study will be required. In this study, the Persistent Scatterer Interferometric Synthetic Aperture Radar (PS-InSAR) method has been used in Terra SAR-X multitemporal data; the process strives to search for the indication of land subsidence in the study area. The benefit of this PS-InSAR is its capability to extract point clouds at a particular temporal coherence setting from the SAR image, representing a current surface level. The data has been collected and processed from 29 scenes from 2017 to 2021, thus showing the changes over the last four years. Analysis during that period found that land subsidence existed at a high rate, compared with 2017 data, perceived that in 2021 subsidence level was cumulatively around 9.85 cm in the Pantai Indah Kapuk, North Jakarta area, and 5.63 cm in the Kembangan area of West Jakarta. A ground check is needed to ensure this strongly connects with geological factors. Still, PS-InSAR can be a beneficial tool for the surface modelling in a broad area and rapid mapping for geoscience analysis.";Time Series Land Subsidence Analysis Based on Persistent Scattered Interferometric Synthetic Aperture Radar Method of Jakarta City Region Using Terra SAR X Spaceborne Data;Not health related;Not health related;0
"S. Basak; F. Khan; R. McDonnell; M. Schukat";2021;Accurate 3D head pose estimation from a 2D image frame is an essential component of modern consumer technology (CT). It enables a better determination of user attentiveness and engagement and can support immersive audio and AR experiences. While deep learning methods have improved the accuracy of head pose estimation models, these depend on the accurate annotation of training data. The acquisition of real-world head pose data with a large variation of yaw, pitch and roll is a very challenging task. Available head-pose datasets often have limitations in terms of the number of data samples, image resolution, annotation accuracy and sample diversity (gender, race, age). In this work, a rendering pipeline is proposed to generate pixel-perfect synthetic 2D headshot images from high-quality 3D facial models with accurate pose angle annotations. A diverse range of variations in age, race, and gender are provided. The resulting dataset includes more than 300k pairs of RGB images with the corresponding head pose annotations. For every hundred 3D models there are multiple variations in pose, illumination and background. The dataset is evaluated by training a state-of-the-art head pose estimation model and testing against the popular evaluation dataset BIWI. The results show training with purely synthetic data produced by the proposed methodology can achieve close to state-of-the-art results on the head pose estimation task and is better generalized for age, gender and racial diversity than solutions trained on ‘real-World’ datasets.;Learning Accurate Head Pose for Consumer Technology From 3D Synthetic Data;Not health related;Not health related;0
"Y. Kokubo; S. Asada; H. Maruyama; M. Koide; K. Yamamoto; Y. Suetsugu";2021;We simulated the exact features of raindrops on a camera lens and conducted an experiment to evaluate the performance of a network trained to remove raindrops using synthetic raindrop data. Although research has been conducted to precisely evaluate methods to remove raindrops, with some evaluation networks trained on images with real raindrops and others trained on images with synthetic raindrops, there have not been any studies that have directly compared the performance of two networks trained on each respective kind of image. In a previous study wherein images with synthetic raindrops were used for training, the network did not work effectively on images with real raindrops because the shapes of the raindrops were simulated using simple arithmetic expressions. In this study, we focused on generating raindrop shapes that are closer to reality with the aim of using these synthetic raindrops in images to develop a technique for removing real-world raindrops. After categorizing raindrops by type, we further separated each raindrop type into its constituent elements, generated each element separately, and finally combined the generated elements. The proposed technique was used to add images with synthetic raindrops to the training data, and when we evaluated the model, we confirmed that the technique's precision exceeded that of when only images with actual raindrops were used for training. The evaluation results proved that images with synthetic raindrops can be used as training data for real-world images.;Removing Raindrops from a Single Image using Synthetic Data;Not health related;Not health related;1
"P. Karamolegkos; A. Kiourtis; A. Karabetian; K. Voulgaris; Y. Poulakis; A. Mavrogiorgou; M. Filippakis; D. Kyriazis";2023;The use of systems to calculate mathematical operations has facilitated people to automate processes in the corporate sector. Such systems lie behind everything from calculating the final amount on a grocery receipt, to complex mathematical operations involving finding behaviors in a business's customers. However, when a company or organization has a large amount of data on which to perform mathematical operations, the procedure becomes time-consuming, whereas to execute mathematical operations on entire datasets, one typically needs strong programming skills. In this paper, a service called MathBlock is analyzed that is able to be used as a language agnostic mathematical expression parser and executioner, on batch data. MathBlock consists of four types of functions, including arithmetic, comparison, logical, and statistical. To evaluate the applicability of MathBlock, an experiment is carried out on the mentioned service as a proof of concept. This experimentation uses batch and synthetic data, covering the domains of maritime and healthcare, with the aim of performing mathematical operations through MathBlock. The derived results showcase that MathBlock can assist users on their need to calculate and gather results for many different datasets. Overall, it can be clearly stated that through MathBlock the challenge of the need to perform arithmetic, logical, comparison and statistical operations on different datasets to get results in an automated manner is well addressed, whereas additional experimentation with datasets from multiple domains should take place in order to conclude to more concrete and reliable results.;MathBlock: Performing Complex Mathematical Operations on Synthetic Data;Not health related;Not health related;0
"D. Ma; W. -C. Huang; T. Toda";2021;"Sequence-to-sequence (seq2seq) voice conversion (VC) can achieve high-quality VC performance with a parallel dataset, but it is still challenging when the parallel dataset is not available. One way to overcome this issue is using synthetic parallel data (SPD) produced by text-to-speech (TTS) models trained with source and target speakers' voices in an available VC dataset. In this paper, we conduct a systematic study on the effects of SPD on seq2seq VC performance. Some factors, such as a comparison of SPD by source or target speaker, the effects of SPD in a semiparallel setting including a parallel subset, and the usage of SPD with external text data are investigated. The results show the effects of SPD depend on TTS performance and VC training datasize; i.e., 1) when the datasize is small, causing SPD quality degradation as the resulting TTS performance is limited, the training pair containing source SPD and target natural speech tends to yield slightly better VC results than that containing source natural speech and target SPD, 2) although SPD makes it possible to use a nonparallel dataset, using parallel subset is still effective, and 3) SPD with external text data as data augmentation can improve parallel seq2seq VC performance.";Investigation of Text-to-Speech-based Synthetic Parallel Data for Sequence-to-Sequence Non-Parallel Voice Conversion;Not health related;Not health related;0
"Y. Assayag; H. Oliveira; E. Souto; R. Barreto; R. Pazzi";2021;Indoor Positioning Systems (IPSs) are used to estimate the position of mobile devices in indoor environments. Fingerprinting is the most used technique because of its higher accuracy. However, this technique requires a labor-intensive training phase that measures the Received Signal Strength Indicator (RSSI) at all Reference Points (RPs) locations. On the other hand, model-based IPSs use signal propagation models to estimate distances from RSSI. Thus, they do not require expensive training but result in higher positioning errors. In this work, we propose SynTra-IPS (Synthetic Training Indoor Positioning System), a hybrid approach between a fingerprint and a model-based IPS that uses synthetic, simulated datasets combined with data fusion techniques to eliminate the fingerprint collection cost. In our solution, we use the map of the scenario, with known anchor nodes’ positions and the log-distance signal propagation model, to generate several synthetic, model-based, fingerprint training datasets. In the online phase of our solution, the positions estimated by the several synthetic datasets using K-Nearest Neighbors (KNN) are combined using data fusion techniques into a single, more accurate position. We evaluated the performance of our SynTra solution in a real-world, large-scale environment using mobile devices with Bluetooth Low Energy (BLE) technology, and we compared our solution to classic approaches from the literature. Our results show that SynTra can locate mobile devices with an average error of only 2.36 m while requiring no real-world environment training.;Indoor Positioning System Using Synthetic Training and Data Fusion;Not health related;Not health related;0
"W. Maeda; Y. Higuchi; K. Minami; I. Morikawa";2022;When publishing a machine learning model trained with personal data, the privacy threats, such as membership inference attacks and model inversion, of information leakage on training data from the model should be considered. This paper explores the relations between the resistance against membership inference attack and the mixing ratio of a partially synthetic dataset, which is a mixture of a fully synthetic dataset and the original dataset. In our experiments, we considered the membership inference attack and adopted perturbation using the post-randomization method to generate synthetic data. We confirmed that models trained with partially perturbed datasets could achieve an attack resistance comparable to that of fully perturbed datasets. Conversely, the prediction accuracy of the models could be retained more effectively as compared to the models with the fully perturbed whose accuracy degraded to random guess level. The experiments also suggest that a high mixing ratio, i.e., only a small portion of the original dataset is retained, can potentially contribute to the sustaining prediction accuracy. This suggestion can provide a potential guideline about the tuning of partially synthetic data to the field of membership inference countermeasure using synthetic data.;Membership Inference Countermeasure With A Partially Synthetic Data Approach;Not health related;Not health related;0
"R. Techentin; D. Foti; S. Al-Saffar; P. Li; E. Daniel; B. Gilbert; D. Holmes";2014;Over the past decade, the use of semantic databases has served as the basis for storing and analyzing complex, heterogeneous, and irregular data. While there are similarities with traditional relational database systems, semantic data stores provide a rich platform for conducting non-traditional analyses of data. In support of new graph analytic algorithms and specialized graph analytic hardware, we have developed a large semi-synthetic, semantically rich dataset. The construction of this dataset mimics the real-world scenario of using relational databases as the basis for semantic data construction. In order to achieve real-world variable distributions and variable dependencies, data.gov data was used as the basis for developing an approach to build arbitrarily large semi-synthetic datasets. The intent of the semi-synthetic dataset is to serve as a testbed for new semantic graph analyses and computational software/hardware platforms. The construction process and basic data characterization is described. All code related to the data collection, consolidation, and augmentation are available for distribution.;Characterization of semi-synthetic dataset for big-data semantic analysis;Not health related;Not health related;0
"I. A. S. Hokmabadi; M. Ai; C. Minaretzis; M. Sideris; N. El-Sheimy";2023;Pose detection of objects is an important topic in object-level mapping and indoor localization. In the past, pose estimation methods were performed either with the help of artificial markers or natural features found on the object. However, due to the fact that the markers can only be utilized in controlled environment experiments, the application of marker-based approaches is very limited. Furthermore, methods that depend on the object's natural visual features require texture on the object and lack robustness to illumination and camera viewpoint variations. With the advent of Deep Learning (DL), the classical pose estimation methods have been outperformed. The DL-based pose estimation can detect deep features of the object and exhibits higher robustness to many distortions and variabilities caused by the changes in the illumination and viewpoint conditions. However, the massive training data set requirement is the main challenge with most DL-based methods. The training set is often a real set of images that have been manually labeled or annotated. In addition, such methods face problems related to the degradation of their predicted accuracy in the presence of uncertainties due to the symmetrical structure of many objects. To address the aforementioned issues, a novel and very fast method for generating synthetic data, as well as a contour-based technique for accurate pose estimation (that can handle pose ambiguities for a symmetrical object) are proposed in this paper. The tests that are conducted in multiple indoor scenarios demonstrate not only the effectiveness of the synthetic data generation but also exhibit, in many cases, the very high accuracy of the proposed pose estimation method.;Accurate and Scalable Contour-based Camera Pose Estimation Using Deep Learning with Synthetic Data;Not health related;Not health related;0
"S. Schmerwitz; N. Peinecke; H. -U. Dohler; B. Korn";2008;Enhanced vision systems (EVS) are a possibility to improve the situation awareness of an aircrew during poor visibility conditions. EVS are based on sensor data which might be difficult to interpret especially for radar data. Fast scanning radar systems in the millimeter waveband (35 or 94 GHz) are commonly unable to measure the elevation of a target. Nevertheless these elevation data can sometimes be reconstructed from a series of images or from images taken from different viewpoints or bank angles. In case of forward looking millimeter wave radar it is more promising to use different bank angles. The authors have detailed these ideas using the term ldquoStereo Radarrdquo in previous publications. In this paper we take a closer look at the accuracy and the resolution of the algorithm. For this a series of experiments using synthetic data is performed. Furthermore we show the influence of size, elevation, shape, and different (radar) textures of a target on the reconstructed elevation. Finally some tests are carried out to demonstrate the robustness against different kinds of noise.;Evaluation of a “Stereo” Radar approach for terrain reconstruction using synthetic data;Not health related;Not health related;0
"I. Joshi; M. Grimmer; C. Rathgeb; C. Busch; F. Bremond; A. Dantcheva";2024;Deep neural networks have become prevalent in human analysis, boosting the performance of applications, such as biometric recognition, action recognition, as well as person re-identification. However, the performance of such networks scales with the available training data. In human analysis, the demand for large-scale datasets poses a severe challenge, as data collection is tedious, time-expensive, costly and must comply with data protection laws. Current research investigates the generation of synthetic data as an efficient and privacy-ensuring alternative to collecting real data in the field. This survey introduces the basic definitions and methodologies, essential when generating and employing synthetic data for human analysis. We conduct a survey that summarises current state-of-the-art methods and the main benefits of using synthetic data. We also provide an overview of publicly available synthetic datasets and generation models. Finally, we discuss limitations, as well as open research problems in this field. This survey is intended for researchers and practitioners in the field of human analysis.;Synthetic Data in Human Analysis: A Survey;Not health related;Not health related;0
"C. Bartz; L. Seidel; D. -H. Nguyen; J. Bethge; H. Yang; C. Meinel";2020;Archives contain a wealth of information and are invaluable for historical research. Thanks to digitization, many archives are preserved in a digital format, making it easier to share and access documents from an archive. Handwriting and handwritten notes are commonly found in archives and contain a lot of information that can not be extracted by analyzing documents with Optical Character Recognition (OCR) for printed text. In this paper, we present an approach for determining whether a scan of a document contains handwriting. As a preprocessing step, this approach can help to identify documents that need further analysis with a full recognition pipeline. Our method consists of a deep neural network that classifies whether a document contains handwriting. Our method is designed in such a way that we overcome the most significant challenge when working with archival data, which is the scarcity of annotated training data. To overcome this problem, we introduce a data generation method to successfully train our proposed deep neural network. Our experiments show that our model, trained on synthetic data, can achieve promising results on a real-world dataset from an art-historical archive.;Synthetic Data for the Analysis of Archival Documents: Handwriting Determination;Not health related;Not health related;0
"J. M. Malof; S. Chelikani; L. M. Collins; K. Bradbury";2017;We consider the problem of detecting objects (such as trees, rooftops, roads, or cars) in remote sensing data including, for example, color or hyperspectral imagery. Many detection algorithms applied to this problem operate by assigning a decision statistic to all, or a subset, of spatial locations in the imagery for classification purposes. In this work we investigate a recently proposed method, called Local Averaging for Improved Predictions (LAIP), which can be used for trading off the classification accuracy of detector decision statistics with their spatial precision. We explore the behaviors of LAIP on controlled synthetic data, as we vary several experimental conditions: (a) the difficulty of the detection problem, (b) the spatial area over which LAIP is applied, and (c) how it behaves when the estimated ROC curve of the detector becomes increasingly inaccurate. These results provide basic insights about the conditions under which LAIP is effective.;Trading spatial resolution for improved accuracy in remote sensing imagery: an empirical study using synthetic data;Not health related;Not health related;0
"Z. Shi; Z. Liu; L. Liu; R. Liu; T. Yamamoto; X. Mi; D. Uchida";2023;In this paper, we propose a method called CheckSORT for automatic retail checkout. We demonstrate CheckSORT on the multi-class product counting and recognition task in Track 4 of AI CITY CHALLENGE 2023. This task aims to count and identify products as they move along a retail checkout white tray, which is challenging due to occlusion, similar appearance, or blur. Based on the constraints and training data provided by the sponsor, we propose two new ideas to solve this task. The first idea is to design a controllable synthetic training data generation paradigm to bridge the gap between training data and real test videos as much as possible. The second innovation is to improve the efficiency of existing SORT tracking algorithms by proposing decomposed Kalman filter and dynamic tracklet feature sequence. Our experiments resulted in state-of-the-art (when compared with DeepSORT and StrongSORT) F1-scores of 70.3% and 62.1% on the TestA data of AI CITY CHALLENGE 2022 and 2023 respectively in the estimation of the time (in seconds) for the product to appear on the tray. Training and testing code will be available soon on github.;CheckSORT: Refined Synthetic Data Combination and Optimized SORT for Automatic Retail Checkout;Not health related;Not health related;0
"J. Sánchez; C. W. Walter; H. Awan; J. Chiang; S. F. Daniel; E. Gawiser; T. Glanzman; D. Kirkby; R. Mandelbaum; A. Slosar; W. M. Wood-Vasey; Y. AlSayyad; C. J. Burke; S. W. Digel; M. Jarvis; T. Johnson; H. Kelly; S. Krughoff; R. H. Lupton; P. J. Marshall; J. R. Peterson; P. A. Price; G. Sembroski; B. Van Klaveren; M. P. Wiesner; B. Xin; The LSST Dark Energy Science Collaboration";2020;"Data Challenge 1 (DC1) is the first synthetic data set produced by the Rubin Observatory Legacy Survey of Space and Time (LSST) Dark Energy Science Collaboration (DESC). DC1 is designed to develop and validate data reduction and analysis and to study the impact of systematic effects that will affect the LSST data set. DC1 is comprised of r-band observations of 40 deg2 to 10_yr LSST depth. We present each stage of the simulation and analysis process: (a) generation, by synthesizing sources from cosmological N-body simulations in individual sensor-visit images with different observing conditions; (b) reduction using a development version of the LSST Science Pipelines; and (c) matching to the input cosmological catalogue for validation and testing. We verify that testable LSST requirements pass within the fidelity of DC1. We establish a selection procedure that produces a sufficiently clean extragalactic sample for clustering analyses and we discuss residual sample contamination, including contributions from inefficiency in star–galaxy separation and imperfect deblending. We compute the galaxy power spectrum on the simulated field and conclude that: (i) survey properties have an impact of 50 per_cent of the statistical uncertainty for the scales and models used in DC1; (ii) a selection to eliminate artefacts in the catalogues is necessary to avoid biases in the measured clustering; and (iii) the presence of bright objects has a significant impact (2_–6_) in the estimated power spectra at small scales (_ > 1200), highlighting the impact of blending in studies at small angular scales in LSST.";The LSST DESC data challenge 1: generation and analysis of synthetic images for next-generation surveys;Not health related;Not health related;0
"K. Park; H. Park; H. Bae";2022;To manufacture synthetic rubber, rubber manufacturers require optimal recipes to ensure that it satisfies the required quality standards. Several experiments are required to create the optimal recipe, which adversely affects not only the cost and time required but also the health of workers. Suppose the experimental results can be predicted in advance at the recipe design stage before direct experimentation. In that case, the cost of the experiment can be reduced, and the workers’ health can be significantly less impacted. For this purpose, a method called the prediction walk model using a machine learning model was developed to generate the temperature trajectory in a kneading machine. A cross-updating method to predict the quality of the kneading operation is also proposed. From the results of the experiment, it was confirmed that the performance of the proposed models was superior to that of the existing prediction models.;A Data-Driven Recipe Simulation for Synthetic Rubber Production;Not health related;Not health related;0
"M. E. Fathy; S. A. Mohamed; M. I. Awad; H. E. A. E. Munim";2023;Underwater computer vision applications are challenged by limited access to annotated underwater datasets. Additionally, convolutional neural networks (CNNs) trained on in-air datasets do not perform well underwater due to the high domain variance caused by the degradation impact of the water column. This paper proposes an air-to-water dataset generator to create visually plausible underwater scenes out of existing in-air datasets. SubmergeStyleGAN, a generative adversarial network (GAN) designed to model attenuation, backscattering, and absorption, utilizes depth maps to apply range-dependent attenuation style transfer. In this work, the generated attenuated images and their corresponding original pairs are used to train an underwater image enhancement CNN. Real underwater datasets were used to validate the proposed approach by assessing various image quality metrics, including UCIQE, UIQM and CCF, as well as disparity estimation accuracy before and after enhancement. SubmergeStyleGAN exhibits a faster and more robust training procedure compared to existing methods in the literature.;SubmergeStyleGAN: Synthetic Underwater Data Generation with Style Transfer for Domain Adaptation;Not health related;Not health related;0
"S. Choenni; T. Busker; M. S. Bargh";2023;"Data collection for studying social phenomena is not only costly but is also, at best, a time-consuming and tedious task. Therefore, tools that may ease the task of data collection will speed up these studies and improve their efficiency. In this contribution, we argue that in some cases Large Language Models (LLMs) may serve as a tool to generate data for studying social phenomena. The rationale is that LLMs absorb a vast amount of data from various types and sources; and embed (an abstraction of) the data in models. Querying these models generates synthetic data that can be considered as a good approximation of the data on which they are trained. The methodological and practical issues involved in our rationale are discussed in this paper. By means of a use case, we illustrate how synthetic data can be generated (or collected) from GPT and how the data can be used for studying stereotypical views on social groups.";Generating Synthetic Data from Large Language Models;Not health related;Not health related;0
"H. O. Shahreza; S. Marcel";2023;In this paper, we use synthetic data and propose a new method to reconstruct high-resolution face images from facial templates in a template inversion attack against face recognition systems. We use a pre-trained face generator network to generate synthetic face images, and then learn a mapping from the facial templates to the intermediate latent space of the face generator network. We train our mapping network with a multi-term loss function. During the inference stage, we use our mapping network to map facial templates to the intermediate latent code and then generate high-quality face images using the face generator network. We propose our method for whitebox and blackbox template inversion attacks against face recognition systems. We use our model (trained on synthetic data) to evaluate the vulnerability of state-of-the-art face recognition systems on real face datasets, including Labeled Faces in the Wild (LFW) and MOBIO datasets. Experimental results show the vulnerability of the state-of-the-art face recognition system to our template inversion attack. Our experiments also show that our template inversion method outperforms previous methods in the literature. The source code of our experiments is publicly available to facilitate reproducibility of our work.;Inversion of Deep Facial Templates using Synthetic Data;Not health related;Not health related;0
"R. Hernández-García; E. H. Salazar-Jurado; R. J. Barrientos; F. M. Castro; J. Ramos-Cózar; N. Guil";2023;Palm vein recognition has relevant advantages in comparison with most traditional biometrics, such as high security and recognition performance. In recent years, CNN-based models for vascular biometrics have improved the state-of-the-art, but they have the disadvantage of requiring a larger number of samples for training. In this context, the generation of synthetic databases is very effective for evaluating the performance of biometric systems. The present study proposes a new perspective of a transfer learning approach for palm vein recognition, evaluating the use of Synthetic-sPVDB and NS-PVDB synthetic databases for pre-training deep learning models and validating their performance on real databases. The proposed methodology comprises two different branches as inputs. Firstly, a synthetic database is used to train a CNN model, and in the second branch, a real database is used to finetune and evaluate the performance of the resulting pre-trained model. For the feature learning process, we implemented two end-to-end CNN architectures based on AlexNet and Resnet32. The experimental results on the most representative public datasets have shown the usefulness of using palm vein synthetic images for transfer learning, outperforming the state-of-the-art results.;From Synthetic Data to Real Palm Vein Identification: a Fine-Tuning Approach;Not health related;Not health related;0
"C. Pschierer; J. Schiefele; D. Howland; N. Barraci; M. Meuter; A. Sindlinger; U. Klingauf";2006;During the last years Jeppesen has developed digital terrain, obstacle and airport databases as well as different electronic displays as part of the NASA aviation safety program. This paper describes the continuation of this work, which is now focused on a completely dynamic channel depiction of navigation procedures inside a SVS display. A human factors workshop has been conducted to identify the pilot's expectations and requirements for channel guidance. Pilots from the GA, BA and CA segment and charting experts participated in the workshop. The workshop covered three main topics of the program. A general information and task analysis revealed what information the pilots need while flying a procedure. The two other sessions dealt with the generation of the channel trajectory and the depiction of the channel trajectory;A Dynamic Channel Depiction of Navigation Data in Synthetic Vision Displays;Not health related;Not health related;0
"V. Wolf; F. Neubürger; R. Lanwehr";2023;The changing world of work brings with it massive changes. However, it is largely unclear what exactly these changes will look like in terms of the skills required. In order to close part of this knowledge gap, we collected, classified, and analyzed 12.66 million job advertisements published online in Germany from 2014 to 2020 in the fields of administration & customer care, human resources, marketing & sales, and information technology. We specifically focus on the ability to evaluate skills in higher-level clusters and how these develop in different industries and company sizes over time. We find major differences in the speed of adjustment between the professional fields. In addition, we cluster industries according to their demand and development over time for specific skills. This allows industries to be identified in which the demand for skills is much alike. Companies that are a part of a particular cluster might therefore specifically look for employees from other companies in the same cluster, as their overall skill requirements are similar. In addition, we train a Long Short-Term Memory (LSTM) predictor with synthetically generated data using a Time-Generative-Adversarial-Network (TimeGAN) on real validation and test data, which outperforms the model trained only with real data. For the training, time series of the demand of entire industries is aggregated into single time series, which further emphasizes the benefit of generating synthetic data in applications with a small subset of data.;Generating Synthetic Data for Better Prediction Modeling in Skill Demand Forecasting;Not health related;Not health related;0
"A. Dadgar; G. Brunnett";2020;We propose a specific formation of synthetic hand images, to train a fully convolutional neural network, for detecting hands in real scenarios. Our methodology aims to achieve this by further exploiting the invariancy concept of these networks. Thus the conventional, and expensive, techniques of domain randomization are not employed to create realistic synthetic images. We train the networks using two types of training images: 1. purely synthetic, and 2. with a combination of a few (100) real images. The results suggest our simplistic and sorely cheap approach of generating synthetic hand could successfully be utilized to detect hands in challenging scenarios.;SaneNet: Training a Fully Convolutional Neural Network Using Synthetic Data for Hand Detection;Not health related;Not health related;0
"K. Hydock; A. Elliott; M. Busch; L. Lipchak; D. Blair; J. Chapman";2023;"Computer vision has the potential to accelerate decision support in a variety of medical applications, but a paucity of high-quality, open-source datasets hinders the development of decision support applications for open medical environments (i.e., non-laparoscopic). Synthetic data holds promise as a solution for difficult-to-obtain data, such as images from high stress environments or complicated by privacy concerns associated with medical imagery. Modern applications of synthetic data generation such as digital twins (DT) can create high-fidelity and photo-realistic images the surpass traditional data augmentation practices. Moreover, synthetic data is cost-effective, efficient, and highly scalable after initial creation of the assets and environment. This study presents a framework to synthetically generate annotated images from digital 3-D assets in random perspectives and orientations for use in computer-vision (CV) for open medical applications. Datasets of 1,000 annotated synthetic images and 681 real-world images of six object classes were employed in a transfer learning toolkit for an experiment to determine the feasibility of utilizing synthetic data to train models on CV applications to augment or replace training with real-world data. The experiment tested model performance based on three datasets: 1. Synthetic data only; 2. Real-world data only; 3. Training with synthetic data for evaluation with real-world data. Results showed that training on synthetic data to evaluate real-world data met or exceeded the performance of training on real-world data in four of six classes within the datasets utilized. Results show promise for utilizing synthetic data as an alternative to costly, time consuming, and difficult to obtain data types with many areas for further study, such as more detailed and comprehensive environments and assets as well as methodology for noise injection to improve model performance.";Generation of Synthetic Data for Medical Decision Support Applications;health related;health related;1
"M. S. Akter; H. Shahriar; A. Cuzzocrea; F. Wu; J. Rodriguez-Cardenas";2023;Social media cyberbullying has a detrimental effect on human life. As online social networking grows daily, the amount of hate speech also increases. Such terrible content can cause depression and actions related to suicide. This paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection on social media using synthetic data. We have demonstrated a cutting-edge method to address data availability difficulties by producing machine-translated data. However, several languages such as Hindi and Bangla still lack adequate investigations due to a lack of datasets. We carried out experimental identification of aggressive comments on Hindi, Bangla, and English datasets using the proposed model and traditional models, including Long Short-Term Memory (LSTM), Bidirectional Long ShortTerm Memory (BiLSTM), LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models. We employed evaluation metrics such as f1-score, accuracy, precision, and recall to assess the models’ performance. Our proposed model outperformed all the models on all datasets, achieving the highest accuracy of 95%. Our model achieves state-of-the-art results among all the previous works on the dataset we used in this paper.;A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data;Not health related;Not health related;0
"F. Specht; J. Otto; D. Ratz";2023;Machine learning based security monitoring can be used to detect cyberattacks and malfunctions in cyber-physical production systems. Acquiring real data sets for training machine learning algorithms is a problem due to high costs, low data quality, data diversity, and the violation of privacy policies. This paper introduces CyberSyn, a novel approach to generate synthetic data sets for machine learning based security monitoring systems. The generated data sets are analyzed using data quality metrics. Two scenarios from process manufacturing and industrial communication networks are used to evaluate the introduced approach. The proposed approach is able to generate synthetic data sets for both scenarios.;Generation of Synthetic Data to Improve Security Monitoring for Cyber-Physical Production Systems;Not health related;Not health related;0
"B. Xu; S. Lang; X. Cui; L. Li; X. Liu; J. Guo; B. Sun";2022;The spatial correlation of ice-sounding data can be used to trace internal isochronic layers and synchronize the age–depth relationship between different ice core sites, which is difficult using existing imaging methods. In this study, we propose a new algorithm to address the opportunity that applying spatial correlation to ice-sheet imaging offers. The algorithm is a spatial-correlation-based ice-sounding imaging method using fast back projection (FBP) that successfully improves the spatial correlation of imaging results with high efficiency. We give the specific steps to implement the algorithm and apply it to simulate both point targets and ice-sounding radar data to demonstrate its validity in imaging ice sheets. Furthermore, compared with two previous methods, the proposed algorithm improves the spatial correlation without degrading the ability of signal-to-noise ratio (SNR) improvement and processing efficiency.;Focused Synthetic Aperture Radar Processing of Ice-Sounding Data Collected Over East Antarctic Ice Sheet via Spatial-Correlation-Based Algorithm Using Fast Back Projection;Not health related;Not health related;0
"C. Zhang; Q. Yang; L. Fan; S. Yu; L. Sun; C. Cai; X. Ding";2023;The generation of synthetic data using physics-based modeling provides a solution to limited or lacking real-world training samples in deep learning methods for rapid quantitative magnetic resonance imaging (qMRI). However, synthetic data distribution differs from real-world data, especially under complex imaging conditions, resulting in gaps between domains and limited generalization performance in real scenarios. Recently, a single-shot qMRI method, multiple overlapping-echo detachment imaging (MOLED), was proposed, quantifying tissue transverse relaxation time (T2) in the order of milliseconds with the help of a trained network. Previous works leveraged a Bloch-based simulator to generate synthetic data for network training, which leaves the domain gap between synthetic and real-world scenarios and results in limited generalization. In this study, we proposed a T2 mapping method via MOLED from the perspective of domain adaptation, which obtained accurate mapping performance without real-label training and reduced the cost of sequence research at the same time. Experiments demonstrate that our method outshined in the restoration of MR anatomical structures.;Towards Better Generalization Using Synthetic Data: A Domain Adaptation Framework for T2 Mapping via Multiple Overlapping-Echo Acquisition;Not health related;Not health related;0
"N. Niederhametner; R. Mayer";2023;With the ever-increasing amount of data collected, there is also an increased demand for data analysis and machine learning methods, which are consequently frequently deployed. However, many of the data collected are very sensitive and of a personal nature – thus, data confidentiality and privacy become important considerations. In the wake of this, the use of synthetic data as a privacy-preserving measure for micro-data is gaining more and more popularity, especially due to its ability to maintain a high level of data utility. Synthetic data is artificially generated by a model that has been trained on real data. This means that the observations in the synthetic data do not directly correspond to any individual in the original dataset. While there are many tools for creating synthetic data available, only a little research has focused on specifically treating sensitive attributes and generating synthetic data in a way that concentrates on protecting these selected attributes from inference attacks while keeping the data utility as high as possible. This can be achieved done by setting certain constraints when learning the model from the original data. Earlier work proposed a modification to extend the DataSynthesizer, an approach for synthetic data generation that uses Bayesian Networks to capture the underlying structures in the original data, to protect one sensitive attribute. In this paper, we investigate two different techniques for extending this approach to protect multiple attributes from inference and analyse the subsequent effects on the data utility.;Protecting Multiple Sensitive Attributes in Synthetic Micro-data;Not health related;Not health related;1
"X. Chen; C. Ye; Y. Wang; Y. Dai; Q. Hu";2023;Inverse synthetic aperture radar imaging is an important technique for radar detection and recognition of non-cooperative targets. However, for most of the existing imaging methods, pulse signals are transmitted and received at equal intervals, without considering the possible problems under condition of non-uniform data rate, such as accuracy decline of echo's range alignment and high side lobes in Doppler spectrum. The resulting vertical and horizontal defocusing can reduce the imaging quality and affect the feature extraction of the target. Therefore, an inverse synthetic aperture radar imaging method for non-uniform data rate is proposed. First, by designing the weight to simulate the alignment situation of uniform data rate, the correlation method with weighted sliding window is used to solve the problems of declined range alignment accuracy and vertical defocusing. Then, based on the linear form of non-uniform Fourier transform, the Fourier base matrix of non-uniform data rate is built, and CLEAN technology is used to complete the projection of echo signal on the bases iteratively. Finally, the Fourier basis matrix with uniform data rate is used to reconstruct the echo signal, which solves the problem of horizontal defocusing and makes the data rate of echo signal uniform. The effectiveness of the proposed method is verified by experiments using measured data.;Imaging Algorithm for Inverse Synthetic Aperture Radar in Condition of Non-Uniform Data Rate;Not health related;Not health related;0
"P. Sarah; A. V. Anne; Y. Karuna; K. Karthik; S. K. Priya; S. Saladi";2023;Early brain tumor identification is one of the most critical challenges that neurologists and radiologists face. Effective segmentation and classification still need to be improved despite numeroussignificant efforts and encouraging outcomes. Images of various sorts are employed for tumor segmentation, categorization, and diagnosis. For a noninvasive and more accurate image of the interior anatomy of the tumor, magnetic resonance imaging (MRI) is preferred above all other imaging techniques. Nevertheless, manually identifying and differentiating brain tumors from magnetic resonance Imaging (MRI) scans are difficult and error-prone, and it calls for the need for an automated brain tumor detection system for early tumordetection. This study suggests a deep-learning approach for analyzing MRI data in order to detect brain tumors. The suggested approach comprises three key phases: pre-processing, segmentation, adopting k means clustering, and finally, tumor classification lastly MRI data using a customized VGG19 (19 layered Visual Geometric Group) model. Besides that, the synthetic data augmentation idea is adopted to enhance the amount of data accessible for classifier training to improve classification accuracy. The outcomes support the efficacy of the suggested strategy and demonstrate that it is more accurate than already available methodologies.;Brain Tumor Detection Using Deep Learning With Synthetic Data Augmentation;health related;Not health related;1
"M. Boedihardjo; T. Strohmer; R. Vershynin";2023;Privacy-preserving data analysis is emerging as a challenging problem with far-reaching impact. In particular, synthetic data are a promising concept toward solving the aporetic conflict between data privacy and data sharing. Yet, it is known that accurately generating private, synthetic data of certain kinds is NP-hard. We develop a statistical framework for differentially private synthetic data, which enables us to circumvent the computational hardness of the problem. We consider the true data as a random sample drawn from a population  $\Omega $  according to some unknown density. We then replace  $\Omega $  by a much smaller random subset  $\Omega ^{\ast}$ , which we sample according to some known density. We generate synthetic data on the reduced space  $\Omega ^{\ast}$  by fitting the specified linear statistics obtained from the true data. To ensure privacy we use the common Laplacian mechanism. Employing the concept of Rényi condition number, which measures how well the sampling distribution is correlated with the population distribution, we derive explicit bounds on the privacy and accuracy provided by the proposed method.;Privacy of Synthetic Data: A Statistical Framework;Not health related;Not health related;0
"P. J. Bentley; S. L. Lim; S. Jindal; S. Narang";2021;Machine Learning has the potential to discover new correlations between energy usage in apartments and variables such as seasonality, apartment location, size, efficiency and details of those staying in the apartments, thus helping apartments to become more sustainable and helping those who stay in them to use less energy. The biggest impedance to creating such ML tools is lack of viable data - without the data, the tools cannot be created - yet it is not feasible to wait for several years' worth of good data before creating the tools. Here we present a solution to this problem: the use of a digital twin to generate synthetic data. This approach is viable even when there is no existing data, but when expert knowledge about the relationship between systems exist. To achieve this, we develop a new agent-based synthetic data generator (ASDG) and explore a case study with a corporate housing and luxury alternate accommodation marketplace called TheSqua.re. We show that unlimited quantities of realistic data can be automatically generated, including data for different scenarios, and that it can be used by Machine Learning to discover the underlying correlations.;Generating Synthetic Energy Usage Data to Enable Machine Learning for Sustainable Accommodation;Not health related;Not health related;0
"C. Li; J. Li; Y. Dai; T. Yang; Y. Xie; Z. Lu";2019;Synthetic aperture imaging, which has been proved to be an effective approach for occluded object imaging, is one of the challenging problems in the field of computational imaging. Currently most of the related researches focus on fixed synthetic aperture which usually accompanies with mixed observation angle and foreground de-focus blur. But the existence of them is frequently a source of perspective effect decrease and occluded object imaging quality degradation. In order to solve this problem, we propose a novel data-driven variable synthetic aperture imaging based on semantic feedback. The semantic content we concerned for better de-occluded imaging is the foreground occlusions rather than the whole scene. Therefore, unlike other methods worked on pixel-level, we start from semantic layer and present a semantic labeling method based on feedback. Semantic labeling map deeply mines visual data in synthetic image and preserves the semantic information of foreground occluder. On the basis of semantic feedback strategy, semantic labeling map will conversely pass to synthetic imaging process. The proposed data-driven variable synthetic aperture imaging contains two levels: one is adaptive changeable imaging aperture driven by synthetic depth and perspective angle, the other is light ray screening driven by visual information in semantic labeling map. On this basis, the hybrid camera view and superimposition of foreground occlusion can be removed. Evaluations on several complex indoor scenes and real outdoor environments demonstrate the superiority and robustness performance of our proposed approach.;Data-Driven Variable Synthetic Aperture Imaging Based on Semantic Feedback;Not health related;Not health related;0
"N. Ashrafi; V. Schmitt; R. P. Spang; S. Möller; J. -N. Voigt-Antons";2023;Preservation of private user data is of paramount importance for high Quality of Experience (QoE) and acceptability, particularly with services treating sensitive data, such as IT-based health services. Whereas anonymization techniques were shown to be prone to data re-identification, synthetic data generation has gradually replaced anonymization since it is relatively less time and resource-consuming and more robust to data leakage. Generative Adversarial Networks (GANs) have been used for generating synthetic datasets, especially GAN frameworks adhering to the differential privacy phenomena. This research compares state-of-the-art GAN-based models for synthetic data generation to generate time-series synthetic medical records of dementia patients which can be distributed without privacy concerns. Predictive modeling, autocorrelation, and distribution analysis are used to assess the Quality of Generating (QoG) of the generated data. The privacy preservation of the respective models is assessed by applying membership inference attacks to determine potential data leakage risks. Our experiments indicate the superiority of the privacy-preserving GAN (PPGAN) model over other models regarding privacy preservation while maintaining an acceptable level of QoG. The presented results can support better data protection for medical use cases in the future.;Protect and Extend - Using GANs for Synthetic Data Generation of Time-Series Medical Records;health related;health related;1
"J. Boone; B. Hopkins; F. Afghah";2023;Drone-based wildfire detection models allow for real-time fire monitoring which is critical for the most efficient intervention and mitigation techniques needed for wildfires. However, due to restrictions on the usage of UAVs during wildfires and prescribed burns, current UAV-sourced wildfire imagery datasets are limited to images of individual burns. While deep learning fire detection models trained and tested on these datasets can achieve high fire classification accuracy, these models fail to generalize when given images of wildfires from other forestry types due to the differences in vegetation, climate, time of year, and other factors that contribute to the visual appearance of these burns. Synthetic augmentation techniques can increase the diversity of training datasets. In this work, we develop an attention-guided image-to-image translation tool that utilizes Generative Adversarial Networks (GANs) to generate wildfire images from aerial forestry images in order to increase classification accuracy. We illustrate the need for attention mechanisms for generating wildfire images through image-to-image translation techniques. We observe increased classification accuracy for a test set based on a separate burn from the training dataset when augmenting training data with diverse synthetic wildfire images.;Attention-Guided Synthetic Data Augmentation for Drone-based Wildfire Detection;Not health related;Not health related;1
"A. Baul; W. Kuang; J. Zhang; H. Yu; L. Wu";2021;"Detecting the pedestrian flow from different directions at a traffic intersection has always been a challenging task. Challenges include different crowd densities, occlusions, lack of available data and so on. Emergence of deep learning and computer vision methods has shown potentials to deal with this problem. Most of the recent works focus on detecting combined pedestrian flow or counting the total number of pedestrians. In this work, we propose to detect not only combined pedestrian flow but also pedestrian flows from different directions on a crosswalk at a traffic intersection. Our main contributions are summarized as follows: 1) we introduce a synthetic dataset that we create using computer game and a real-world dataset we collect from the street; 2) we propose a Pedestrian Counting Network (PCNet) to count pedestrians from different directions along a crosswalk and then propose a Pedestrian Flow Inference Model (PFIM) to infer the pedestrian flow parameters of volume and density; 3) we design a structure-aware domain adaptation for learning from synthetic data to real data. The experimental results show the effectiveness and accuracy of the proposed method.";Learning to Detect Pedestrian Flow in Traffic Intersections from Synthetic Data;Not health related;Not health related;0
"Z. Duan; S. Dongye; C. Ji; Y. Song; J. Xiao; Z. Li; L. Cheng";2024;Global land cover heterogeneity is usually directly reflected in the background of remote sensing objects. This substantially affects the generalization ability of object detection models and is a challenge for remote sensing object detection in large-scale areas. In this study, we analyze the heterogeneity of land cover in different climate areas, and propose a remote sensing object image synthesis method based on multi-stage blending strategy (MBS) to achieve a high-quality blending of the labeled runway foreground image and the typical surface image of the area to be detected. Three geometric augmentation algorithms are applied to expand the size of the sample data. Finally, two representative object detection models, YOLOv5 and Faster R-CNN, are used to evaluate the effectiveness of the synthetic data. Experiments in North Africa and Central Asia demonstrate that the MBS method effectively improves the performance of airport runway detection in large-scale heterogeneous areas. Additionally, our method performs better than Copy-Paste and Poisson blending.;Synthetic Data Generation Based on Multi-stage Blending Strategy for Airport Runway Detection in Areas of Heterogeneous Land Cover from Remote Sensing Images;Not health related;Not health related;0
"A. R. Ambili; R. C. Roy";2023;Multilingual based voice activated human computer interaction systems are currently in high demand. The Spoken Language Identification Unit (SPLID) is an inevitable front end unit of such a multilingual system. These systems will be a great boon to a country like India where around 24 official languages are spoken. Deep learning architectures for spoken language identification have progressed to the point that they can now perform well, even in the presence of various background noises. However, a strong phonetic relationship across various Indian languages leads to increased confusion in the SPLID unit. Therefore, the goal of this study is to propose a synthetic voice data augmentation method based on speech synthesis to improve the spoken Indian language identification system. Here the research attempts to determine how well pre-trained computer vision models recognize spoken languages in synthetic and classical audio augmentation environments. The accuracy of the models was compared using bottleneck features extracted from three different pre-trained models VGG16, RESNET50, and Inception-v3 while using an Artificial Neural Network (ANN), Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF), Naive Bayes (NB), Decision Tree (DT) and KNN (K-Nearest Neighbors) as classifiers. The proposed system was tested on three Indian language datasets - two comprising seven Indian languages (Hindi, Malayalam, Tamil, Telugu, Marathi, Kannada and Bengali), one containing five Indian languages (Tamil, Hindi, Malayalam, Oria and Assamese), and on a foreign language dataset. It was found that the addition of synthetic audio samples improved the accuracy by 17%. Among the pre-trained models, VGG16 and Inception-v3 combined with PCA and ANN were found to have the maximum accuracy of 97%.;The Effect of Synthetic Voice Data Augmentation on Spoken Language Identification on Indian Languages;Not health related;Not health related;0
"G. Nikseresht; G. Agam; K. Arfanakis";2022;Data scarcity is a fundamental obstacle to the adoption of deep learning models for a variety of detection tasks in medical images due to inherent difficulties in acquiring and labeling such images. This is particularly so for cerebral microbleed (CMB) detection in community cohorts where datasets are small and targets are hard to find. While numerous methods have been proposed for leveraging unlabeled data and semi-supervised learning to improve detection performance, unlabeled data may not be available in some domains. Data synthesis provides an alternative approach for increasing the size of the dataset but learning a realistic synthesis model using state-of-the-art methods such as generative adversarial networks suffers from the same data scarcity challenges as the detection model. In this work, we propose an end to end approach for refining synthesized examples developed using a priori domain knowledge, in order to improve detection performance using limited amounts of actual data. We combine detection and synthesis in a single network and incorporate detection loss into the synthesis refinement block in order to encourage the generation of realistic synthetic data, while concurrently updating the detection model trained with a small set of real data to enforce realistic synthesis and prevent overfitting. We demonstrate the effectiveness of our approach to improve CMB detection on postmortem MRI data and show significant improvements in average precision over baseline data synthesis and adversarial approaches.;End-to-End Task-Guided Refinement of Synthetic Images for Data Efficient Cerebral Microbleed Detection;health related;Not health related;1
"A. Zhang; P. Dehghanian; M. Stevens; J. Snodgrass; T. Overbye";2023;A key challenge to Geomagnetic Disturbance (GMD) studies is the scarcity of severe geomagnetic field data available to researchers due to its low event occurrence. This study aims to address this challenge by first creating realistic “synthetic” data that represents the geomagnetic field fluctuations caused by recent GMD events. This paper utilizes a machine-learning approach to generate synthetic geomagnetic field data. Specifically, the application and preliminary results of a modified form of the generative adversarial network (GAN) to create time-series synthetic geomagnetic field data of three different severities are described here. The purpose of this paper is to document the first step towards creating severe synthetic geomagnetic field data to advance power system research. Future studies beyond this paper will extend on this work to generate data representing severe GMD storms.;Synthetic Geomagnetic Field Data Creation for Geomagnetic Disturbance Studies using Time-series Generative Adversarial Networks;Not health related;Not health related;0
"L. Zeng; L. Chen; W. Bao; Z. Li; Y. Xu; J. Yuan; N. K. Kalantari";2023;Accurate facial landmark detection on wild images plays an essential role in human-computer interaction, entertainment, and medical applications. Existing approaches have limitations in enforcing 3D consistency while detecting 3D/2D facial landmarks due to the lack of multi-view in-the-wild training data. Fortunately, with the recent advances in generative visual models and neural rendering, we have witnessed rapid progress towards high quality 3D image synthesis. In this work, we leverage such approaches to construct a synthetic dataset and propose a novel multi-view consistent learning strategy to improve 3D facial landmark detection accuracy on in-the-wild images. The proposed 3D-aware module can be plugged into any learning-based landmark detection algorithm to enhance its accuracy. We demonstrate the superiority of the proposed plug-in module with extensive comparison against state-of-the-art methods on several real and synthetic datasets.;3D-aware Facial Landmark Detection via Multi-view Consistent Training on Synthetic Data;Not health related;Not health related;0
"S. -H. Lee; Y. -S. Lim; W. -H. Kwon; Y. -H. Park";2023;This article proposes a novel model-based synthetic data learning approach to suppress the multipath interference (MPI) in the amplitude-modulated continuous wave (AMCW) coaxial-scanning LiDAR. Previous works have focused on the MPI suppression in conventional AMCW time-of-flight (ToF) sensors with flash-type illumination sources based on the various MPI assumptions, whose MPI errors still remain in cm-scale. To achieve mm-scale MPI error suppression, this article proposes a novel learning-based MPI error suppression method implemented in a coaxial type AMCW scanning LiDAR in which the MPI phenomenon can be precisely modeled. Specifically, to model the MPI-generating mechanism in the AMCW coaxial-scanning LiDAR, a novel dedicated light transport model is designed at the simulation level. Using this precise MPI simulation model, the synthetic dataset reflecting the MPI mechanism in coaxial scanning LiDAR is generated and used for the training of the Bayesian-optimized eXtreme gradient boosting (XGBoost) ensemble. This trained XGBoost is then used for the MPI error suppression of the AMCW coaxial-scanning LiDAR. Experimental validation showed that the mean absolute error (MAE) of MPI error can be reduced to less than 2 mm by the proposed method. Such precise MPI suppression results are also maintained in real object scenes. Specifically, the MAE of MPI error in an object scene with a sharp corner is reduced to 2.8 mm, which is extremely low compared to previous works.;Multipath Interference Suppression of Amplitude-Modulated Continuous Wave Coaxial-Scanning LiDAR Using Model-Based Synthetic Data Learning;Not health related;Not health related;0
"Y. Lyu; R. Royen; A. Munteanu";2024;Synthetic traffic datasets provide highly accurate and affordable annotations, which are of crucial importance in complex vision-based perception tasks performed on real-world traffic data. Due to the lack of paired 2-D–3-D data, it remains very challenging when adapting the knowledge of a vehicle’s pose in SE(3) with its known 3-D geometry. In this article, we first propose a synthetic dataset, SynthV6D, enabling 6-D pose estimation of vehicles in monocular traffic images. The dataset comprises industrial-grade vehicles in motion evolving in realistic virtual scenery, covering a wide range of viewpoints and distances. Second, we introduce a weakly supervised domain adaptation approach, dubbed W6DNet, to recover the 6-D pose. To this end, by using the synthetic dataset, a novel linked image feature space-based domain adaptation is introduced. Furthermore, an original two-step double-fusion block is proposed to fuse the multi-modal data representations and the cross-space features. Consequently, the proposed method learns the pose-specific embeddings. We evaluate W6DNet on the real-world ApolloCar3D dataset. Extensive experimental results demonstrate that, when a small amount of real-world data is accessible, the proposed approach can significantly advance the performance when adapting knowledge from SynthV6D. Moreover, it achieves competitive performance compared to fully supervised state-of-the-art methods. The code is available at https://github.com/YangLyu-123/TIM-W6DNet.git.;W6DNet: Weakly Supervised Domain Adaptation for Monocular Vehicle 6-D Pose Estimation With 3-D Priors and Synthetic Data;Not health related;Not health related;0
"F. Rustam; A. D. Jurcut; W. Aljedaani; I. Ashraf";2023;The emergence of new network architectures, protocols, and tools has made it easier for cybercriminals to launch attacks using AI-based tools, presenting challenges in network security. To protect such systems, a versatile malicious traffic detection system is required that can identify attacks regardless of the type of traffic coming toward the network. In this paper, a system is proposed that can singly analyze multi-environment traffic (IoT and traditional IP-based) to detect malicious activity. The existing techniques for managing Multi-Environment traffic are inefficient due to the absence of AI utilization. To overcome these issues, the proposed approach generates a novel multienvironment traffic dataset by merging existing network datasets containing both traditional IP-based traffic and IoT network traffic. Synthetic Data Augmentation TEchnique (S-DATE) is also proposed to overcome the problem of imbalanced data distribution in the new multi-environment dataset. The results show that the utilization of S-DATE results in faster machine learning model convergence and an improvement in the detection rate of normal and abnormal traffic. The proposed approach achieves an impressive overall detection rate of 0.991 and is statistically significant compared to other state-of-the-art approaches.;Securing Multi-Environment Networks using Versatile Synthetic Data Augmentation Technique and Machine Learning Algorithms;Not health related;Not health related;0
"P. D’Alterio; C. Hensel; B. A. S. Hasan";2023;To boost training and adaptation of end to end (E2E) automatic speech recognition (ASR) models, several approaches to use paired speech-text input together with unpaired text input have emerged. They aim at improving the model performance on rare words, personalisation, and long tail. In this work, we present a systematic study of the impact of such training/adaptation and compare it to training with synthetic utterances generated by text-to-speech engines. We experiment with in-house and CommonVoice datasets and conclude that using text data for adaptation is effective, but is outperformed by adapting with synthetic audio even when the TTS engine is sub-optimal. This challenges recent literature on the difficulties of using TTS data including catastrophic forgetting, feature misalignment, and pronunciation errors, which motivated the use of text-only adaptation.;Can Unpaired Textual Data Replace Synthetic Speech in ASR Model Adaptation?;Not health related;Not health related;0
"J. W. Betz; R. W. Pinto; J. L. Prince";1989;An overview of the architecture and implementation of a model-based vision system developed for object recognition with SAR (synthetic aperture radar) data is presented. The initial implementation of the system is currently being used to develop experimental results to guide refinements and enhancements. The system uses detailed analytic prediction models and capable description algorithms, with all knowledge and uncertainty consistently represented. The prediction component automatically generates integrated hierarchical representations of both structural and appearance information, and represents an important step toward automatic object recognition. The recognition system architecture features modular computational agents that support distributed, localized control, with the ability to extract and use object-specific knowledge from the prediction database. The system will serve as a testbed for model-based vision research to allow experimentation with new algorithms and alternative recognition approaches.<>;A model-based vision system for object recognition with synthetic aperture radar data;Not health related;Not health related;0
"H. Vietz; M. Hirth; S. Baum; M. Weyrich";2023;Gathering sufficient labeled training data to effectively train a high-performing deep learning model can be particularly challenging in the realm of industrial automation. Depending on the data type, this may require expensive interruptions to production processes or similar disruptions on the factory floor for data collection. It is often uncertain which data types are crucial for enhancing the performance of the trained model. For vision models, factors such as specific viewing angles or lighting conditions may be important, while for models utilizing radio signals, unique reflections generated by moving metal surfaces could be significant. Moreover, data labeling is expensive as it is primarily conducted manually by human workers. This paper demonstrates how to automatically generate relevant, labeled synthetic training data to boost a neural network's accuracy for deep learning-based 5G indoor positioning tasks. We reveal that employing this generated synthetic data to train a convolutional neural network can improve its median positioning accuracy by a notable 25%.;Synthetic Data Generation for improving Deep Learning-based 5G Indoor Positioning;Not health related;Not health related;0
"J. Lee; Y. Jeon; W. Lee; Y. Kim; G. G. Lee";2023;Dialogue state tracking plays a crucial role in extracting information in task-oriented dialogue systems. However, preceding research are limited to textual modalities, primarily due to the shortage of authentic human audio datasets. We address this by investigating synthetic audio data for audio-based DST. To this end, we develop cascading and end-to-end models, train them with our synthetic audio dataset, and test them on actual human speech data. To facilitate evaluation tailored to audio modalities, we introduce a novel PhonemeF1 to capture pronunciation similarity. Experimental results showed that models trained solely on synthetic datasets can generalize their performance to human voice data. By eliminating the dependency on human speech data collection, these insights pave the way for significant practical advancements in audio-based DST. Data and code are available at https://github.com/JihyunLee1/E2E-DST.1;Exploring the Viability of Synthetic Audio Data for Audio-Based Dialogue State Tracking;Not health related;Not health related;0
"M. Martone; N. Gollin; P. Rizzoli; M. Villano; M. Younis; G. Krieg";2019;For next-generation synthetic aperture radar (SAR) missions, an increasing volume of onboard data is going to be required, due to the employment of large band-widths, multiple polarizations, and large swath widths, which leads to hard requirements in terms of onboard memory and downlink capacity. This paper presents novel methods for data volume reduction in the context of multi-azimuth channel (MAC) SAR and staggered SAR, which allow for high-resolution imaging of a wide swath by means of different system architectures. In particular, for such systems, a pulse repetition frequency (PRF) typically higher than the processed Doppler bandwidth is selected. The resulting over-sampling and correlation properties of the azimuth SAR raw signal can be exploited by applying an efficient encoding and quantization of the SAR raw data in order to reduce the onboard data volum. Simulation results show that the proposed methods allow for a significant reduction of the data volume to be downlinked to the ground at the cost of a modest increase of onboard computational effort.;Advances in Onboard Data Reduction Methods for Next-Generation Synthetic Aperture Radar Systems;Not health related;Not health related;0
"R. Beche; S. Nedevschi";2020;Image-to-image translation is an emerging method of computer vision dataset augmentation, which allows transferring the style of real life images onto synthetic ones, making them more realistic. In our work we propose an incremental improvement over the adversarial learning generator architectures used by image-to-image translation models. First, we managed to use a single network, instead of 2, thus creating a more memory-efficient model, which allowed for an end-to-end training on high resolutions. Second, inspired from recent work on semantic segmentation architectures, we enhanced our model by implying a multi-scale encoding and stylization phase, allowing for a better control over the contextual and spatial features. Given a synthetic image, our framework allows for its multimodal translation into the real domain. Our model shows promising results at narrowing the semantic gap between synthetic and real data.;Narrowing the semantic gap between real and synthetic data;Not health related;Not health related;0
"D. Mallios; X. Cai";2021;"Image guidance nowadays is a crucial component for doctors to facilitate the design of the planning radiation therapy dosage. The delineation of soft organs in the planning phase and during the radiation therapy is crucial for the treatment procedure. Deep Learning (DL) flourishes, presenting state-of-the-art results in challenging computer vision tasks; however, the lack of annotated data hardens the research advancements for medical applications. The research in this paper develops DL approaches for the segmentation of organs-at-risk and specifically from images retrieved from a computed tomography system during the radiation treatment of each patient. The proposed approaches are based on convolutional neural architectures trained with only a couple of thousand images, and can also be trained online, showing its learning ability from new patients. The lack of annotated data is also addressed with synthetic data generated by a modified GAN. Experimental results demonstrate the excellent performance of the proposed approaches in rectum segmentation task.";Deep Rectum Segmentation for Image Guided Radiation Therapy with Synthetic Data;health related;health related;1
"S. Lin; K. Wang; X. Zeng; R. Zhao";2023;Few-shot object detection (FSOD) aims to expand an object detector for novel categories given only a few instances for training. The few training samples restrict the performance of FSOD model. Recent text-to-image generation models have shown promising results in generating high-quality images. How applicable these synthetic images are for FSOD tasks remains under-explored. This work extensively studies how synthetic images generated from state-of-the-art text-to-image generators benefit FSOD tasks. We focus on two perspectives: (1) How to use synthetic data for FSOD? (2) How to find representative samples from the large-scale synthetic dataset? We design a copy-paste-based pipeline for using synthetic data. Specifically, saliency object detection is applied to the original generated image, and the minimum enclosing box is used for cropping the main object based on the saliency map. After that, the cropped object is randomly pasted on the image, which comes from the base dataset. We also study the influence of the input text of text-to-image generator and the number of synthetic images used. To construct a representative synthetic training dataset, we maximize the diversity of the selected images via a sample-based and cluster-based method. However, the severe problem of high false positives (FP) ratio of novel categories in FSOD can not be solved by using synthetic data. We propose integrating CLIP, a zero-shot recognition model, into the FSOD pipeline, which can filter 90% of FP by defining a threshold for the similarity score between the detected object and the text of the predicted category. Extensive experiments on PASCAL VOC and MS COCO validate the effectiveness of our method, in which performance gain is up to 21.9% compared to the few-shot baseline.;Explore the Power of Synthetic Data on Few-shot Object Detection;Not health related;Not health related;0
"O. Petrovic; D. L. D. Duarte; W. Herfs";2023;Instead of taking images with a camera, synthetic data is generated in a computer simulation. One advantage of this is that training data can be generated on-demand, e.g., to automatically retrain robots when a task changes. While this makes synthetic data a promising approach for autonomous productions, realizing such autonomous setups is difficult with current systems for generating synthetic data, which usually require a programmer for every dataset to be generated.To overcome this problem, we present a novel framework for generating synthetic data. This framework restructures the generation process into asynchronous phases to increase the level of autonomy in two ways. First, by letting programmers write parameterized scripts, many different datasets can be autonomously generated. Secondly, by introducing a user interface, domain experts are enabled to influence the generation process on their own without a programmer. Furthermore, by being built as a new layer on top of existing systems for generating synthetic data, our framework shows a new way to maximize compatibility with other research on synthetic data generation.To test our framework, we have developed a fully functional prototype based on it. Successfully using this prototype for an example experiment, we conclude that our ideas work. Future research can use our prototype for more elaborate experiments on autonomous productions and to further assess its usability.;Generating Synthetic Data Using a Knowledge-based Framework for Autonomous Productions;Not health related;Not health related;0
"R. Cazorla; L. Poinel; P. Papadakis; C. Buche";2022;The use of deep learning in semantic segmentation of point clouds enables a drastic improvement of segmentation precision. However, available datasets are restrained to a few applications with limited applicability to other fields. Using synthetic and real data can alleviate the burden of creating a dedicated dataset at the cost of domain-shift that is mostly addressed during training, while treating the problem directly on the data has been less explored. Towards this goal, two methods to alleviate domain shift are proposed, firstly by enhanced generation and sampling of synthetic data and secondly by leveraging color information of unlabeled point clouds to color synthetic, uncoloured data. Obtained results confirm their usefulness in improving semantic segmentation result (+3.43 into mIoU for a network trained on S3DIS zone 1). More importantly, the devised coloring method shows the ability of a point-based network to link color information with recurrent geometric features. Finally, the presented methods are able to bridge the domain-shift gap even in cases where inclusion of raw synthetic data during training impedes learning.;Reducing domain shift in synthetic data augmentation for semantic segmentation of 3D point clouds;Not health related;Not health related;0
"M. Fang; M. Huber; J. Fierrez; R. Ramachandra; N. Damer; A. Alkhaddour; M. Kasantcev; V. Pryadchenko; Z. Yang; H. Huangfu; Y. Chen; Y. Zhang; Y. Pan; J. Jiang; X. Liu; X. Sun; C. Wang; X. Liu; Z. Chang; G. Zhao; J. Tapia; L. Gonzalez-Soler; C. Aravena; D. Schulz";2023;This paper presents a summary of the Competition on Face Presentation Attack Detection Based on Privacy-aware Synthetic Training Data (SynFacePAD 2023) held at the 2023 International Joint Conference on Biometrics (IJCB 2023). The competition attracted a total of 8 participating teams with valid submissions from academia and industry. The competition aimed to motivate and attract solutions that target detecting face presentation attacks while considering synthetic-based training data motivated by privacy, legal and ethical concerns associated with personal data. To achieve that, the training data used by the participants was limited to synthetic data provided by the organizers. The submitted solutions presented innovations and novel approaches that led to outperforming the considered baseline in the investigated benchmarks.;SynFacePAD 2023: Competition on Face Presentation Attack Detection Based on Privacy-aware Synthetic Training Data;Not health related;Not health related;0
"S. Kaada; S. A. Hamideche; C. Daems; M. L. Alberi Morel";2023;In sensing-enabled mobile infrastructure, the network itself acts as a whole sensor by leveraging radio data or signals collected within Base Stations (BSs). This data is exploited for the development of data-driven machine learning solutions to augment network’s capabilities. Nevertheless, large-scale qualitative data is required for achieving high accuracy learning. However, their training phase leads to prohibitive cost and heavy constraints on data collection and storage that are not desirable for network. To overcome this problem, we propose to use synthetic data instead of real data for training machine learning models to avoid high cost data sharing/storage. In this paper, we are interested in real-life Environment Sensing Network in a context of limited data amount sharing. We focus on Indoor-Outdoor Detection (IOD) using unsupervised machine learning classification models. For this purpose, experiments are conducted following the paradigm of Training on Synthetic data and Testing on Real Data (TSTR). We conduct a comparative study of four well-known generative models, that are able to generate synthetic 3GPP radio data with similar distribution than the source data. We investigate the quality of these synthetic generated radio data according to three dimensions: distribution similarity, data variability and detection capability. The classification models trained with synthetic generated data are tested in real-life context to infer whether a user connected to the network is inside or outside a building. The study shows convincing results with an Indoor/Outdoor unsupervised classification performance up to 80% of F1 _ score like in real-life data training scenarios.;Classification with Synthetic Radio Data for Real-life Environment Sensing;Not health related;Not health related;0
"P. K. Mohanty; G. Panda; M. Basu; D. S. Roy";2023;Electric vehicles (EVs) are an effective solution for reducing reliance on non-renewable energy sources. However, the lack of charging infrastructure and concerns over their range are some of the biggest hurdles to adopting EVs. Charging infrastructure for EVs is, however, on the rise. Proper planning of charging stations vis-à-vis road networks and related points of interest such as transportation hubs, schools, shopping centres, etc., alongside such roads become vital to laying out a plan for such infrastructure, particularly for developing countries like India where EV adoption is relatively in a nascent stage. Synthetic datasets can help overcome these hurdles and promote EV adoption. This article presents a synthetic dataset mechanism for EV charging infrastructure planning, taking the Indian city of Berhampur, Odisha with its existiing EV charging infrastructure as a reference. The dataset includes information on the number of charging sessions for EVs, allocation to chargers in EVCS, reach time, charging start and end time, waiting time, total time spent at EVCS, total charged amount, energy used, and cost for charging. This information can help city planners and utilities identify the optimal locations for charging stations and plan for future charging infrastructure augmentation. The dataset can also be used to predict energy usage for the near future and identify the key factors affecting the planning with the help of Explainable AI (XAI) techniques. This information can help forecast the demand for charging services and optimize energy usage in the city. The article contributes to the EV charging behaviour and infrastructure planning and aims to promote broader EV adoption for future sustainable transportation.;Interpreting Energy Utilisation with Shapley Additive Explanations by Defining a Synthetic Data Generator for Plausible Charging Sessions of Electric Vehicles;Not health related;Not health related;0
"S. Kumi; M. Hilton; C. Snow; R. K. Lomotey; R. Deters";2023;Health Digital Twins (HDTs) are virtual replicas of a patient’s physical/actual data. The major setbacks for applying Machine Learning (ML) in HDTs are the lack of availability of patients’ data due to privacy concerns and Artificial Intelligence (AI) bias. Given these shortcomings, synthetic data has been leveraged to solve privacy issues and increase diversity in datasets. In this paper, we evaluate four synthetic data generation models namely, Gaussian Copula, Conditional Tabular Generative Adversarial Network (CTGAN), CopulaGAN, and Tabular Variational Autoencoder (TVAE) which are used to generate synthetic data for actual sleep data retrieved from a wearable device. Gaussian Copula performed best in capturing the correlation between the variables with the real data with a quality score of approximately 96%. Additionally, we evaluate the efficacy of the synthetic generation models by training five well-known ML models on the generated synthetic data. Our experimental results show that the ML models trained on the synthetic data achieve an MAE (Mean Absolute Error) of less than 10% in the prediction of sleep quality score. The results from this work indicate that synthetic data could be used for ML tasks while preserving the privacy of data subjects.;SleepSynth: Evaluating the use of Synthetic Data in Health Digital Twins;health related;health related;1
"M. Grimmer; C. Rathgeb; C. Busch";2024;Evaluating the quality of facial images is essential for operating face recognition systems with sufficient accuracy. The recent advances in face quality standardisation (ISO/IEC CD3 29794-5) recommend the usage of component quality measures for breaking down face quality into its individual factors, hence providing valuable feedback for operators to re-capture low-quality images. In light of recent advances in 3D-aware generative adversarial networks, we propose a novel dataset, Syn-YawPitch, comprising 1, 000 identities with varying yaw-pitch angle combinations. Utilizing this dataset, we demonstrate that pitch angles beyond 30 degrees have a significant impact on the biometric performance of current face recognition systems. Furthermore, we propose a lightweight and explainable pose quality predictor that adheres to the draft international standard of ISO/IEC CD3 29794-5 and benchmark it against state-of-the-art face image quality assessment algorithms.;Pose Impact Estimation on Face Recognition Using 3D-Aware Synthetic Data With Application to Quality Assessment;Not health related;Not health related;0
"S. Ren; F. Zhou; W. Fan; C. Wang";2019;This paper presents an automatic clustering algorithm for polarimetric synthetic aperture radar (PolSAR) data. Based on the Wishart mixture model which describes non-Gaussian areas more accurately, the PolSAR image is clustered by variational Bayesian expectation maximization algorithm. The incomplete gamma mixture distribution is incorporated to describe the equivalent number of looks for each cluster, which stands for the homogeneity and non-Gaussian statistics of each cluster. Then, the lower bound is computed each iteration until it reaches convergence. Meanwhile, spatial information is utilized by introducing two similarity measures. Finally, the experimental results of real measured AIRSAR dataset show that the proposed algorithm can obtain comparable clustering accuracy and determine the number of clusters automatically.;Variational Bayesian Wishart Mixture Model for Polarimetric Synthetic Aperture Radar Data;Not health related;Not health related;0
"E. Sariyanidi; C. Ferrari; S. Berretti; R. T. Schultz; B. Tunc";2023;The standard benchmark metric for 3D face reconstruction is the geometric error between reconstructed meshes and the ground truth. Nearly all recent reconstruction methods are validated on real ground truth scans, in which case one needs to establish point correspondence prior to error computation, which is typically done with the Chamfer (i.e., nearest neighbor) criterion. However, a simple yet fundamental question have not been asked: Is the Chamfer error an appropriate and fair benchmark metric for 3D face reconstruction? More generally, how can we determine which error estimator is a better benchmark metric? We present a meta-evaluation framework that uses synthetic data to evaluate the quality of a geometric error estimator as a benchmark metric for face reconstruction. Further, we use this framework to experimentally compare four geometric error estimators. Results show that the standard approach not only severely underestimates the error, but also does so inconsistently across reconstruction methods, to the point of even altering the ranking of the compared methods. Moreover, although non-rigid ICP leads to a metric with smaller estimation bias, it could still not correctly rank all compared reconstruction methods, and is significantly more time consuming than Chamfer. In sum, we show several issues present in the current benchmarking and propose a procedure using synthetic data to address these issues.;Meta-evaluation for 3D Face Reconstruction Via Synthetic Data;Not health related;Not health related;0
"A. Takmaz; J. Schult; I. Kaftan; M. Akçay; B. Leibe; R. Sumner; F. Engelmann; S. Tang";2023;Segmenting humans in 3D indoor scenes has become increasingly important with the rise of human-centered robotics and AR/VR applications. To this end, we propose the task of joint 3D human semantic segmentation, instance segmentation and multi-human body-part segmentation. Few works have attempted to directly segment humans in cluttered 3D scenes, which is largely due to the lack of annotated training data of humans interacting with 3D scenes. We address this challenge and propose a framework for generating training data of synthetic humans interacting with real 3D scenes. Furthermore, we propose a novel transformer-based model, Human3D, which is the first end-to-end model for segmenting multiple human instances and their body-parts in a unified manner. The key advantage of our synthetic data generation framework is its ability to generate diverse and realistic human-scene interactions, with highly accurate ground truth. Our experiments show that pre-training on synthetic data improves performance on a wide variety of 3D human segmentation tasks. Finally, we demonstrate that Human3D outperforms even task-specific state-of-the-art 3D segmentation methods.;3D Segmentation of Humans in Point Clouds with Synthetic Data;Not health related;Not health related;0
"M. S. Malovichko; A. V. Tarasov; N. B. Yavich; K. V. Titov";2022;Last decades, there has been an increased interest in the use of the self-potential (SP) method in hydrogeophysics. In response to this strong interest, we develop a novel approach to the inversion of SP data. Mathematically, the SP inverse problem is the source identification problem for the Poisson equation. Our approach substantially differs from the standard regularization approach, which explicitly includes the forward-problem operator into the cost functional. We formulated the inverse problem is as an optimal control (OC) problem and then translate it into a variational system. The system is approximated in suitable finite-element (FE) spaces giving rise to an algebraic problem with the saddle point structure. In contrast to the standard approach, which leads to a dense linear system, our method results in a system with a sparse block matrix. It can be efficiently solved by either direct sparse solvers or preconditioned iterative solvers. In this article, we present the formulation of the problem and its FE approximation. We discuss the iterative solution and preconditioning strategies. Our software implementation is based on an industrial FE package. We also present a numerical experiment with node-based linear FEs on tetrahedral grids. Our results suggest that the proposed approach may serve as a rapid and reliable tool for large-scale SP inverse problems. Moreover, the same technique can easily be extended to a wide range of geophysical linear inverse problems, such as inversions of magnetic and gravity data.;Application of Optimal Control to Inversion of Self-Potential Data: Theory and Synthetic Examples;Not health related;Not health related;0
"C. Pendão; I. Silva; A. Moreira; F. J. Aranda; J. Torres-Sospedra";2023;Synthetic data of high quality can provide research teams with an effective means of conducting large-scale evaluations of their indoor positioning systems under controlled conditions, while avoiding the significant effort and costs associated with real-world experiments and data collection/labelling. Moreover, it facilitates the fair comparison with other solutions, since data can be generated for more diverse conditions and can be shared without concerns. The work described in this paper aims to improve the quality of WiFi synthetic data by integrating new models for channel noise and beacon receive probability into the Dioptra tool. We compare the results of 13 fingerprinting methods used on 15 synthetic databases and 14 real-world databases. The results indicate that synthetic data can be an effective alternative/complement for the evaluation and comparison of WiFi-based positioning methods.;Towards Quality Wi-Fi Synthetic Data for Indoor Positioning Evaluation;Not health related;Not health related;0
"S. K. Jangir; R. Bahmanyar";2024;Employing a two-step pipeline that encompasses an image-to-image translation and a superresolution (SR) network, we significantly enhance satellite images with a ground sample distance (GSD) of 30 cm to a superior 15 cm GSD. Our translation network learns from the characteristics of satellite images and replicates these onto aerial images with a GSD of 15 cm, creating a tailored training dataset. The SR network then uses this dataset to train and render enhanced satellite images at a GSD of 15 cm. This innovative approach can provide a cost-effective substitute for commercial high-definition images, and broadens the usage of high-quality data across various applications.;Enhancing Very High-Resolution Satellite Images At 15 cm: A Novel Pipeline With Synthetic Data and Single Image Superresolution;Not health related;Not health related;0
"Y. Song; Z. Sun; Y. Wu; Y. Sun; S. Luo; Q. Li";2022;The data-hungry nature of deep learning and the high cost of annotating point-level labels for point clouds make it difficult to apply semantic segmentation methods to unlabeled real-world indoor scenes. Therefore, label-efficient point cloud segmentation has become a promising research topic. We noticed that the online housing design platforms can provide a large number of synthetic indoor 3D scenes, which are created with semantic labels. In this paper, we propose to learn semantic segmentation on synthetic point clouds and adapt the model for unlabeled real-world data. The main challenge is that directly using models trained on synthetic data for real-world data produces poor results due to the large domain gap between synthetic and real-world data. We design a point cloud style transfer network and a feature discrimination network to reduce the domain gap in both the input space and the feature space. Experiments show that our approach significantly improves the performance on real-world data for models learned from synthetic data.;Learning Semantic Segmentation on Unlabeled Real-World Indoor Point Clouds via Synthetic Data;Not health related;Not health related;0
"M. Zarei; M. Jahed; M. R. Daliri; M. Esghaei";2021;"Quantifying the spike-LFP phase coupling strength is a valuable approach to measure the inter-neuronal rhythmic synchronization. This synchronization has been commonly quantified by either the phase-locking value (PLV) method. However, this method suffers from a strong bias to the number of spikes. Although other methods, such as the pairwise phase consistency (PPC) were later introduced to overcome this issue, low spike count bias has remained a concern. Given the importance of measuring spike-phase coupling in short trials or neurons with small firing rates, we introduce a new approach for measuring spike-phase coupling which performs reliably for neurons responding at low spike rates, or neurons from which we have recorded only a small number of spikes. We use an exponential-based model to estimate the ideal spike-phase coupling based on measurements at different spike counts. Our approach significantly enhances the accuracy of spike-phase coupling at small spike-counts. This method calculates the spike-phase coupling significantly more accurately than the previous methods using both simulation and experimental data, in conditions with small numbers of spikes (R-squared criterion, $R^{2} > 0.98$ for the exponential model and $R^{2} > 0.65$ for the logarithmic model). Importantly, comparison of the MSE across the three N-spike (20, 35, and 50) shows that our model's estimated SPC is significantly closer to the ideal PLV, compared to that of the original PLV ($P\ll 0.001$; sign test). The proposed method may aid physiologists in accurate measurement of the degree of spikes' coupling to the phase of LFP in data-starving situations. Therefore, this is an important step towards introducing an accurate computation of spike-phase coupling with the minimum needed data.";Quantification of Spike-LFP Synchronization Based on Mathematical Function of Neural and Synthetic Data;Not health related;Not health related;0
"D. Duplevska; V. Medvedevs; D. Surmacs; A. Aboltins";2023;The increasing popularity and accessibility of un-manned aerial vehicles (UAVs) presents both opportunities and challenges. On the one hand, UAVs has a wide range of civilian, industrial, and military applications. On the other hand, the popularity of UAVs can lead to illegal or dangerous usage. Thus, the development of UAV recognition systems is crucial for ensuring safety and security. However, collecting and labeling large amounts of real-world data for training these systems can be time-consuming and labor-intensive.In this study, we propose a methodology, which can help to accelerate the development of new UAV recognition systems. This work demonstrates the effectiveness of training a neural network using a combination of real-world and synthetic data that can achieve similar performance to a network trained on real-world data only.;The Synthetic Data Application in the UAV Recognition Systems Development;Not health related;Not health related;0
"D. Bothra; S. Dixit; D. P. Mohanty; M. Haseeb; S. Tiwari; A. Chaulwar";2023;Modern deep learning methods have achieved remarkable performance on many real-world problems. However, the availability of large real data is a prerequisite for these methods to work. For applications like private ID card detection, it is difficult to gather real-world data because of privacy reasons and the sensitive nature of the data. Therefore, we propose a pipeline for synthetic card generations with random details followed by different methodologies for inserting them in random images such that they present different real-world scenarios such as cards held in the hand, placed on flat surfaces, etc. To simulate the environmental conditions affecting the card appearance in the image, we also propose to apply different image transformations to cards like rotation, blurring, etc. Finally, we also show the quality of the generated dataset by training the lightweight EfficientDet-Lite1 model with it and then testing it with images containing the real ID cards.;Synthetic Data Generation Pipeline for Private ID Cards Detection;Not health related;Not health related;0
"K. Baraka; F. S. Melo; M. Veloso";2017;Behavioral data on children with Autism Spectrum Disorders (ASD) are available thanks to standardized diagnostic tools, such as the Autism Diagnostic Observation Schedule (ADOS). This data can be of great use to enhance the learning and reasoning of agents interacting with children with ASD. However, the amount of such available data is limited and may not prove useful by itself to inform the algorithms of complex agents. To address this data scarcity problem, we present a method for generating synthetic behavioral data in the form of feature vectors characterizing a wide range of children with ASD. Our method relies on a thorough analysis and partition of the feature space based on a real dataset containing the ADOS scores of 279 children. We first analyze the real dataset using dimensionality reduction techniques, then introduce data-driven descriptors that partition the feature space into regions naturally arising from the data. We end by presenting a descriptor-based sampling method to generate synthetic feature vectors that successfully preserves the correlation structure of the real dataset.;Data-driven generation of synthetic behavioral feature vectors modeling children with autism spectrum disorders;Not health related;Not health related;0
"B. Su; F. Liu";2020;A large amount of data is often needed to train machine learning algorithms with confidence. One way to achieve the necessary data volume is to share and combine data from multiple parties. On the other hand, how to protect sensitive personal information during data sharing is always a challenge. We focus on data sharing when parties have overlapping attributes but non-overlapping individuals. One approach to achieve privacy protection is through sharing differentially private synthetic data. Each party generates synthetic data at its own preferred privacy budget, which are then released and horizontally merged across the parties. The total privacy cost for this approach is capped at the maximum individual budget employed by a party. We derive the mean squared error bounds for the parameter estimation in common regression analysis based on the merged sanitized data across parties. We identify through theoretical analysis the conditions under which the utility of sharing and merging sanitized data outweighs the perturbation introduced for satisfying differential privacy and surpasses that based on individual party data. The experiments suggest that sanitized HOMM data obtained at a practically reasonable small privacy cost can lead to smaller prediction and estimation errors than individual parties, demonstrating benefits of data sharing while protecting privacy.;Utility Analysis of Horizontally Merged Multi-Party Synthetic Data with Differential Privacy;Not health related;Not health related;0
"M. Benmahdjoub; A. Thabit; W. J. Niessen; E. B. Wolvius; T. Van Walsum";2023;"Surgical navigation guides surgeons during interventions. It provides them with spatial insights on where the anatomy and surgical instruments are in the patient space, and with respect to preoperative images. Image-to-patient alignment in this case is an important step which enables the visualization of preoperative images directly overlayed on the patient. Conventionally, image-to-patient alignment can be done with surface or point-based registration using anatomical or artificial landmarks. In case of point-based registration, surgeons use a trackable pointer to pinpoint some landmarks on the patient (fiducial markers placed preoperatively) and match them with their counterparts in the preoperative image. This method although accurate can be cumbersome and time-consuming. Direct detection of these landmarks in video may speed up the registration process, making it a first step towards AR navigation using head-mounted displays. Detection of objects, including such landmarks, is a task that can be performed with deep learning networks; however, the training of such networks requires large sets of annotated data, which are normally not available in clinical practice. In this study, we investigate the feasibility of using a deep learning model trained on synthetic images in detecting medical fiducial markers in real images, therefore bypassing the need for large sets of annotated patient data. To this end, we generate photorealistic synthetic images of subjects with landmarks using Unreal Engine and MetaHuman, train the detection model using these generated images and assess the model’s capability of detecting the registration markers on real 2D images. Our experimental results demonstrate that the object detection model, although trained exclusively on synthetic data, is capable of detecting the markers on the HoloLens 2 video feed with a F1 score of 81%, which can be used for image-to-patient alignment.";Fiducial markers detection trained exclusively on synthetic data for image-to-patient alignment in HMD-based surgical navigation;health related;Not health related;1
"Y. Feng; B. Q. Chandio; S. I. Thomopoulos; T. Chattopadhyay; P. M. Thompson";2023;White matter tracts generated from whole brain tractography are often processed using automatic segmentation methods with standard atlases. Atlases are generated from hundreds of subjects, which becomes time-consuming to create and difficult to apply to all populations. In this study, we extended our prior work on using a deep generative model - a Convolutional Variational Autoencoder - to map complex and data-intensive streamlines to a low-dimensional latent space given a limited sample size of 50 subjects from the ADNI3 dataset, to generate synthetic population-specific bundle templates using Kernel Density Estimation (KDE) on streamline embeddings. We conducted a quantitative shape analysis by calculating bundle shape metrics, and found that our bundle templates better capture the shape distribution of the bundles than the atlas data used in the original segmentation derived from young healthy adults. We further demonstrated the use of our framework for direct bundle segmentation from whole-brain tractograms.;Variational Autoencoders for Generating Synthetic Tractography-Based Bundle Templates in a Low-Data Setting;health related;Not health related;1
"M. D. Larsen; J. C. Huckett";2010;Government agencies must simultaneously maintain confidentiality of individual records and disseminate useful microdata. We propose a method to create synthetic data that combines quantile regression, hot deck imputation, and rank swapping. The result from implementation of the proposed procedure is a releasable data set containing original values for a few key variables, synthetic quantile regression predictions for several variables, and imputed and perturbed values for remaining variables. To measure the disclosure risk in the resulting synthetic data set, we extend existing probabilistic risk measures that aim to imitate an intruder attempting to match a record in the released data with information previously available on a target respondent.;Measuring Disclosure Risk for Multimethod Synthetic Data Generation;Not health related;Not health related;0
"R. Musto; A. Tricomi; R. Bruno; G. Pasquali";2023;Hyperspectral pansharpening has gained significant importance in recent years as hyperspectral systems have become more widely available. In order to generate an enhanced hyperspectral cube with improved spatial and spectral resolution, this technique fuses a panchromatic image with a hyperspectral cube. In this paper, we investigate the application of pansharpening to PRISMA data by proposing a novel training approach and deep learning model. To overcome the lack of ground truth data, we first leverage synthetic data, generated using AVIRIS-NG imagery, to pretrain the model in a supervised manner, enabling it to learn robust representations of spectral and spatial features. Subsequently, unsupervised transfer learning and PRISMA data are used to fine-tune the model and adapt it to the specific characteristics of the PRISMA sensor. The results demonstrate the effectiveness of our approach in achieving promising pansharpening performances, paving the way for enhanced hyperspectral data analysis in various remote sensing applications.;Advancing Prisma Pansharpening: A Deep Learning Approach with Synthetic Data Pretraining and Transfer Learning;Not health related;Not health related;0
"G. Fu; Q. Zhang; L. Zhu; C. Xiao; P. Li";2023;This paper aims to remove specular highlights from a single object-level image. Although previous methods have made some progresses, their performance remains somewhat limited, particularly for real images with complex specular highlights. To this end, we propose a three-stage network to address them. Specifically, given an input image, we first decompose it into the albedo, shading, and specular residue components to estimate a coarse specular-free image. Then, we further refine the coarse result to alleviate its visual artifacts such as color distortion. Finally, we adjust the tone of the refined result to match the tone of the input as closely as possible. In addition, to facilitate network training and quantitative evaluation, we present a large-scale synthetic dataset of object-level images, covering diverse objects and illumination conditions. Extensive experiments illustrate that our network is able to generalize well to unseen real object-level images, and even produce good results for scene-level images with multiple background objects and complex lighting.;Towards High-Quality Specular Highlight Removal by Leveraging Large-Scale Synthetic Data;Not health related;Not health related;0
"P. Addabbo; M. L. Bernardi; F. Biondi; M. Cimitile; C. Clemente; N. Fiscante; G. Giunta; D. Orlando; L. Yan";2023;One of the greatest limitations of Synthetic Aperture Radar imagery is the capability to obtain an arbitrarily high spatial resolution. Indeed, despite optical sensors, this capability is not just limited by the sensor technology. Instead, improving the SAR spatial resolution requires large transmitted bandwidth and relatively long synthetic apertures that for regulatory and practical reasons are impossible to be met. This issue gets particularly relevant when dealing with Stripmap mode acquisitions and with low carrier frequency sensors (where relatively large bandwidth signals are more difficult to be transmitted). To overcome this limitation, in this paper a deep learning based framework is proposed to enhance the spatial resolution of low-resolution SAR images while retaining the complex image accuracy. Results on simulated and real SAR data demonstrate the effectiveness of the proposed framework.;Super-Resolution of Synthetic Aperture Radar Complex Data by Deep-Learning;Not health related;Not health related;0
"N. Tsagarakis; A. Antonaras; M. Maniadakis";2023;Today, waste recycling is supported by intelligent robots that use machine learning to identify and sort recyclables. The development of computer vision applications based on machine learning relies heavily on large datasets that are used to train deep neural network models. In recent years, methods that allow the creation of large training datasets from a limited initial set of images have been investigated. This paper describes a method in which segmented images of real recyclables (polyethylene terephthalate, PETE) are artificially deformed using mesh transformation to create new instances of the recyclable objects. The new instances are placed on real backgrounds to create synthetic images. This process allows the generation of large artificial datasets used for training neural networks. We evaluate the usability of these datasets by studying the extent to which they can improve the performance of trained models when applied in real and challenging industrial images. In particular, we consider the main metrics used to evaluate the performance of classification models, namely Accuracy, Precision and Recall. The results obtained show that including even small-scale object deformations in the artificial datasets can slightly improve the Accuracy and significantly improve the model Recall, while Precision of the model remains unchanged.;Synthetic data generation based on grid deformation for waste recycling applications;Not health related;Not health related;0
"S. M. Dias; L. E. Zarate";2006;Nowadays, artificial neural networks (ANN) are been widely used in the representation of physical process. Once trained, the nets are capable to solve unprecedented situations, keeping tolerable errors in their outputs. However, humans cannot assimilate the knowledge kept by those networks, since such knowledge is implicitly represented by their connection weights. Formal concept analysis (FCA) can be used in order to facilitate the extraction, representation and understanding of rules described by ANN. In this work, the approach FCANN to extract rules via FCA will be applied to the cold rolling process. The approach has a sequence of steps as the use of a synthetic database and intervals of discretization where the data number variation per parameter and the intervals variation of discretization is an adjustment factor to obtain more representative and precision rules. The approach can be used to understand the relationship among the process parameters through implication rules.;FCANN: An Approach to Knowledge Representation From ANN Through FCA Effects of Synthetic Data Base and Discretization Process, Application in the Cold Rolling Process;Not health related;Not health related;0
"S. Agarwal; J. Chaudhary; H. Savani; S. Sharma; M. Vatsa; R. Singh; S. P. Adhikari; S. Reddy; K. Agrawal; H. Misra";2023;This paper delves into the challenging task of selfie vs ID face verification which involves matching high-resolution selfies with low-resolution faces extracted from scanned ID documents. Existing face verification models often face performance degradation when confronted with this task, mainly due to disparities in data distributions, such as age-difference, degradation due to scanning, and difference in appearance. To address this issue and enhance performance, the paper explores the implementation of facial quality assessment and hard-pair mining techniques. In addition, the paper investigates the potential of synthetic data for training face verification models tailored for this specific task. The integration of synthetic data as an alternative training source is explored to improve robustness and overcome legal and privacy concerns arising from authentic datasets. By combining hard pair mining, facial quality assessment, and the utilization of synthetic data, this paper presents a comprehensive framework that aims to achieve improved face verification results in the complex scenario of selfie vs ID matching. The goal is to optimize the models’ performance and enhance their ability to accurately match selfies with the corresponding ID images, even under challenging conditions.;Leveraging Synthetic Data and Hard Pair Mining for Selfie vs ID Face Verification;Not health related;Not health related;0
"H. M. Beggs; L. J. Renzullo; C. Rüdiger; J. L. Lieser";2023;During 2022 and 2023, the Australian Bureau of Meteorology (Bureau) led a “Pre-Phase A” Mission Study to ascertain the technical and user requirements for an Australian sovereign Synthetic Aperture Radar (SAR) Pathfinder small satellite to support the Bureau’s observational needs across key terrestrial, marine and cryosphere SAR application areas. This paper describes the application areas considered and the respective SAR data requirements.;The Australian Bureau of Meteorology’s requirements for Synthetic Aperture Radar data;Not health related;Not health related;0
"G. F. Araujo; R. Machado; M. I. Pettersson";2023;Synthetic Aperture Radar (SAR) technology has unique advantages but faces challenges in obtaining enough data for noncooperative target classes. We propose a method to generate synthetic SAR data using a modified pix2pix Conditional Generative Adversarial Networks (cGAN) architecture. The cGAN is trained to create synthetic SAR images with specific azimuth and elevation angles, demonstrating its capability to closely mimic authentic SAR imagery through convergence and collapsing analyses. The study uses a model-based algorithm to assess the practicality of the generated synthetic data for Automatic Target Recognition (ATR). The results reveal that the classification accuracy achieved with synthetic data is comparable to that attained with original data, highlighting the effectiveness of the proposed method in mitigating the limitations imposed by noncooperative SAR data scarcity for ATR. This innovative approach offers a promising solution to craft customized synthetic SAR data, ultimately enhancing ATR performance in remote sensing.;Synthetic SAR Data Generator Using Pix2pix cGAN Architecture for Automatic Target Recognition;Not health related;Not health related;0
"H. Nguyen; J. M. Chang";2023;This work explores how class-imbalanced data affects deep learning and proposes a data balancing technique for mitigation by generating more synthetic data for the minority class. In contrast to random-based oversampling techniques, our approach prioritizes balancing the most informative region by finding high entropy samples. This approach is opportunistic and challenging because well-placed synthetic data points can boost machine learning algorithms’ accuracy and efficiency, whereas poorly-placed ones can cause a higher misclassification rate. In this study, we present an algorithm for maximizing the probability of generating a synthetic sample in the correct region of its class by placing it toward maximizing the class posterior ratio. In addition, to preserve data topology, synthetic data are closely generated within each minority sample neighborhood. Overall, experimental results on forty-one datasets show that our technique significantly outperforms experimental methods in terms of boosting deep-learning performance.;Synthetic Information towards Maximum Posterior Ratio for deep learning on Imbalanced Data;Not health related;Not health related;0
"P. Cascante-Bonilla; K. Shehada; J. S. Smith; S. Doveh; D. Kim; R. Panda; G. Varol; A. Oliva; V. Ordonez; R. Feris; L. Karlinsky";2023;Large-scale pre-trained Vision & Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost arbitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models. For example, their difficulty to understand Visual Language Concepts (VLC) that go ‘beyond nouns’ such as the meaning of non-object words (e.g., attributes, actions, relations, states, etc.), or difficulty in performing compositional reasoning such as understanding the significance of the order of the words in a sentence. In this work, we investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable data to improve VLC understanding and compositional reasoning of VL models. Additionally, we propose a general VL finetuning strategy for effectively leveraging SyViC towards achieving these improvements. Our extensive experiments and ablations on VL-Checklist, Winoground, and ARO benchmarks demonstrate that it is possible to adapt strong pre-trained VL models with synthetic data significantly enhancing their VLC understanding (e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their zero-shot accuracy.;Going Beyond Nouns With Vision & Language Models Using Synthetic Data;Not health related;Not health related;0
"A. G. Waldum; O. Pizarro; M. Ludvigsen";2023;The lack of suitable sonar datasets has made it difficult to train robust deep-learning models for object detection with sonar. Using synthetic sonar data to expand existing or create new data sets can facilitate robust training of machine learning based detectors for underwater sonar applications. In this work a pipeline is presented that enables the generation of synthetic sonar images from 3D models for both forward-looking sonar (FLS) and side scan sonar (SSS) by combining 3D renderings with the style transfer capabilities of generative adversarial neural networks. Images generated by this pipeline are used to train object detectors which are evaluated on two data sets, one with shipwrecks and the other with subsea infrastructure collected with a synthetic-aperture sonar and forward-looking sonar respectively. The results of this study show that an object detector trained purely on the synthetic FLS data generated by this approach, was able to detect and correctly classify the structures observed in the majority of the subsea infrastructure images. A detector trained on real images of the same structures performed better with a recall of 87.4% against 64.8% . The object detector trained purely on synthetic side scan sonar data yielded a recall of 83% on our shipwreck dataset at 80% confidence, but yielded a high rate of false positives when the confidence threshold was lowered. The results indicate that an object detector trained on synthetic data of specific objects generated with this approach is able to learn features that translate to detection in real images.;Sonar Object Detection from Synthetic Sonar Data Generated by a Sonar Simulator and an Adversarial Neural Network;Not health related;Not health related;0
"X. Ren; J. Y. J. Chow";2023;"Incorporating individual user preferences in statewide transportation planning is of great importance regarding revenue management and behavioral equity. However, an enduring challenge is that consistent population travel data remains scarce, particularly in underserved and rural areas. Moreover, large-scale optimization models are computationally demanding when considering stochastic travel demands in a discrete choice model (DCM) framework. These can be addressed with a combination of synthetic population data and deterministic taste coefficients. We formulate a choice-based optimization model, in which the mode share in each block group-level trip origin-destination (OD) is determined by a set of deterministic coefficients reflecting user preferences. In that case, statewide service region design becomes an assortment optimization problem with known parameters and linear constraints, which can be efficiently solved through linear or quadratic programming (depending on variant). We test the method using a hypothetical new mobility service considered for New York State. The proposed model is applied to optimize its service region with one of the three objectives: (1) maximizing the total revenue; (2) maximizing the total change of consumer surplus; (3) minimizing the disparity between disadvantaged and non-disadvantaged communities.";Choice-Based Service Region Assortment Problem: Equitable Design with Statewide Synthetic Data;Not health related;Not health related;0
"A. K. Das; J. Mitra; A. H. Victoria";2023;"Automatic synthesis of realistic images is complex; even cutting-edge AI and machine learning systems sometimes suffer from not fulfilling this expectation. However, the development of image processing has made it possible to perform operations on an image to improve or extract information from it and synthesize images from textual descriptions, making this a prominent area of study. Based on the idea of text-guided image production, the image inpainting task is replacing the damaged parts of an image with new, appropriate features that make sense in the given context. Researchers in this area have made some encouraging advances using neural image inpainting techniques. In contrast to previously developed text-guided image production methods, inpainting models must first compare the semantic content of the provided text to the remaining portion of the image to determine the semantic information for the missing section. Providing a mask and a text prompt with the information you need to make the desired changes can alter certain parts of an image. To boost the semantic similarity between the produced image and the text, an image-text matching loss is utilised. Further research is needed to resolve the challenge of guessing the missed content with only the context pixels. We propose a computationally efficient system to perform image inpainting on synthetic images that have been generated from textual data while ensuring visual coherence and accuracy.";Image Inpainting of Synthetic Images Generated from Textual Data;Not health related;Not health related;0
"J. Liu; H. Liu; H. Fu; Y. Ye; K. Chen; Y. Lu; J. Mao; R. X. Xu; M. Sun";2024;Retinal arteriovenous nicking (AVN) manifests as a reduced venular caliber of an arteriovenous crossing. AVNs are signs of many systemic, particularly cardiovascular diseases. Studies have shown that people with AVN are twice as likely to have a stroke. However, AVN classification faces two challenges. One is the lack of data, especially AVNs compared to the normal arteriovenous (AV) crossings. The other is the significant intra-class variations and minute inter-class differences. AVNs may look different in shape, scale, pose, and color. On the other hand, the AVN could be different from the normal AV crossing only by slight thinning of the vein. To address these challenges, first, we develop a data synthesis method to generate AV crossings, including normal and AVNs. Second, to mitigate the domain shift between the synthetic and real data, an edge-guided unsupervised domain adaptation network is designed to guide the transfer of domain invariant information. Third, a semantic contrastive learning branch (SCLB) is introduced and a set of semantically related images, as a semantic triplet, are input to the network simultaneously to guide the network to focus on the subtle differences in venular width and to ignore the differences in appearance. These strategies effectively mitigate the lack of data, domain shift between synthetic and real data, and significant intra- but minute inter-class differences. Extensive experiments have been performed to demonstrate the outstanding performance of the proposed method.;Edge-Guided Contrastive Adaptation Network for Arteriovenous Nicking Classification Using Synthetic Data;Not health related;Not health related;0
"E. Bonetto; A. Ahmad";2023;Nowadays, there is a wide availability of datasets that enable the training of common object detectors or human detectors. These come in the form of labelled real-world images and require either a significant amount of human effort, with a high probability of errors such as missing labels, or very constrained scenarios, e.g. VICON systems. On the other hand, uncommon scenarios, like aerial views, animals, like wild zebras, or difficult-to-obtain information, such as human shapes, are hardly available. To overcome this, synthetic data generation with realistic rendering technologies has recently gained traction and advanced research areas such as target tracking and human pose estimation. However, subjects such as wild animals are still usually not well represented in such datasets. In this work, we first show that a pre-trained YOLO detector can not identify zebras in real images recorded from aerial viewpoints. To solve this, we present an approach for training an animal detector using only synthetic data. We start by generating a novel synthetic zebra dataset using GRADE, a state-of-the-art framework for data generation. The dataset includes RGB, depth, skeletal joint locations, pose, shape and instance segmentations for each subject. We use this to train a YOLO detector from scratch. Through extensive evaluations of our model with real-world data from i) limited datasets available on the internet and ii) a new one collected and manually labelled by us, we show that we can detect zebras by using only synthetic data during training. The code, results, trained models, and both the generated and training data are provided as open-source at https://eliabntt.github.io/grade-rr.;Synthetic Data-Based Detection of Zebras in Drone Imagery;Not health related;Not health related;0
"H. Ghanadian; I. Nejadgholi; H. A. Osman";2024;Suicidal ideation detection is a vital research area that holds great potential for improving mental health support systems. However, the sensitivity surrounding suicide-related data poses challenges in accessing large-scale, annotated datasets necessary for training effective machine learning models. To address this limitation, we introduce an innovative strategy that leverages the capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to create synthetic data for suicidal ideation detection. Our data generation approach is grounded in social factors extracted from psychology literature and aims to ensure coverage of essential information related to suicidal ideation. In our study, we benchmarked against state-of-the-art NLP classification models, specifically, those centered around the BERT family structures. When trained on the real-world dataset, UMD, these conventional models tend to yield F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, informed by social factors, offers consistent F1-scores of 0.82 for both models, suggesting that the richness of topics in synthetic data can bridge the performance gap across different model complexities. Most impressively, when we combined a mere 30% of the UMD dataset with our synthetic data, we witnessed a substantial increase in performance, achieving an F1-score of 0.88 on the UMD test set. Such results underscore the cost-effectiveness and potential of our approach in confronting major challenges in the field, such as data scarcity and the quest for diversity in data representation.;Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models;Not health related;Not health related;0
"A. Amer; O. Álvarez-Tuñón; H. _. U_urlu; J. Le Fevre Sejersen; Y. Brodskiy; E. Kayacan";2023;Underwater robotic surveys can be costly due to the complex working environment and the need for various sensor modalities. While underwater simulators are essential, many existing simulators lack sufficient rendering quality, restricting their ability to transfer algorithms from simulation to real-world applications. To address this limitation, we introduce UNav-Sim, which, to the best of our knowledge, is the first simulator to incorporate the efficient, high-detail rendering of Unreal Engine 5 (UE5). UNav-Sim is open-source11https://github.com/open-airlab/UNav-Sim and includes an autonomous vision-based navigation stack. By supporting standard robotics tools like ROS, UNav-Sim enables researchers to develop and test algorithms for underwater environments efficiently.;UNav-Sim: A Visually Realistic Underwater Robotics Simulator and Synthetic Data-Generation Framework;Not health related;Not health related;0
"N. P. Martins; Y. Kalaidzidis; M. Zerial; F. Jug";2023;Microscopy images are crucial for life science research, allowing detailed inspection and characterization of cellular and tissue-level structures and functions. However, microscopy data are unavoidably affected by image degradations, such as noise, blur, or others. Many such degradations also contribute to a loss of image contrast, which becomes especially pronounced in deeper regions of thick samples. Today, best performing methods to increase the quality of images are based on Deep Learning approaches, which typically require ground truth (GT) data during training. Our inability to counteract blurring and contrast loss when imaging deep into samples prevents the acquisition of such clean GT data. The fact that the forward process of blurring and contrast loss deep into tissue can be mod-eled, allowed us to propose a new method that can circumvent the problem of unobtainable GT data. To this end, we first synthetically degraded the quality of microscopy images even further by using an approximate forward model for deep tissue image degradations. Then we trained a neural network that learned the inverse of this degradation function from our generated pairs of raw and degraded images. We demonstrated that networks trained in this way can be used out-of-distribution (OOD) to improve the quality of less severely degraded images, e.g. the raw data imaged in a microscope. Since the absolute level of degradation in such microscopy images can be stronger than the additional degradation introduced by our forward model, we also explored the effect of iterative predictions. Here, we observed that in each iteration the measured image contrast kept improving while detailed structures in the images got increasingly removed. Therefore, dependent on the desired downstream analysis, a balance between contrast improvement and retention of image details has to be found.;DeepContrast: Deep Tissue Contrast Enhancement using Synthetic Data Degradations and OOD Model Predictions;Not health related;Not health related;1
"J. Ireland; I. Radwan; D. Herath; R. Goecke";2022;Counting is a common preventative measure taken to ensure surgical instruments are not retained during surgery, which could cause serious detrimental effects including chronic pain and sepsis. A hybrid human-AI system could support or partially automate this manual counting of instruments. An important element to evaluate the viability of using deep learning computer vision-based counting is a suitable large-scale dataset of surgical instruments. Other domains, such as crowd analysis and instance counting, have leveraged synthetic datasets to evaluate and augment different approaches. We present a synthetic dataset (SORT), which is complemented by a smaller real-world dataset of surgical instruments (MSMI), to assess the hypothesis of whether synthetic training data can improve the performance of multi-class multi-instance counting models when applied to real-world data. In this preliminary study, we provide comparative baselines for various popular counting techniques on synthetic data, such as direct regression, segmentation, localisation, and density estimation. These experiments are repeated at different resolutions – full high-definition ($1080\times 1920$ pixels), half ($690 \times 540$ pixels), and a quarter ($480\times 270$ pixels) – to measure the robustness of different supervision methods to varying image scales. The results indicate that neither the degree of supervision nor the image resolution during model training impact performance significantly on the synthetic data. However, when testing on the real-world instrument dataset, the models trained on synthetic data were significantly less accurate. These results indicate a need for further work in either the refinement of the synthetic depictions or fine-tuning upon real-world data to achieve similar performance in domain adaptation scenarios compared to training and testing solely on the synthetic data.;Can Synthetic Data Improve Multi-Class Counting of Surgical Instruments?;Not health related;Not health related;0
"Y. Zeng; P. Thiyagarajan; B. M. Chan; R. Jin";2023;The deployment of Industrial Internet offers abundant passive data from manufacturing systems and networks, which enables data-driven modeling with high-data-demand, advanced statistical models such as Deep Neural Networks (DNNs). Deep Neural Networks (DNNs) have proven to be remarkably effective in supervised learning in critical manufacturing applications, such as AI-enabled automatic inspection, quality modeling, etc. However, there is a lack of performance guarantee of DNN models primarily due to data class imbalance, shifting distribution, and multi-modality variables (e.g., time series and images) in training and testing datasets collected in manufacturing. Inspired by active data generation through Design of Experiments (DoE) and passive observational data collection for manufacturing data analytics, we propose a SynthetIc Data gEneration and Sampling (SIDES) framework, to provide adequate DNN model performance through the improvement of training data preparation. In the SIDES framework, a bi-level Hierarchical Contextual Bandits is proposed to provide a scientific way to integrate DoE and observational data sampling, which optimizes DNNs' online learning performance. Multimodality-Aligned Variational Autoencoder transforms the multimodal predictors from manufacturing into a shared low-dimensional latent space for controlled data generation from DoE and effective sampling from observational data. The merits of SIDES are evaluated by a real case study of printed electronics with a binary multimodal data classification problem. Results show the advantages of the cost-effective integration of DoE in improving the DNNs' online learning performance.;Synthetic Data Generation and Sampling for Online Training of DNNs in Manufacturing Supervised Learning Problems;Not health related;Not health related;0
"M. N. Riaz; M. Wielgosz; A. G. Romera; A. M. López";2023;Pedestrian intention prediction is crucial for autonomous driving. In particular, knowing if pedestrians are going to cross in front of the ego-vehicle is core to performing safe and comfortable maneuvers. Creating accurate and fast models that predict such intentions from sequential images is challenging. A factor contributing to this is the lack of datasets with diverse crossing and non-crossing (C/NC) scenarios. We address this scarceness by introducing a framework, named ARCANE, which allows programmatically generating synthetic datasets consisting of C/NC video clip samples. As an example, we use ARCANE to generate a large and diverse dataset named PedSynth. We will show how PedSynth complements widely used real-world datasets such as JAAD and PIE, so enabling more accurate models for C/NC prediction. Considering the onboard deployment of C/NC prediction models, we also propose a deep model named PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a GNN-GRU architecture that takes a sequence of pedestrian skeletons as input to predict crossing intentions. ARCANE, PedSynth, and PedGNN is publicly released11https://github.com/NomiMalik0207/PedSynth-and-PedGNN-for-Pedestrian-Intention-Prediction.;Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction;Not health related;Not health related;0
"M. Sriram; P. Ramesh Kumar; S. Hussain; K. L. Sailaja";2023;Soil moisture Content is an essential entity for plant growth and has a direct impact on crop yield. Traditional moisture estimation methods involve Gravimetric method, Water balance calculation method, Estimation using thermal and electrical conductance properties of soil. These traditional methods are accurate but can be time consuming and fail for estimation over large areas of land like forests and grasslands. A more suitable technique for estimating soil moisture over a large area is through use of Synthetic Aperture Radar which uses microwaves to map the images of land instead of visible light. This presence of moisture interferes with the backscattering of microwaves which in turn gives details about the moisture content. Existing methods involving SAR Data use empirical and semi-empirical methods to calculate SMC but these require properties of soil like soil roughness to be known beforehand. Therefore, this research work intends to employ a novel method to detect soil moisture content without coming into direct contact with it. The proposed model employs a Regression Convolutional Neural Network (RCNN) architecture that uses SAR data taken by Sentinel-1 C- band radar to forecast soil moisture content across agricultural areas. This method brings the advantage of estimating SMC over remote areas where knowing the properties of soil is not possible. The outcome of this project is a GUI based interface, where the user can upload the VV and VH images and get the predicted soil moisture output.;SMMap: Soil Moisture Content Mapping/Estimation Using Synthetic Aperture Radar (SAR) Data;Not health related;Not health related;0
"D. Sagmeister; D. Schörkhuber; M. Nezveda; F. Stiedl; M. Schimkowitsch; M. Gelautz";2023;The training of computer vision models for human pose estimation requires large amounts of data. Since labelling image data with pose keypoints is very time consuming and costly, we aim to alleviate this requirement by using synthetic data during pre-training and thus relax the need for large amounts of real data samples during fine-tuning. To this end, we investigate the impact of synthetic data on the performance of a 2D keypoint detection model in the context of driver body pose estimation. We present our approach for synthetic data generation to automatically provide large amounts of in-cabin views as training data. The utilization of the generated synthetic data is evaluated in different learning schemes. We achieve a notable performance gain of +30.5% by pre-training with our in-cabin synthetic data when only 1% of real training data from the DriPE dataset is available. The proposed approach also outperforms pre-training with PeopleSansPeople by +8.3% when the reduced DriPE dataset is used for fine-tuning.;Transfer Learning for Driver Pose Estimation from Synthetic Data;Not health related;Not health related;0
"X. Zheng; A. Pinceti; L. Sankar; L. Xie";2023;In this study, a machine learning based method is proposed for creating synthetic eventful phasor measurement unit (PMU) data under time-varying load conditions. The proposed method leverages generative adversarial networks to create quasi-steady states for the power system under slowly-varying load conditions and incorporates a framework of neural ordinary differential equations (ODEs) to capture the transient behaviors of the system during voltage oscillation events. A numerical example of a large power grid suggests that this method can create realistic synthetic eventful PMU voltage measurements based on the associated real PMU data without any knowledge of the underlying nonlinear dynamic equations. The results demonstrate that the synthetic voltage measurements have the key characteristics of real system behavior on distinct time scales.;Synthetic PMU Data Creation Based on Generative Adversarial Network Under Time-varying Load Conditions;Not health related;Not health related;0
"V. M. Sánchez-Cartagena; M. Esplà-Gomis; J. A. Pérez-Ortiz; F. Sánchez-Martínez";2024;When the amount of parallel sentences available to train a neural machine translation is scarce, a common practice is to generate new synthetic training samples from them. A number of approaches have been proposed to produce synthetic parallel sentences that are similar to those in the parallel data available. These approaches work under the assumption that non-fluent target-side synthetic training samples can be harmful and may deteriorate translation performance. Even so, in this paper we demonstrate that synthetic training samples with non-fluent target sentences can improve translation performance if they are used in a multilingual machine translation framework as if they were sentences in another language. We conducted experiments on ten low-resource and four high-resource translation tasks and found out that this simple approach consistently improves translation performance as compared to state-of-the-art methods for generating synthetic training samples similar to those found in corpora. Furthermore, this improvement is independent of the size of the original training corpus, the resulting systems are much more robust against domain shift and produce less hallucinations.;Non-Fluent Synthetic Target-Language Data Improve Neural Machine Translation;Not health related;Not health related;0
"A. Würth; M. Binois; P. Goatin";2023;We analyze two calibration approaches for parameter identification and traffic speed reconstruction in macroscopic traffic flow models. We consider artificially created noisy loop detector data as our field measurements. Due to the knowledge of the ground truth calibration parameter, we can give a sound assessment with respect to the performance of the considered methods. Our analysis shows that, in the proposed setting, the first order traffic flow model together with the proposed Kennedy O’Hagan approach performs better in reconstructing the speed traffic quantity than the other approaches.;Validation of Calibration Strategies for Macroscopic Traffic Flow Models on Synthetic Data;Not health related;Not health related;0
"P. Guo; G. Visser; E. Saygin";2020;"Seismic full waveform inversion (FWI) is a state-of-the-art technique for estimating subsurface physical models from recorded seismic waveform, but its application requires care because of high non-linearity and non-uniqueness. The final outcome of global convergence from conventional FWI using local gradient information relies on an informative starting model. Bayesian inference using Markov chain Monte Carlo (MCMC) sampling is able to remove such dependence, by a direct extensive search of the model space. We use a Bayesian trans-dimensional MCMC seismic FWI method with a parsimonious dipping layer parametrization, to invert for subsurface velocity models from pre-stack seismic shot gathers that contain mainly reflections. For the synthetic study, we use a simple four-layer model and a modified Marmousi model. A recently collected multichannel off-shore seismic reflection data set, from the Lord Howe Rise (LHR) in the east of Australia, is used for the field data test. The trans-dimensional FWI method is able to provide model ensembles for describing posterior distribution, when the dipping-layer model assumption satisfies the observed data. The model assumption requires narrow models, thus only near-offset data to be used. We use model stitching with lateral and depth constraints to create larger 2-D models from many adjacent overlapping submodel inversions. The inverted 2-D velocity model from the Bayesian inference can then be used as a starting model for the gradient-based FWI, from which we are able to obtain high-resolution subsurface velocity models, as demonstrated using the synthetic data. However, lacking far-offset data limits the constraints for the low-wavenumber part of the velocity model, making the inversion highly non-unique. We found it challenging to apply the dipping-layer based Bayesian FWI to the field data. The approximations in the source wavelet and forward modelling physics increase the multimodality of the posterior distribution; the sampled velocity models clearly show the trade-off between interface depth and velocity. Numerical examples using the synthetic and field data indicate that trans-dimensional FWI has the potential for inverting earth models from reflection waveform. However, a sparse model parametrization and far offset constraints are required, especially for field application.";Bayesian trans-dimensional full waveform inversion: synthetic and field data application;Not health related;Not health related;0
"D. A. Co; S. Ng; G. L. Tan; A. P. Ty; J. B. Cruz; C. Cheng";2022;Response generation is a task in natural language processing where a model is trained to respond to human statements. Conversational response generators take this one step further with the ability to respond within the context of previous responses. While techniques exist for training such models, they all require large amounts of conversational data, which are not always available for low-resource languages. In this research, we make three contributions. First, we released the first Filipino conversational dataset collected from a popular Philippine online forum called the “PEx Conversations Dataset.” Second, we introduce a data augmentation methodology for Filipino data by employing a Tagalog RoBERTa model to increase the size of the existing corpora. Lastly, we published the first Filipino conversational response generator capable of generating responses related to the previous three responses. With the supplementary synthetic data, we were able to improve the performance of the response generator by up to 10.7% in perplexity, and 11.7% in content word usage as compared to training with zero synthetic data.;Using Synthetic Data to Train a Conversational Response Generation Model in Low Resource Settings;Not health related;Not health related;0
"H. Radak; C. Scheunert; G. T. Nguyen; V. Nguyen; F. H. P. Fitzek";2023;Accurate orientation estimation is crucial in many application areas, including unmanned ground and aerial navigation for industrial automation and human motion tracking for human-robot interaction. State-of-the-art techniques leverage Inertial Measurement Units (IMU) due to their small size, low energy footprint, and ever-increasing accuracy, which provide Magnetic, Angular Rate, and Gravity (MARG) sensor measurements. Available attitude determination techniques rely on advanced signal processing algorithms to compensate for the gyroscope integration drift. The comparison of different algorithms depends solely on the collected ground-truth data set, which is difficult to replicate. This paper introduces a lightweight software framework to generate synthetic IMU sensor data. We generate the ground-truth orientation of the sensor body frame and apply an inverse navigation process to obtain corresponding synthetic sensor data. Additionally, we compare two well-known orientation estimation algorithms applied to the synthetically generated data from our framework. Evaluation results demonstrate that the proposed software framework represents a fast and easy-to-use solution to the problem of evaluation of different orientation estimation algorithms while providing access to ground truth measurements.;Lightweight Generator of Synthetic IMU Sensor Data for Accurate AHRS Analysis;Not health related;Not health related;0
"A. Feng; A. S. Gordon";2020;Accurate recognition of group behaviors is essential to the design of engaging networked multiplayer games. However, contemporary data-driven machine learning solutions are difficult to apply during the game development process, given that no authentic gameplay data is yet available for use as training data. In this paper, we investigate the use of synthetic training data, i.e., gameplay data that is generated by AI-controlled agent teams programmed to perform each of the behaviors to be recognized in groups of human players. The particular task we focus on is to recognize group movement formations in player-controlled avatars in a realistic virtual world. We choose five typical military team movement patterns for the formation recognition task and train machine learning models using procedurally generated unit trajectories as training data. The experiments were conducted using ResNet and EfficientNet, which are two popular convolutional neural network architectures for image classifications. The synthetic data is augmented by creating variations in image rotation, unit spacing, team size, and positional perturbations to bridge the gap between synthetic and human gameplay data. We demonstrate that high-accuracy behavior recognition can be achieved using deep neural networks by applying the aforementioned data augmentation methods to simulated gameplay data.;Recognizing Multiplayer Behaviors Using Synthetic Training Data;Not health related;Not health related;0
"L. Schölch; J. Steinhäuser; M. Beichter; C. Seibold; K. Yang; M. Knaeble; T. Schwarz; A. Maedche; R. Stiefelhagen";2022;Structured Visual Content (SVC) such as graphs, flow charts, or the like are used by authors to illustrate various concepts. While such depictions allow the average reader to better understand the contents, images containing SVCs are typically not machine-readable. This, in turn, not only hinders automated knowledge aggregation, but also the perception of displayed information for visually impaired people. In this work, we propose a synthetic dataset, containing SVCs in the form of images as well as ground truths. We show the usage of this dataset by an application that automatically extracts a graph representation from an SVC image. This is done by training a model via common supervised learning methods. As there currently exist no large-scale public datasets for the detailed analysis of SVC, we propose the Synthetic SVC (SSVC) dataset comprising 12,000 images with respective bounding box annotations and detailed graph representations. Our dataset enables the development of strong models for the interpretation of SVCs while skipping the time-consuming dense data annotation.We evaluate our model on both synthetic and manually annotated data and show the transferability of synthetic to real via various metrics, given the presented application. Here, we evaluate that this proof of concept is possible to some extend and lay down a solid baseline for this task. We discuss the limitations of our approach for further improvements. Our utilized metrics can be used as a tool for future comparisons in this domain. To enable further research on this task, the dataset is publicly available at https://bit.ly/3jN1pJJ.;Towards Automatic Parsing of Structured Visual Content through the Use of Synthetic Data;Not health related;Not health related;0
"F. H. Nezhad; Y. Rotalinti; P. Myles; A. Tucker";2023;In this paper, we quantify the privacy gain of synthetic patient data drawn from two generative models, MST and PrivBayes, which is based on real anonymized primary care patient data. This evaluation is implemented for two types of inference attacks, namely membership and attribute inference attacks using a new toolbox, TAPAS. The aim is to quantitatively evaluate the privacy gain of each attack where these two differentially private generators and different threat models are used with a focus on black-box knowledge. The evaluation that was carried out in this paper demonstrates that vulnerabilities of synthetic patient data depend on the different attack scenarios, threat models, and algorithms used to generate the synthetic patient data. It was shown empirically that although the synthetic patient data achieved high privacy gain in most attack scenarios, it does not behave uniformly against adversarial attacks, and some records and outliers remain vulnerable depending on the attack scenario. Moreover, it was shown that the PrivBayes generator is the more robust generator in comparison to MST in terms of the privacy-preservation of synthetic data.;Privacy Assessment of Synthetic Patient Data;health related;Not health related;1
"Y. Karabatis; X. Lin; N. J. Sanket; M. G. Lagoudakis; Y. Aloimonos";2023;Modern robotics has enabled the advancement in yield estimation for precision agriculture. However, when applied to the olive industry, the high variation of olive colors and their similarity to the background leaf canopy presents a challenge. Labeling several thousands of very dense olive grove images for segmentation is a labor-intensive task. This paper presents a novel approach to detecting olives without the need to manually label data. In this work, we present the world's first olive detection dataset comprised of synthetic and real olive tree images. This is accomplished by generating an auto-labeled photorealistic 3D model of an olive tree. Its geometry is then simplified for lightweight rendering purposes. In addition, experiments are conducted with a mix of synthetically generated and real images, yielding an improvement of up to 66% compared to when only using a small sample of real data. When access to real, human-labeled data is limited, a combination of mostly synthetic data and a small amount of real data can enhance olive detection.;Detecting Olives with Synthetic or Real Data? Olive the Above;Not health related;Not health related;0
"A. R. Darlis; N. Ibrahim; A. Subiantoro; F. Yusivar; N. N. Albaqami; A. S. Prabuwono; B. Kusumoputro";2023;Human and animal classification under the rubble is necessary to successfully rescue survivors post-disaster, and interference from other animals is becoming an issue in noncontact monitoring with radar. Many animals in indoor and outdoor environments have characteristics similar to humans, where they are easily mistaken for human targets, which would trigger a false alarm. A novel human and animal classification through single-receiver and dual-receiver mmWave radar at 77 GHz is presented. The system uses feedback signal responses from targets with dual-receiver mmWave radar and classifies human and animal features using a convolution neural network (CNN) based on synthetic 2D Tensor data. The performance is compared to the classification result using a deep learning and backpropagation neural network (BPNN) from a single mmWave radar dataset under the measurement distance and number of objects. Our experimental results showed that using dual receivers was very useful and performed well in classifying humans and animals, with an average accuracy rate of 99% in the classification of 2 and 3 classes, 83% in 6 classes, and 68% in 10 classes. The performance showed an excellent result considering that this system’s application avoids false alarms in human rescue applications during post-disaster periods and exceeds the accuracy of several previous studies.;Autonomous Human and Animal Classification Using Synthetic 2D Tensor Data Based on Dual-Receiver mmWave Radar System;Not health related;Not health related;0
"Y. Chen; N. Li; J. Zhang; W. Chen; Y. Li; H. Li";2022;Artificial intelligence-driven collaborative robots (cobots) have attracted significant interest. Object perception is one of the important capabilities for robotic grasping in complex environments. Vision-based methods in the main perception tasks of robotic systems mostly require large pre-labeled training datasets. Building large-scale datasets that satisfy the conditions has always been a challenge in this field. In this work, we propose a robot vision system for robotic grasping tasks. The proposed system's primary design goal is to minimize the cost of human annotation during system setup. Moreover, since it is difficult to collect sufficient labeled training data, the existing methods are typically trained on real data that are highly correlated with test data. The system we presented includes a one-shot deep neural network trained with high-fidelity synthetic data based entirely on domain randomization to avoid collecting large amounts of human-annotated data and inaccurate annotation data in real world. At last, we build the vision system in the real environment and simulation with the robot operating system (ROS).;Grasp Detection for Assembly Robots Using High-fidelity Synthetic Data;Not health related;Not health related;0
"M. P. Anguswamy; M. Datta; L. Meegahapola; A. Vahidnia";2023;Widespread deployments of optimally placed real-time power quality (PQ) monitoring tools such as distribution level micro-phasor measurement units (D-PMUs or  $\mu $ PMU), digital fault recorders, and PQ analyzers are expected to play a critical role in improving the stability and reliability of the smart grid. In this paper, an improved PQ disturbance (PQD) classification method using discrete wavelet transform (DWT) with a cubic multi-class support vector machine (CMSVM) classifier is proposed, which incorporates a decade’s worth of high-quality continuous waveform PQ data from the Australian power network. This research also introduces misclassification cost (MC) and cost-sensitive classification theory into the area of PQD classifiers to build improved and more robust network models for the future. The method is evaluated using four case studies of synthetic and real-world PQD field data combinations and five application case studies using optimally placed  $\mu $ PMUs. The results indicate similar classification performance for standard PQDs than previous literature, alongside improved MC for complex PQD classes. Comparative analysis with previous literature highlights the importance of using high-quality real PQD field data to improve the fidelity of classifiers to provide better PQ insights as more complex components are added to the distribution network.;Distribution Network Power Quality Insights With Optimally Placed Micro-PMUs Incorporating Synthetic and Real Field Data;Not health related;Not health related;0
"B. Groß; G. Wunder";2023;Synthetic data has been hailed as the silver bullet for privacy preserving data analysis. If a record is not real, then how could it violate a person’s privacy? In addition, deep-learning based generative models are employed successfully to approximate complex high-dimensional distributions from data and draw realistic samples from this learned distribution. It is often overlooked though that generative models are prone to memorising many details of individual training records and often generate synthetic data that too closely resembles the underlying sensitive training data, hence violating strong privacy regulations as, e.g., encountered in health care. Differential privacy is the well-known state-of-the-art framework for guaranteeing protection of sensitive individuals’ data, allowing aggregate statistics and even machine learning models to be released publicly without compromising privacy. The training mechanisms however often add too much noise during the training process, and thus severely compromise the utility of these private models. Even worse, the tight privacy budgets do not allow for many training epochs so that model quality cannot be properly controlled in practice. In this paper we explore an alternative approach for privately generating data that makes direct use of the inherent stochasticity in generative models, e.g., variational autoencoders. The main idea is to appropriately constrain the continuity modulus of the deep models instead of adding another noise mechanism on top. For this approach, we derive mathematically rigorous privacy guarantees and illustrate its effectiveness with practical experiments.;Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders;Not health related;Not health related;0
"P. de Reus; A. Oprescu; K. van Elsen";2023;To address increasing societal concerns regarding privacy and climate, the EU adopted the General Data Protection Regulation (GDPR) and committed to the Green Deal. Considerable research studied the energy efficiency of software and the accuracy of machine learning models trained on anonymised data sets. Recent work began exploring the impact of privacy-enhancing techniques (PET) on both the energy consumption and accuracy of the machine learning models, focusing on k-anonymity. As synthetic data is becoming an increasingly popular PET, this paper analyses the energy consumption and accuracy of two phases: a) applying privacy-enhancing techniques to the concerned data set, b) training the models on the concerned privacy-enhanced data set. We use two privacy-enhancing techniques: k-anonymisation (using generalisation and suppression) and synthetic data, and three machine-learning models. Each model is trained on each privacy-enhanced data set. Our results show that models trained on k-anonymised data consume less energy than models trained on the original data, with a similar performance regarding accuracy. Models trained on synthetic data have a similar energy consumption and a similar to lower accuracy compared to models trained on the original data.;Energy Cost and Machine Learning Accuracy Impact of k-Anonymisation and Synthetic Data Techniques;Not health related;Not health related;0
"K. Lakkaraju; A. Gupta; B. Srivastava; M. Valtorta; D. Wu";2023;Sentiment Analysis Systems (SASs) are data-driven Artificial Intelligence (AI) systems that output polarity and emotional intensity when given a piece of text as input. Like other AIs, SASs are also known to have unstable behavior when subjected to changes in data which can make them problematic to trust out of concerns like bias when AI works with humans and data has protected attributes like gender, race, and age. Recently, an approach was introduced to assess SASs in a blackbox setting without training data or code, and rating them for bias using synthetic English data. We augment it by introducing two human-generated chatbot datasets and also considering a round-trip setting of translating the data from one language to the same through an intermediate language. We find that these settings show SASs performance in a more realistic light. Specifically, we find that rating SASs on the chatbot data showed more bias compared to the synthetic data, and round-tripping using Spanish and Danish as intermediate languages reduces the bias (up to 68% reduction) in human-generated data while, in synthetic data, it takes a surprising turn by increasing the bias! Our findings will help researchers and practitioners refine their SAS testing strategies and foster trust as SASs are considered part of more mission-critical applications for global use.;The Effect of Human v/s Synthetic Test Data and Round-Tripping on Assessment of Sentiment Analysis Systems for Bias;Not health related;Not health related;0
"M. E. Khoda; J. Kamruzzaman; I. Gondal; T. Imam; A. Rahman";2020;Mobile malware detection is inherently an imbalanced data problem since the number of benign applications in the market is far greater than the number of malicious applications. Existing methods to handle imbalanced data, such as synthetic minority over-sampling, do not translate well into this domain since mobile malware detection generally deals with binary features and these methods are designed for continuous features. Also, methods adapted for categorical features cannot be applied here since random modifications of features can result in invalid sample generation. In this work, we propose a novel technique for generating synthetic samples for mobile malware detection with imbalanced data. Our proposed method adds new data points in the sample space by generating synthetic malware samples which also preserves the original functionality of the malicious apps. Experiments show that the proposed approach outperforms existing techniques in terms of precision, recall, F1score, and AUC. This study will be useful in building deep neural network-based systems to handle imbalanced data for mobile malware detection.;Mobile Malware Detection with Imbalanced Data using a Novel Synthetic Oversampling Strategy and Deep Learning;Not health related;Not health related;0
"L. Zherdeva; D. Zherdev; A. Nikonorov";2021;"The truly relevant dataset creation tasks are aimed at assessing human action. The approach to detect and recognize a person falling allows to promptly warn about dangerous incidents for further analysis of the consequences that can help in the tasks of healthcare industry. The problem can be solved using modeled data based on digital human simulation. The result of a proposed modular pipeline for synthetic data generation of digital human interaction with the 3D environment was demonstrated in this paper. The research includes the following contributions: the synthetic dataset based on procedural generation of realistic movements and fall which taking into account physics model of a digital human; registering basic rgb and segmentation rendering maps while simulating a digital human fall; in segmentation maps, we present hitting coordinate masks with the interaction of the human model and 3D scene. The pipeline is implemented using Unreal Engine that provides automatic “playback” of various scenarios for simulation. We used the generated synthetic data to train the Mask R-CNN framework. It is shown that a fallen person can be recognized with an accuracy of 97.6% and the type of person’s impact can be classified, including hitting the head when falling with training the model on simulation data. The proposed method also allows covering a variety of scenarios that can have a positive effect at a CNN training stage in the tasks of data creation for human action estimation.";Prediction of human behavior with synthetic data;Not health related;Not health related;0
"J. Metzler; F. Bahrpeyma; D. Reichelt";2023;Object detection is a task in computer vision that involves detecting instances of visual objects of a particular class in digital images. Numerous computer vision tasks highly depend on object detection such as instance segmentation, image captioning and object tracking. A major purpose of object detection is to develop computational models that provide inputs crucial to computer vision applications. Convolutional Neural Networks (CNNs) have recently become popular due to their key roles in enabling object detection. However, the performance of CNNs is largely dependent upon the quality and quantity of training datasets, which are often difficult to obtain in real-world applications. In order to ensure the robustness of such models, it is vital that training instances are provided under various randomized conditions. These conditions are typically a combination of a variety of factors, including lighting conditions, object location, the presence of multiple objects in the scene, varieties of backgrounds, and the angle of the camera. In particular, companies, depending on their applications (such as fault detection, anomaly detection, condition monitoring, predictive quality and so on), require specialized models for their custom products and so always face difficulties in providing a large number of randomized conditioned instances of their objects. The primary reason is that the process of capturing randomized conditioned images of real objects is usually costly, time-consuming, and challenging in practice. Due to the efficiency gained so far via the use of synthetic data for training such systems, synthetic data has recently attracted considerable attention. This paper presents an end-to-end synthetic data generation method for building a robust object detection model for customized products using NVIDIA Omniverse and CNNs. In this paper, we demonstrate and evaluate our contribution to the modeling of chess pieces, where a total accuracy of 98.8 % was obtained.;An end to end workflow for synthetic data generation for robust object detection*;Not health related;Not health related;0
"E. Aydin Gol; D. Densmore; C. Belta";2013;Automatic design of synthetic gene networks with specific functions is an emerging field in synthetic biology. Quantitative evaluation of gene network designs is a missing feature of the existing automatic design tools. In this work, we address this issue and present a framework to probabilistically analyze the dynamic behavior of a gene network against specifications given in a rich and high level language. Given a gene network built from primitive DNA parts, and given experimental data for the parts, the tool proposed here allows for the automatic construction of a stochastic model of the gene network and in silico probabilistic verification against a rich specification.;Data-driven verification of synthetic gene networks;Not health related;Not health related;0
"K. Khadka; J. Chandrasekaran; Y. Lei; R. N. Kacker; D. Richard Kuhn";2023;Data is a crucial component in machine learning. However, many datasets contain sensitive information such as personally identifiable health and financial data. Access to these datasets must be restricted to avoid potential security concerns. Synthetic data generation addresses this problem by generating artificial data that are similar to, and thus could be used in place of, the original real-world data. This research introduces a synthetic data generation approach called CT-VAE that uses Combinatorial Testing (CT) and Variational Autoencoder (VAE). We first use VAE to learn the distribution of the real-world data and encode it in a latent, lower-dimensional space. Next, we use CT to sample the latent space by generating a t-way set of latent vectors, each of which represents a data point in the latent space. A synthetic dataset is generated from the t-way set by decoding each latent vector in the set. Our experimental evaluation suggests that machine learning models trained with synthetic datasets generated using our approach could achieve performance that is very similar to those trained with real-world datasets. Furthermore, our approach performs better than several state-of-the-art synthetic data generation approaches.;Synthetic Data Generation Using Combinatorial Testing and Variational Autoencoder;Not health related;Not health related;0
"T. Rosenstatter; K. Melnyk";2023;Vehicle-to-Vehicle communication can improve traffic safety and efficiency. This technology, however, increases the attack surface, making new attacks possible. To cope with these threats, researchers have made a great effort to identify and explore the potential of cyberattacks and also proposed various intrusion or misbehaviour detection systems, in particular machine learning-based solutions. Simulations have become essential to design and evaluate such detection systems as there are no real publicly available Vehicular Ad-Hoc Network (VANET) datasets containing a variety of attacks. The drawback is that simulations require a significant amount of computational resources and time for configuration.In this paper, we present an attack simulation and generation framework that allows training the attack generator with either simulated or real VANET attacks. We outline the structure of our proposed framework and describe the setup of a standard-compliant attack simulator that generates valid standardised CAM and DENM messages specified by ETSI in the Cooperative Intelligent Transport Systems (C-ITS) standards. Based on the introduced framework, we demonstrate the feasibility of using deep learning for the generation of VANET attacks, which ultimately allows us to test and verify prototypes without running resource-demanding simulations.;Towards Synthetic Data Generation of VANET Attacks for Efficient Testing;Not health related;Not health related;0
"N. Zarayeneh; M. Sankaranarayanasamy; P. Mavaie; O. Thapliyal; P. Singh; R. Vennelakanti";2023;Colocation data centers play a pivotal role in the digital era, serving as the backbone for a diverse range of businesses, from startups to large enterprises. The global data center construction market, which reached an approximate value of US ${\$}$218.88 billion in 2021, is experiencing unprecedented growth. This surge can be attributed to the ever-increasing volume of data, propelled by economic advancements and population expansion. Consequently, long-term private equity firms and real estate investment trusts (REITs) are increasingly drawn to data center investments for their attributes of transparency and accountability. However, data centers face challenges in terms of project lead times and budget constraints, particularly during expansion phases. In response to these challenges, we propose an innovative power forecasting system that takes into account both internal and external demand signals, leading to more accurate and realistic predictions. These predictions serve as valuable guides for resource allocation and utilization. The system lever- ages a combination of data generation, deep learning (DL), and time-series analysis methods, and later, we use it in our future work to proactively address issues such as supply shortages, ensure the maintenance of Service Level Agreements (SLAs), and optimize resource usage. We synthesized a comprehensive dataset tailored to the data center environment using the small dataset provided by S&P Global. This dataset was enriched with macroeconomic data to capture external influences accurately. Subsequently, we conducted a rigorous evaluation, testing various machine learning models, including linear models, transformer-based models, and a multivariate LSTM model. Our experiments revealed that the PatchTST model outperformed the others, providing the most reliable and precise results. The implementation of advanced analytics further enhances energy efficiency, optimizes equipment utilization, and maximizes the effective utilization of floor space within data centers. Furthermore, efficient resource allocation, guided by the power forecasting system, ensures that customer demands are met promptly and effectively.;Colocation Datacenter Customer Power Usage Forecasting Using Synthetic Data and Integration of Macroeconomic Indicators;Not health related;Not health related;0
"H. Chen; H. Vikalo";2023;"Federated learning (FL) is a privacy-promoting framework that enables potentially large number of clients to collaboratively train machine learning models. In an FL system, a server coordinates the collaboration by collecting and aggregating clients' model updates while the clients' data remains local and private. A major challenge in federated learning arises when the local data is non-iid - the setting in which performance of the learned global model may deteriorate significantly compared to the scenario where the data is identically distributed across the clients. In this paper we propose FedDPMS (Federated Differentially Private Means Sharing), an FL algorithm in which clients augment local datasets with data synthesized using differentially private information collected and communicated by a trusted server. In particular, the server matches the pairs of clients having complementary local datasets and facilitates differentially-private sharing of the means of latent data representations; the clients then deploy variational auto-encoders to enrich their datasets and thus ameliorate the effects of non-iid data distribution. Our experiments on deep image classification tasks demonstrate that FedDPMS outperforms competing state-of-the-art FL methods specifically developed to address the challenge of federated learning on non-iid data.";Federated Learning in Non-IID Settings Aided by Differentially Private Synthetic Data;Not health related;Not health related;0
"R. Delussu; L. Putzu; G. Fumera";2022;Person re-identification is a prominent topic in computer vision due to its security-related applications, and to the fact that issues such as variations in illumination, background, pedestrian pose and clothing appearance make it a very challenging task in real-world scenarios. State-of-the-art supervised methods require a huge manual annotation effort for training data and exhibit limited generalisation capability to unknown target domains. Synthetic data sets have recently been proposed as one possible solution to mitigate these problems, aimed at improving generalisation capability by encompassing a larger amount of variations in the above mentioned visual factors, with no need for manual annotation. However, existing synthetic data sets differ in many aspects, including the number of images, identities and cameras, and in their degree of photorealism, and there is not yet a clear understanding of how all such factors affect person re-identification performance. This work makes a first step towards filling this gap through an indepth empirical investigation, where we use existing synthetic data sets for model training and real benchmark ones for performance evaluation. Our results provide interesting insights towards developing effective synthetic data sets for person re-identification.;On the Effectiveness of Synthetic Data Sets for Training Person Re-identification Models;Not health related;Not health related;0
"H. O. Shahreza; A. George; S. Marcel";2023;State-of-the-art face recognition networks are often computationally expensive and cannot be used for mobile applications. Training lightweight face recognition models also requires large identity-labeled datasets. Meanwhile, there are privacy and ethical concerns with collecting and using large face recognition datasets. While generating synthetic datasets for training face recognition models is an alternative option, it is challenging to generate synthetic data with sufficient intra-class variations. In addition, there is still a considerable gap between the performance of models trained on real and synthetic data. In this paper, we propose a new framework (named SynthDistill) to train lightweight face recognition models by distilling the knowledge of a pretrained teacher face recognition model using synthetic data. We use a pretrained face generator network to generate synthetic face images and use the synthesized images to learn a lightweight student network. We use synthetic face images without identity labels, mitigating the problems in the intra-class variation generation of synthetic datasets. Instead, we propose a novel dynamic sampling strategy from the intermediate latent space of the face generator network to include new variations of the challenging images while further exploring new face images in the training batch. The results on five different face recognition datasets demonstrate the superiority of our lightweight model compared to models trained on previous synthetic datasets, achieving a verification accuracy of 99.52% on the LFW dataset with a lightweight network. The results also show that our proposed framework significantly reduces the gap between training with real and synthetic data. The source code for replicating the experiments is publicly released.;SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data;Not health related;Not health related;0
"K. C. Yuen; L. Haoyang; C. E. Siong";2023;Automatic speech recognition (ASR) for rare words is difficult as there are little relevant text-audio data pairs to train an ASR model. To obtain more text-audio pairs, text-only data are fed to Text-To-Speech (TTS) systems to generate synthetic audio. Previous works use a single TTS system conditioned on multiple speakers to produce different speaker voices to improve the output data’s speaker diversity, and they show that training an ASR model on the more diverse data can avoid overfitting and improve the model’s robustness. As an alternative way to improve the diversity, we study the speaker embedding distribution of audios synthesized by different TTS systems and found that the audios synthesized by different TTS systems have different speaker distributions even when they are conditioned on the same speaker. Inspired by this, this paper proposes to condition multiple TTS systems repeatedly on a single speaker to synthesize more diverse speaker data, so ASR models can be trained more robustly. When we apply our method to a rare word dataset partitioned from National Speech Corpus SG, which contains mostly road names and addresses in its text transcripts, experiments show that a pretrained ASR model adapted to our multi-TTS-same-SPK data gives relatively 9.8% lower word error rate (WER) compared to the ASR models adapted to same-TTS-multi-SPK data of the same data size, and our overall adaptation improves the model’s WER from 57.6% to 16.5% without using any real audio as training data.;ASR Model Adaptation for Rare Words Using Synthetic Data Generated by Multiple Text-To-Speech Systems;Not health related;Not health related;0
"T. Vu Ho; S. Horiguchi; S. Watanabe; P. Garcia; T. Sumiyoshi";2023;Recent studies have shown that synthetic speech can effectively serve as training data for automatic speech recognition models. Text data for synthetic speech is mostly obtained from in-domain text or generated text using augmentation. However, obtaining large amounts of in-domain text data with diverse lexical contexts is difficult, especially in low-resource scenarios. This paper proposes using text from a large generic-domain source and applying a domain filtering method to choose the relevant text data. This method involves two filtering steps: 1) selecting text based on its semantic similarity to the available in-domain text and 2) diversifying the vocabulary of the selected text using a greedy-search algorithm. Experimental results show that our proposed method outperforms the conventional text augmentation approach, with the relative reduction of word-error-rate ranging from 6% to 25% on the LibriSpeech dataset and 15% on a low-resource Vietnamese dataset.;Synthetic Data Augmentation for ASR with Domain Filtering;Not health related;Not health related;0
"A. Nigam; S. Srivastava";2023;Intelligent transportation systems (ITS) are a set of technologies that can be used to improve the efficiency, safety, and sustainability of transportation systems. One of the key challenges in ITS is data sparsity. One of the main reasons for data sparsity in ITS is the high cost of maintaining and deploying ITS infrastructure. To overcome the data sparsity is-sue, researchers have proposed different approaches to generate synthetic traffic data, such as employing traffic simulators or statistical methods. Generative Adversarial Networks (GANs) are a powerful class of deep learning models that excel in generating synthetic data that exhibits the characteristics and found patterns in the training data, particularly in the domains of images and text. In this paper, we discuss the data sparsity challenge in ITS and present our pioneering use of Conditional Tabular Generative Adversarial Networks (CTGANs) to generate synthetic traffic data. It takes into account the conditional dependencies among columns in the dataset, enabling the generation of samples that respect the relationships and patterns observed in the real data. In the field of ITS, the application of CTGANs is unexplored. While CTGANs have been used in other domains such as intrusion detection systems and network traffic analysis, their utilization in ITS is novel and unprecedented. CTGANs offer the potential to generate fine-grained and realistic data, making them valuable for creating accurate simulation setups in ITS. We evaluate our approach using a range of real-world traffic datasets and demonstrate that our method produces realistic and accurate synthetic traffic data. Additionally, we assess the generalization capability of our synthetic data, finding that it exhibits strong performance on unseen data.;Generating Realistic Synthetic Traffic Data using Conditional Tabular Generative Adversarial Networks for Intelligent Transportation Systems;Not health related;Not health related;0
"S. Kane; S. Gupta; P. A. Wilsey";2019;Synthetic workloads are commonly used to exercise simulation tools for performance, performance tuning, and scalability studies. Sometimes these workloads are simple streams of test data following various distributions and in other cases these workloads are generated by more complex, configurable systems. An example of the former is a stream of input events at different arrival rates that might be used to test the performance of an event queue data structure. An example of the latter is the PHOLD simulation model that is often used to contrast the performance implications of different design solutions in a parallel simulation engine. One of the key challenges for synthetic workloads is the question of setting the parameters so that the workload properly reflects the behavior of actual workloads. This paper collects profile data from multiple real-world discrete-event simulation models in multiple configurations and sizes from the ROSS and WARPED2 repositories. A principle focus of this paper is the capture and reporting of profiling data to understand event granularities and event profile data to assist in the configuration of synthetic discrete event model generators.;Analyzing Simulation Model Profile Data to Assist Synthetic Model Generation;Not health related;Not health related;0
"E. C. Elson; S. L. Blyth; A. J. Baker";2016;Much of our current understanding of neutral, atomic gas in galaxies comes from radio observations of the nearby Universe. Until the next generation of instruments allow us to push to much higher redshifts, we must rely mostly upon theoretical models of galaxy formation to provide us with key insights into the likely cosmic evolution of H_i in the Universe, and its links to molecular clouds and star formation. In this work, we present a new set of methods to convert mock galaxy catalogues into synthetic data cubes containing model galaxies with realistic spatial and spectral H_i distributions over large cosmological volumes. Such synthetic data products can be used to guide observing and data handling/analysis strategies for forthcoming H_i galaxy surveys. As a demonstration of the potential use of our simulated products we use them to conduct several mock H_i stacking experiments for both low and high-redshift galaxy samples. The stacked spectra can be accurately decomposed into contributions from target and non-target galaxies, revealing in all co-added spectra large fractions of contaminant mass due to source confusion. Our results are consistent with similar estimates extrapolated from z = 0 observational data. The amount of confused mass in a stacked spectrum grows almost linearly with the size of the observational beam, suggesting potential overestimates of $\Omega _{\mathrm{H\,\small {I}}}$ by some recent H_i stacking experiments. Our simulations will allow the study of subtle redshift-dependent effects in future stacking analyses.;Synthetic data products for future H_i galaxy surveys: a tool for characterizing source confusion in spectral line stacking experiments;Not health related;Not health related;0
"J. Garron; C. Stoner; F. Meyer";2017;Processing large data sets on a cloud-based platform greatly reduces the costs normally associated with the required computing infrastructure due to its on-demand availability and scalability. Cloud-based computing can therefore be leveraged to speed operational delivery of large data sets to research scientists and decision-makers in an emergency response, specifically during a marine oil spill response. Amazon Web Services (AWS) offers a suite of cloud computing services that compose a scalable, on-demand, computing platform to perform small to very large processing jobs without investment in on-site hardware and data support infrastructure. Synthetic aperture radar (SAR) data is a processing intensive, remote-sensing data product, that often requires additional secondary-processing to be formatted for general consumption as a GIS-ready product or other commonly consumable format. SAR has been shown to be effective for the identification of oil spills on water by using either the backscattered amplitude of the signal or the specific components of the signal phase. Oil detection algorithms have been developed for use with both SAR signal-based detection methods, but few have been operationalized, i.e. integrated into an operational service environment or the common operational picture of an oil spill response. An adaptation thresholding oil spill detection algorithm modified from Solberg et al., 2004 has been incorporated into the Sentinel-1Tool Box, on-demand, processing tool suite available from the European Space Agency (ESA), and is accessible to users via downloadable Python scripts. Adaptive thresholding is a straightforward amplitude-based, supervised, oil classification methodology that focuses the analyses on the shape of an anomalous feature, the contrast of that feature from the background water, and the homogeneity of the processed SAR image. In this experiment, the oil spill detection python scripts based on the Solberg et al., 2004 segmentation thresholding algorithm were constructed and downloaded from the ESA Sentinel Tool Box, and integrated into a prototypical processing pipeline in the AWS cloud-computing environment to determine the operational feasibility of performing oil spill detection using SAR in a cloud-processing environment. SAR images were staged to a Simple Storage Service (S3) object storage bucket in AWS, as was the oil detection script downloaded from ESA. Sample SAR images staged to the AWS S3 bucket were a matrix of both satellite-collected C-band SAR data (Sentinel-1A and Sentinel-1B) downloaded from the Alaska Satellite Facility, and simulated SAR data containing different kinds of oil slicks, ocean surface anomalies due to winds and currents, and combinations thereof. Processing took place in a scalable AWS Elastic Compute Cloud (EC2) computing instance, using AWS CloudWatch functions to call and run the algorithm scripts against the sample SAR data to create a GIS-ready product deliverable to multiple end-users from S3 bucket at the end of this prototypical processing pipeline in AWS. End-users only require log in credentials to download the GIS-ready products from the oil detection processing pipeline. This work demonstrates the ability to apply oil detection algorithms to SAR data in a cloud-based environment to create oil spill footprints for integration into an operational data delivery system, and the common operational picture of a marine oil spill response.;Cloud-based oil detection processing pipeline prototype for C-band synthetic aperture radar data;Not health related;Not health related;0
"E. Arkangil; M. Yildirimoglu; J. Kim; C. Prato";2023;Synthetic datasets are useful when real-world data is limited or unavailable. They can be used in transport simulation models to predict travel behavior or estimate demand for transportation services. However, building these models requires large amounts of data. We propose a novel framework to generate a synthetic population with trip chains using a combination of generative adversarial network (GAN) with recurrent neural network (RNN). Our model is compared with other recent methods, such as Composite Travel Generative Adversarial Networks for Tabular and Sequential Population Synthesis (CTGAN) and shows improved results in predicting trip distributions. The model is evaluated using multiple assessment metrics to gauge its performance and accuracy.;A deep learning framework to generate synthetic mobility data;Not health related;Not health related;0
"T. Lapthawan; S. Prom-On; P. Birkholz; Y. Xu";2022;Representation learning is one of the fundamental issues in modeling articulatory-based speech synthesis using target-driven models. This paper proposes a computational strategy for learning underlying articulatory targets from a 3D articulatory speech synthesis model using a bi-directional long short-term memory recurrent neural network based on a small set of representative seed samples. Using a seeding set from VocalTractLab, a larger training set was generated that provided richer contextual variations for the model to learn. The deep learning model for acoustic-to-target mapping was then trained to model the inverse relation of the articulation process. This method allows the trained model to map the given acoustic data onto the articulatory target parameters which can then be used to identify the distribution based on linguistic contexts. The model was evaluated based on its effectiveness in mapping acoustics to articulation, and the perceptual accuracy of speech reproduced from the articulation estimated from the recorded speech by native Thai speakers. The model achieved more than 80% phoneme classification accuracy in the listening test conducted with 25 native Thai speakers. The results indicate that the model can accurately imitate speech with a high degree of phonemic precision.;Estimating Underlying Articulatory Targets of Thai Vowels by Using Deep Learning Based on Generating Synthetic Samples From a 3D Vocal Tract Model and Data Augmentation;Not health related;Not health related;0
"S. Sattarzadeh; S. M. Shalmani; S. Azad";2022;Although the remarkable breakthrough offered by Deep Learning (DL) models in numerous computer vision tasks, the need to acquire large amounts of high-quality natural data and fine-grained annotations is a shortcoming that fundamentally increases the cost and time devoted to training these models in real-world applications. Hence, synthetic datasets are considered reliable alternatives that can reduce the data acquisition by replacing or merging with natural data or effective pre-training of the models. To this end, in this work, we propose a novel approach to integrate structural data structures with the synthetic noise structures learned by unsupervised models that mimic the noise structures in natural data. Based on the proposed approach, we introduce the Sinusoid Feature Recognition (SFR) dataset, which contains hard-to-detect fixed-period sinusoid waves. While the previous works in this regard use generative models to sample synthetic data to inflate the training set, we instead apply unsupervised learning models to generate deep synthetic noise which makes training models in the proposed dataset more challenging. We evaluate the segmentation, image reconstruction, and sinusoid characterization models pre-trained or fully trained on the synthetic SFR dataset on a private dataset of grayscale Acoustic Tele-Viewer (ATV) images. Experimental results show that supervision on our proposed synthetic dataset can improve the accuracy of the models by 3-4% via pre-training, and by 17-27% via ad-hoc training while dealing with challenging, realistic real-world images.;Mitigating Paucity of Data in Sinusoid Characterization Using Generative Synthetic Noise;Not health related;Not health related;0
"M. Meiser; B. Duppe; I. Zinnikus";2023;Recently, an increasing number of Artificial Intelligence services have been developed for a variety of domains. Machine Learning and especially Deep Learning services require a large amount of data to provide their functionality. Since data collection is typically complex and difficult, there is often not enough data available. Machine learning services such as anomaly detection or disaggregation algorithms are also being developed in the smart living domain. In practice, however, only a few energy datasets are publicly available, as the collection of such data is expensive and time-consuming due to the equipment required. One way to generate more smart meter data is to use a simulation. Developing such a simulation that is capable of generating meaningful data is a complex task. Therefore, in this paper, we present the Synthetic Time Series Data Generator (SynTiSeD), a multi-agent-based simulation tool that generates meaningful synthetic energy data based on real-world data. Furthermore, SynTiSeD allows generating data of critical situations, which are important for the development of such services, but which cannot be provoked in the real world. For transferability, we demonstrate that Nonintrusive Load Monitoring algorithms trained on synthetic data generated by SynTiSeD provide meaningful results that are even better than those of models trained on real data.;SynTiSeD – Synthetic Time Series Data Generator;Not health related;Not health related;0
"J. Zhang; T. -M. Chung";2022;Pulmonary nodules are an early indication of lung cancer, and their detection is crucial in the diagnosis of lung cancer. Many researches have been conducted and some progress has been made, but it is still a challenge since the boundaries of pulmonary nodules are blurred, small in size and diverse in shape, making nodules difficult to detect. Moreover, medical datasets are often scarce and precious. To alleviate the above problems, we propose an improved YOLO V5 framework for pulmonary nodule detection based on synthetic data from multiple generative adversarial networks (GANs). Specifically, we use Resnet50 as the YOLO V5 backbone, which introduces residual connections in extracting pulmonary nodule features, thus reducing the effect of diminishing returns and avoiding the gradient explosion problem. In addition, a deformable convolutional network is presented to take into account the detection ability of different shapes and sizes of pulmonary nodules. Furthermore, we suggest using DCGAN, StyleGAN and SAGAN separately to generate realistic and diverse training images to complement the dataset, thus solving the problem of lack of lung nodule samples. The experimental results show that 78.2% precision and 68.1% recall were achieved using only Resnet50. After augmenting the dataset with GAN and using deformable convolution, both the detection and recall of pulmonary nodules have been improved, achieving the best result with 87.8% precision and 82.7% recall after augmenting the dataset with SAGAN.;An Improved YOLO V5 Model for Pulmonary Nodule Detection with Synthetic Data Generated by GAN;Not health related;Not health related;0
"A. Galloni; I. Lendák; T. Horváth";2023;This paper explores the impact of the evolving evaluation metrics used to evaluate synthetic data and the related generative mechanism in the context of Differential Privacy (DP). Specifically, the authors provide an overview of current synthetic data evaluation methodologies and most commonly used metrics, identifying improvement possibilities. As the field continues to evolve, the authors also propose an improved version of a previously introduced comprehensive evaluation metric useful to redefine such evaluation frameworks in order to make them more complete and exhaustive. This proposed composite metric offers new ideas and directions for future comprehensive quantitative analysis of differentially private synthetic data and, thus, of their related generators.;Extending Synthetic Data Evaluation Metrics;Not health related;Not health related;0
"C. Bartz; H. Raetz; J. Otholt; C. Meinel; H. Yang";2022;One of the most pressing problems in the automated analysis of historical documents is the availability of annotated training data. The problem is that labeling samples is a time-consuming task because it requires human expertise and thus, cannot be automated well. In this work, we propose a novel method to construct synthetic labeled datasets for historical documents where no annotations are available. We train a StyleGAN model to synthesize document images that capture the core features of the original documents. While originally, the StyleGAN architecture was not intended to produce labels, it indirectly learns the underlying semantics to generate realistic images. Using our approach, we can extract the semantic information from the intermediate feature maps and use it to generate ground truth labels. To investigate if our synthetic dataset can be used to segment the text in historical documents, we use it to train multiple supervised segmentation models and evaluate their performance. We also train these models on another dataset created by a state-of-the-art synthesis approach to show that the models trained on our dataset achieve better results while requiring even less human annotation effort.;Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data;Not health related;Not health related;0
"X. Liu; S. Tao; J. Zheng";2023;Synthetic lethality (SL) is a type of genetic interaction where the simultaneous mutation of two genes leads to cell death, while the mutation in either gene individually does not. It is promising to broaden the range of anti-cancer drug targets. Current machine learning methods primarily use either knowledge graphs or handcrafted features to represent genes for SL prediction, overlooking the effective integration of both data types. Furthermore, most of these methods were designed for pan-cancer SL prediction and only a few methods have been proposed for cancer-specific SL prediction. For these cancer-specific methods, a notable decrease in performance is observed when the training data is reduced. Cancer-specific SL prediction under low-data scenarios remains a challenging problem. To address these issues, we propose a novel model named Meta-CapSL to predict potential cancer-specific SL with few labeled data. First, we leverage a graph neural network with self-supervised learning to obtain the cancer-free gene embeddings from a biomedical knowledge graph. We design a strategy to get cancer-specific gene expressions by capturing the context of genes. Then, we use a self-attention-based fusion strategy to integrate the representations of KG embedding and gene expression. Moreover, a meta-learning framework is used to transfer meta-knowledge of SL interactions across diverse cancer types and obtain a well-initialized model which is capable of fast adaptation to a new cancer type with few samples. Extensive experiments demonstrate that Meta-CapSL significantly outperforms the best baseline and generalizes well under three low-data scenarios. Our code is available at https://github.com/JieZheng-ShanghaiTech/Meta-CapSL.;Meta Learning for Low-Data Prediction of Cancer-Specific Synthetic Lethality as Drug Targets;health related;health related;0
"X. Gu; G. Liu; X. Zhang; L. Tang; X. Zhou; W. Qiu";2023;Limited by the shooting scenes and angles of fixed cameras, the existing datasets generally lack many detailed pedestrian models in diverse scenarios. Existing deep learning-based image fusion methods, for this reason, bring about overfitting or insufficient information of fusion results in varying degrees. To address this challenge, A new infrared-visible pedestrian synthetic dataset (GIVF) with a synthetic data tagger (GSDT) is constructed and an improved end-to-end image fusion network (FSGAN) is proposed to validate infrared and visible fusion. In the model, the method uses an auxiliary network to extract features that complement the cascade network of the main path, effectively improving the ability to extract pedestrian texture details. Experimental results show that FSGAN can be well applied to GIVF. By conducting extensive comparative experiments with eight state-of-the-art image fusion methods. FSGAN shows better performance than those comparison methods, especially in the two evaluation indexes visual information fidelity (VIF) and structural similarity measurement (SSIM). Besides, by comparing the quantitative analysis results of various methods, and evaluating the fusion results of real images in complex environments on other three datasets, we conclude that FSGAN can be better applied to GIVF datasets than other popular methods, and has outstanding performance in generalization.;Infrared-Visible Synthetic Data from Game Engine for Image Fusion Improvement;Not health related;Not health related;0
"G. Adhikari; S. Halder; S. Banerjee; S. S. Chaudhuri";2021;Before the advent of microwave based imaging radars, most passive high resolution sensors were camera systems with detectors that were sensitive to either solar radiation or thermal radiation emitted from the earth's surface. The Synthetic Aperture Radar (SAR) represented a fundamentally different technique for earth surface observations. A microwave based radar system is an active method of remote sensing that transmits a beam/out-burst of electromagnetic (EM) radiation which falls in the microwave region of the EM spectrum and this instrument is used to observe properties of the earth's surface which were previously not detectable by ordinary photo-sensitive sensors (eg. optical, thermal). As an active system, SAR provides its own source of illuminating a target (microwave energy) and is not dependent on the light from the sun which most of the other type of sensors rely on, this permits a SAR based radar imaging for continuous day/night operation. Furthermore, neither clouds, fog, nor precipitation have a significant effect on microwave, thus permitting all-weather imaging capability. The net result is an instrument that is capable of continuously observing dynamic phenomena of ocean currents, weather patterns, changing patterns of vegetation, etc. However, all radar images appear with some degree of radar speckle i.e. graininess/salt and pepper texture in the image and is inherently present in any of the three modes (spotlight, scanSAR, stripmap) of acquisition. Speckle is a very serious and major issue in processing of SAR images and it is extremely difficult to go for machine interpretation and extraction of useful information from the mapped data. This problem of graininess in the image of an earth feature is caused by random constructive and destructive interference from the multiple scattering returns that occur within each resolution cell. SAR data has been used for a variety of applications (e.g. cartography, geologic structure mapping) for which qualitative analyses of the image products were sufficient to extract the desired information. However, to fully exploit the back-scattered information contained in a raw SAR data, quantitative analysis of the target back-scatter characteristics is required. Also, raw SAR data suffers from geometric distortion which arises from variation in the terrain elevation and pose a problem to side looking ranging instrument as in the case of a SAR system. In this paper, we have proposed a series of processing steps using which a very accurate polarized feature matrix values for a particular back-scatterer on earth's surface can be obtained.;An Edge Aware Polarimetric Co-Variance Feature Matrix Generation of Geo-Coded and Quad-Polarized Single Look Complex Synthetic Aperture Radar Data;Not health related;Not health related;0
"E. Moreu; E. Arazo; K. McGuinness; N. E. O'Connor";2023;Early detection of colorectal polyps is of utmost importance for their treatment and for colorectal cancer prevention. Computer vision techniques have the potential to aid professionals in the diagnosis stage, where colonoscopies are manually carried out to examine the entirety of the patient's colon. The main challenge in medical imaging is the lack of data, and a further challenge specific to polyp segmentation approaches is the difficulty of manually labeling the available data: the annotation process for segmentation tasks is very time-consuming. While most recent approaches address the data availability challenge with sophisticated techniques to better exploit the available labeled data, few of them explore the self-supervised or semi-supervised paradigm, where the amount of labeling required is greatly reduced. To address both challenges, we leverage synthetic data and propose an end-to-end model for polyp segmentation that integrates real and synthetic data to artificially increase the size of the datasets and aid the training when unlabeled samples are available. Concretely, our model, PI-CUT-Seg, transforms synthetic images with an image-to-image translation module and combines the resulting images with real images to train a segmentation model, where we use model predictions as pseudolabels to better leverage unlabeled samples. Additionally, we propose PL-CUT-Seg+, an improved version of the model that incorporates targeted regularization to address the domain gap between real and synthetic images. The models are evaluated on standard benchmarks for polyp segmentation and reach state-of-the-art results in the self- and semi-supervised setups.;Self-Supervised and Semi-Supervised Polyp Segmentation using Synthetic Data;health related;Not health related;1
"A. Midelet; S. Bailly; J. -C. Borel; R. L. Hy; M. -C. Schaeffer; S. Baillieul; R. Tamisier; J. -L. Pépin";2022;"Objective: In obstructive sleep apnea patients on continuous positive airway pressure (CPAP) treatment there is growing evidence for a significant impact of the type of mask on the residual apnea-hypopnea index (rAHI). Here, we propose a method for automatically classifying the impact of mask changes on rAHI. Methods: From a CPAP telemonitoring database of 3,581 patients, an interrupted time series design was applied to rAHI time series at a patient level to compare the observed rAHI after a mask-change with what would have occurred without the mask-change. rAHI time series before mask changes were modelled using different approaches. Mask changes were classified as: no effect, harmful, beneficial. The best model was chosen based on goodness-of-fit metrics and comparison with blinded classification by an experienced respiratory physician. Results: Bayesian structural time series with synthetic controls was the best approach in terms of agreement with the physician.s classification, with an accuracy of 0.79. Changes from nasal to facial mask were more often harmful than beneficial: $13.4\%\; {\rm vs}\; 7.6\% $ (p-value < 0.05), with a clinically relevant increase in average rAHI greater than 8 events/hour in $4.6\%$ of cases. Changes from facial to nasal mask were less often harmful: $6.0\%\; {\rm vs}\; 11.4\%$ (p-value < 0.05). Conclusion: We propose an end-to-end method to automatically classify the impact of mask changes over fourteen days after a switchover. Significance: The proposed automated analysis of the impact of changes in health device settings or accessories presents a novel tool to include in remote monitoring platforms for raising alerts after harmful interventions.";Bayesian Structural Time Series With Synthetic Controls for Evaluating the Impact of Mask Changes in Residual Apnea-Hypopnea Index Telemonitoring Data;Not health related;Not health related;0
"K. L. Fox; K. R. Niewoehner; M. D. Rahmes; R. Razdan";2022;"Machine Learning (ML) offers a revolutionary method to build algorithms from data. This data-driven methodology has proven to be very effective in progressing fields ranging from natural language processing to autonomy. Simultaneously, the technique has also opened significant challenges in system validation with important issues when used in safety-critical systems. If data forms function, how does one know the generated function is safe? Further, fundamental to the ML approach is the availability of vast amounts of data for training a ML model. For a variety of important applications, the availability of data is challenging.In this paper, we propose a unique computational paradigm which combines the inherent advantages of ML with algorithmic transformational methods. Using this technique, one can generate a large amount of synthetic data which preserve the ""signature"" of the original dataset while building variations that aid the ML training process. Further, the nature of the algorithmic transformations offers insights into the validation and verification (V&V) scenario generation process. An example is provided of the methodology exercised in a maintenance application for the infrastructure delivering the safety critical Automatic Dependent Surveillance-Broadcast (ADS-B) services in the United States (US).";Utilizing Synthetic Data for VV&C of Machine Learning Applications;Not health related;Not health related;0
"D. Altinses; A. Schwung";2023;Deep networks have been successfully applied to industrial applications for clean unimodal data (e.g., sensors, images, or audio). Leveraging multimodal data is a common approach to enhance performance, guided by the principle that a larger quantity of data leads to improvement. However, performance may decline considerably if corruption in the data is present (e.g., noise, blur, failure). Although researchers have explored various data augmentation methods to improve the generalization capacity, these methods are not adapted for industrial settings. The primary distinction is that current augmentation methods are designed to enhance model generalization capabilities and not realistically simulate real-world industry scenarios. In this paper, we present industry-related augmentation methods for temporal and spatial data for multimodal fusion with deep neural networks. Our methods are specifically designed to encourage modality collaboration and reinforce generalization capability. The impact of the proposed data extension strategy to train multimodal fusion models is assessed on a synthetic dataset from an industrial UR5 robot with varying degrees of imbalance. In our study, we analyze different combinations of methods and evaluate their performance. Through these experiments, we are able to identify the challenges in multimodal fusion with deep learning models in an industrial setting.;Multimodal Synthetic Dataset Balancing: a Framework for Realistic and Balanced Training Data Generation in Industrial Settings;Not health related;Not health related;0
"D. Leng; C. Huang; J. Lei; S. Sun; Z. X.D.";2018;Accumulating evidence has demonstrated that long non-coding RNAs (lncRNAs) play important roles in initiation and development of human diseases. However, the mechanism of the targets regulated by lncRNA remains unclear. In this study, we performed a multi-step computational analysis to construct dysregulated lncRNA-mRNA networks for the rats' RNAseq data induced by three different synthetic cytotoxic compounds (CARBON TETRACHLORIDE, CHLOROFORM, THIOACETAMIDE). We systematically integrated lncRNA and mRNA expression profiles and lncRNA-mRNA regulatory interactions. The constructed interaction network exhibited biological network characteristics, and functional analysis demonstrated that the networks were specific for inducing synthetic compounds. Additionally, we identified some lncRNA-mRNA modules. This study will provide us new insight into lncRNA-mRNA regulatory mechanisms involved in rats induced by three different synthetic cytotoxic compounds and will facilitate the discovery of candidate diagnostic and prognosis biomarkers for related diseases.;Exploration of dysregulated lncRNA-mRNA network from the RNA-seq data of rats induced by three different synthetic cytotoxic compounds;Not health related;Not health related;0
"A. Kiran; S. S. Kumar";2024;According to a report published by Gartner in 2021, a significant portion of Machine Learning (ML) training data will be artificially generated. This development has led to the emergence of various synthetic data generators (SDGs), particularly those based on Generative Adversarial Networks (GAN). All research endeavors so far have been exploratory, focused on specific objectives such as validating utility or disclosure control or assessing how generators can decrease or increase inherent bias with differential privacy. Hence, we aim to empirically identify an AI-based, data generator that can produce datasets that closely resemble real datasets, while also determining the hyper-parameters that enable a satisfactory balance between utility, privacy, and fairness in the datasets. To achieve this, we utilize the Synthetic Data Vault, Data Synthesizer, and Smartnoise-synth, which are three synthetic data generation packages that are accessible via Python. Different data generation models available within the package are presented with 13 tabular datasets iteratively as sample inputs to generate synthetic data. We generated synthetic data using every dataset and generator and investigated the goodness of the generator using five hypothetical scenarios. The utility and privacy offered by the generated data were compared with those of real data. The fairness in the ML model trained with synthetic data was used as a third metric for evaluation. Finally, we employ synthetic data to train regression and classification Machine Learning (ML) algorithms and evaluate their performance. After conducting experiments, analyzing metrics, and comparing ML scores across all 11 generators, we determined that the CTGAN from SDV and PATECTGAN from the SN-synth package were the most effective in mimicking real data for all 13 datasets utilized in our research.;A Methodology and an Empirical Analysis to Determine the Most Suitable Synthetic Data Generator;Not health related;Not health related;0
"M. Schäfer; L. Groos; T. Forbriger; T. Bohlen";2014;Full-waveform inversion (FWI) of shallow-seismic surface waves is able to reconstruct lateral variations of subsurface elastic properties. Line-source simulation for point-source data is required when applying algorithms of 2-D adjoint FWI to recorded shallow-seismic field data. The equivalent line-source response for point-source data can be obtained by convolving the waveforms with $\sqrt{t^{-1}}$ (t: traveltime), which produces a phase shift of π/4. Subsequently an amplitude correction must be applied. In this work we recommend to scale the seismograms with $\sqrt{2 r v_{\rm ph}}$ at small receiver offsets r, where vph is the phase velocity, and gradually shift to applying a $\sqrt{t^{-1}}$ time-domain taper and scaling the waveforms with $r\sqrt{2}$ for larger receiver offsets r. We call this the hybrid transformation which is adapted for direct body and Rayleigh waves and demonstrate its outstanding performance on a 2-D heterogeneous structure. The fit of the phases as well as the amplitudes for all shot locations and components (vertical and radial) is excellent with respect to the reference line-source data. An approach for 1-D media based on Fourier–Bessel integral transformation generates strong artefacts for waves produced by 2-D structures. The theoretical background for both approaches is presented in a companion contribution. In the current contribution we study their performance when applied to waves propagating in a significantly 2-D-heterogeneous structure. We calculate synthetic seismograms for 2-D structure for line sources as well as point sources. Line-source simulations obtained from the point-source seismograms through different approaches are then compared to the corresponding line-source reference waveforms. Although being derived by approximation the hybrid transformation performs excellently except for explicitly back-scattered waves. In reconstruction tests we further invert point-source synthetic seismograms by a 2-D FWI to subsurface structure and evaluate its ability to reproduce the original structural model in comparison to the inversion of line-source synthetic data. Even when applying no explicit correction to the point-source waveforms prior to inversion only moderate artefacts appear in the results. However, the overall performance is best in terms of model reproduction and ability to reproduce the original data in a 3-D simulation if inverted waveforms are obtained by the hybrid transformation.;Line-source simulation for shallow-seismic data. Part 2: full-waveform inversion—a synthetic 2-D case study;Not health related;Not health related;0
"A. Meléndez; J. Korenaga; V. Sallares; A. Miniussi; C. R. Ranero";2015;We present a new 3-D traveltime tomography code (TOMO3D) for the modelling of active-source seismic data that uses the arrival times of both refracted and reflected seismic phases to derive the velocity distribution and the geometry of reflecting boundaries in the subsurface. This code is based on its popular 2-D version TOMO2D from which it inherited the methods to solve the forward and inverse problems. The traveltime calculations are done using a hybrid ray-tracing technique combining the graph and bending methods. The LSQR algorithm is used to perform the iterative regularized inversion to improve the initial velocity and depth models. In order to cope with an increased computational demand due to the incorporation of the third dimension, the forward problem solver, which takes most of the run time (_90 per cent in the test presented here), has been parallelized with a combination of multi-processing and message passing interface standards. This parallelization distributes the ray-tracing and traveltime calculations among available computational resources. The code's performance is illustrated with a realistic synthetic example, including a checkerboard anomaly and two reflectors, which simulates the geometry of a subduction zone. The code is designed to invert for a single reflector at a time. A data-driven layer-stripping strategy is proposed for cases involving multiple reflectors, and it is tested for the successive inversion of the two reflectors. Layers are bound by consecutive reflectors, and an initial velocity model for each inversion step incorporates the results from previous steps. This strategy poses simpler inversion problems at each step, allowing the recovery of strong velocity discontinuities that would otherwise be smoothened.;TOMO3D: 3-D joint refraction and reflection traveltime tomography parallel code for active-source seismic data—synthetic test;Not health related;Not health related;0
"W. Zürn; A. M. G. Ferreira; R. Widmer-Schnidrig; K. Lentas; L. Rivera; E. Clévédé";2015;We present spectra concentrating on the lowest-frequency normal modes of the Earth obtained from records of the invar-wire strainmeters and STS-1 broad-band seismometers located in the Black Forest Observatory, Germany after the disastrous earthquakes off the NW coast of Sumatra in 2004 and off the coast near Tohoku, Japan in 2011. We compare the spectra to ones obtained from synthetic seismograms computed using a mode summation technique for an anelastic, elliptical, rotating, spherically symmetric Earth model. The synthetics include strain–strain-coupling effects by using coupling coefficients obtained from comparisons between Earth tide signals recorded by the strainmeters and synthetic tidal records. We show that for the low-frequency toroidal and spheroidal modes up to 1 mHz, the strainmeters produce better signal-to-noise ratios than the broad-band horizontal seismometers. Overall, the comparison with the synthetics is satisfactory but not as good as for vertical accelerations. In particular, we demonstrate the high quality of the strainmeter data by showing the Coriolis splitting of toroidal modes for the first time in individual records, the first clear observation of the singlet ${}_{2}S_{1}^{0}$ and the detection of the fundamental radial mode 0S0 with good signal-to-noise ratio and with a strain amplitude of 10_11. We also identify the latter mode in a record of the Isabella strainmeter after the great Chilean quake in 1960, the detection of which was missed by the original studies.;High-quality lowest-frequency normal mode strain observations at the Black Forest Observatory (SW-Germany) and comparison with horizontal broad-band seismometer data and synthetics;Not health related;Not health related;0
"Jiancheng Shi; J. Dozier";1995;;"Corrections to ""Inferring Snow Wetness Using C-Band Data from SIR-C's Polarimetric Synthetic Apertur";Not health related;Not health related;0
"M. Miltner; P. P. Duan; M. U. de Haag";2014;• System degradation due to sensor failures has been a contributing factor in many aircraft accidents and incidents • Sensor failures can potentially disable or modify the automation in flight • Flight crews may not be fully aware of how automation modes operate, especially complex automation modes such as VNAV;Modeling and utilization of synthetic data for improved automation and human-machine interface continuity;Not health related;Not health related;0